{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:12.676366Z",
     "start_time": "2020-08-25T08:31:12.527984Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# db_utils\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import sqlalchemy\n",
    "import os\n",
    "import pyodbc\n",
    "import sys\n",
    "# import pymssql\n",
    "import numpy as np\n",
    "\n",
    "from MySQLdb._exceptions import OperationalError\n",
    "from sqlalchemy import create_engine, exc,event\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "from time import time\n",
    "\n",
    "#connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=192.168.15.126;UID=BRS;PWD=Fint$123;Trusted Connection=yes;DATABASE=\"\n",
    "connection_string = None\n",
    "import logging\n",
    "   \n",
    "# logging = Logging()\n",
    "\n",
    "class DB(object):\n",
    "    def __init__(self, database, host='127.0.0.1', user='root', password='', port='3306', tenant_id=None):\n",
    "        \"\"\"\n",
    "        Initialization of databse object.\n",
    "\n",
    "        Args:\n",
    "            databse (str): The database to connect to.\n",
    "            host (str): Host IP address. For dockerized app, it is the name of\n",
    "                the service set in the compose file.\n",
    "            user (str): Username of MySQL server. (default = 'root')\n",
    "            password (str): Password of MySQL server. For dockerized app, the\n",
    "                password is set in the compose file. (default = '')\n",
    "            port (str): Port number for MySQL. For dockerized app, the port that\n",
    "                is mapped in the compose file. (default = '3306')\n",
    "        \"\"\"\n",
    "\n",
    "        if host in [\"common_db\",\"extraction_db\", \"queue_db\", \"template_db\", \"table_db\", \"stats_db\", \"business_rules_db\", \"reports_db\"]:\n",
    "            self.HOST = os.environ['HOST_IP']\n",
    "            self.USER = os.environ['LOCAL_DB_USER']\n",
    "            self.PASSWORD = os.environ['LOCAL_DB_PASSWORD']\n",
    "            self.PORT = os.environ['LOCAL_DB_PORT']\n",
    "            self.DATABASE = f'{tenant_id}_{database}' if tenant_id is not None and tenant_id else database\n",
    "        else:\n",
    "            self.HOST = os.environ['HOST_IP']\n",
    "            self.USER = os.environ['LOCAL_DB_USER']\n",
    "            self.PASSWORD = os.environ['LOCAL_DB_PASSWORD']\n",
    "            self.PORT = os.environ['LOCAL_DB_PORT']\n",
    "            self.DATABASE = f'{tenant_id}_{database}' if tenant_id is not None and tenant_id else database\n",
    "       \n",
    "        logging.info(f'Host: {self.HOST}')\n",
    "        logging.info(f'User: {self.USER}')\n",
    "        logging.info(f'Password: {self.PASSWORD}')\n",
    "        logging.info(f'Port: {self.PORT}')\n",
    "        logging.info(f'Database: {self.DATABASE}')\n",
    "        # self.connect()\n",
    "    def connect(self, max_retry=5):\n",
    "#         retry = 1\n",
    "\n",
    "#         try:\n",
    "#             start = time()\n",
    "#             logging.debug(f'Making connection to {self.DATABASE}...')\n",
    "#             config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{self.DATABASE}?charset=utf8'\n",
    "#             self.db_ = create_engine(config, connect_args={'connect_timeout': 2}, pool_recycle=300)\n",
    "#             logging.info(f'Engine created for {self.DATABASE}')\n",
    "#             while retry <= max_retry:\n",
    "#                 try:\n",
    "#                     self.engine = self.db_.connect()\n",
    "#                     logging.info(f'Connection established succesfully to {self.DATABASE}! ({round(time() - start, 2)} secs to connect)')\n",
    "#                     break\n",
    "#                 except Exception as e:\n",
    "#                     logging.warning(f'Connection failed. Retrying... ({retry}) [{e}]')\n",
    "#                     retry += 1\n",
    "#                     self.db_.dispose()\n",
    "#         except:\n",
    "#             logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "#             return\n",
    "        data = []\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        print(inds)\n",
    "        for pos, ind in enumerate(inds):\n",
    "            if pos % 2 == 0:\n",
    "                sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "            else:\n",
    "                sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "        if connection_string:\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + self.DATABASE)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pyodbc.connect('DRIVER={' + driver + '};SERVER=' + host_ + ';DATABASE=' + database+ ';UID=' + user_ + ';PWD=' + password_ + ';Trusted Connection=yes;')\n",
    "                else:\n",
    "                    conn = pyodbc.connect('DRIVER={' + driver + '};SERVER=' + host_ + ';DATABASE=' + database + ';Trusted Connection=yes;')\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pyodbc.connect('DRIVER={' + driver + '};SERVER=' + host_ + ';DATABASE=' + database + ';Trusted Connection=yes;')\n",
    "\n",
    "    def convert_to_mssql(self, query):\n",
    "        inds = [i for i in range(len(query)) if query[i] == '`']\n",
    "        for pos, ind in enumerate(inds):\n",
    "            if pos % 2 == 0:\n",
    "                query = query[:ind] + '[' + query[ind+1:]\n",
    "            else:\n",
    "                query = query[:ind] + ']' + query[ind + 1:]\n",
    "       \n",
    "        query = query.replace('%s', '?')\n",
    "\n",
    "        return query\n",
    "\n",
    "    def execute(self, query, database=None, index_col='id', **kwargs):\n",
    "        logging.debug(f'Before converting: {query}')\n",
    "        query = self.convert_to_mssql(query)\n",
    "        logging.debug(f'After converting: {query}')\n",
    "\n",
    "        logging.debug('Connecting to DB')\n",
    "        conn = pyodbc.connect(f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={self.HOST};UID={self.USER};PWD={self.PASSWORD};Trusted Connection=yes;DATABASE={self.DATABASE}', as_dict=True)\n",
    "        logging.debug(f'Connection established with {self.DATABASE}. [{conn}]')\n",
    "        curs = conn.cursor()\n",
    "        logging.debug(f'Cursor object created. [{curs}]')\n",
    "        params = tuple(kwargs.get('params', []))\n",
    "       \n",
    "        logging.debug(f'Params: {params}')\n",
    "        logging.debug(f'Params Type: {type(params)}')\n",
    "        params = [int(i) if isinstance(i, np.int64) else i for i in params]\n",
    "        curs.execute(query, params)\n",
    "        logging.debug(f'Query executed.')\n",
    "       \n",
    "        data = None\n",
    "\n",
    "        try:\n",
    "            logging.debug(f'Fetching all data.')\n",
    "            data = curs.fetchall()\n",
    "            # logging.debug(f'Data fetched: {data}')\n",
    "            columns = [column[0] for column in curs.description]\n",
    "            logging.debug(f'Columns: {columns}')\n",
    "            result = []\n",
    "            for row in data:\n",
    "                result.append(dict(zip(columns, row)))\n",
    "            # logging.debug(f'Zipped result: {result}')\n",
    "            if result:\n",
    "                data = pd.DataFrame(result)\n",
    "            else:\n",
    "                data = pd.DataFrame(columns=columns)\n",
    "            # logging.debug(f'Data to DF: {data}')\n",
    "        except:\n",
    "            logging.debug('Update Query')\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            logging.debug(f'Data is not a DataFrame. Returning True. [{type(data)}]')\n",
    "            return True\n",
    "       \n",
    "        try:\n",
    "            if index_col is not None:\n",
    "                logging.debug(f'Setting ID as index')\n",
    "                return data.where((pd.notnull(data)), None).set_index('id')\n",
    "            else:\n",
    "                return data.where((pd.notnull(data)), None)\n",
    "        except:\n",
    "            logging.exception(f'Failed to set ID as index')\n",
    "            return data.where((pd.notnull(data)), None)\n",
    "\n",
    "    def execute__(self, query, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Executes an SQL query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that needs to be executed.\n",
    "            database (str): Name of the database to execute the query in. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            params (list/tuple/dict): List of parameters to pass to in the query.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data from the executed\n",
    "            query. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        data = None\n",
    "\n",
    "#         # Use new database if a new databse is given\n",
    "#         if database is not None:\n",
    "#             try:\n",
    "#                 config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "#                 engine = create_engine(config, pool_recycle=300)\n",
    "#             except:\n",
    "#                 logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "#                 return False\n",
    "#         else:\n",
    "#             engine = self.engine\n",
    "       \n",
    "        print('query', query)\n",
    "        if database is None:\n",
    "            database = 'karvy'\n",
    "        data = None\n",
    "        sql = query\n",
    "        user_ = self.USER\n",
    "        host_ = self.HOST\n",
    "        database = self.DATABASE\n",
    "        password_ = self.PASSWORD\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        for pos, ind in enumerate(inds):\n",
    "            if pos % 2 == 0:\n",
    "                sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "            else:\n",
    "                sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "               \n",
    "        if connection_string:\n",
    "            print('connection string', connection_string)\n",
    "            print('database', database)\n",
    "            print(type(connection_string + database))\n",
    "            print(type(connection_string + database))\n",
    "\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + database)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pymssql.connect(host=host_,database=database,user=user_,password=password_)\n",
    "                else:\n",
    "                    conn = pymssql.connect(host=host_,database=database)\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pymssql.connect(host=host_,database=database)\n",
    "        try:\n",
    "            logging.debug(f'Query: {query}')\n",
    "            logging.debug(f'Query: {query}')\n",
    "            curs = conn.cursor(as_dict = True)\n",
    "            params = kwargs.get('params', [])\n",
    "            logging.debug(f'Params: {params}')\n",
    "            curs.execute(sql, tuple(params))\n",
    "            print('query executed')\n",
    "            try:\n",
    "                data = curs.fetchall()\n",
    "                data = pd.DataFrame(data)\n",
    "                # print(data)\n",
    "            except Exception as e:\n",
    "                logging.debug('Update Query')\n",
    "            # data = pd.read_sql(sql, conn, index_col='id', **kwargs)\n",
    "        except exc.ResourceClosedError:\n",
    "            logging.warning('Query does not have any value to return.')\n",
    "            return True\n",
    "        except (exc.StatementError, OperationalError) as e:\n",
    "            logging.warning(f'Creating new connection. Engine/Connection is probably None. [{e}]')\n",
    "            self.connect()\n",
    "            data = pd.read_sql(query, self.engine, index_col='id', **kwargs)\n",
    "        except:\n",
    "            logging.exception('Something went wrong executing query. Check trace.')\n",
    "            params = kwargs['params'] if 'params' in kwargs else None\n",
    "            conn.rollback()\n",
    "            return False\n",
    "        conn.close()\n",
    "        return data.where((pd.notnull(data)), None).set_index('id')\n",
    "\n",
    "    def execute_(self, query, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Executes an SQL query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that needs to be executed.\n",
    "            database (str): Name of the database to execute the query in. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            params (list/tuple/dict): List of parameters to pass to in the query.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data from the executed\n",
    "            query. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        logging.debug(f'Executing `execute` instead of `execute_`')\n",
    "        return self.execute(query, index_col=None, **kwargs)\n",
    "       \n",
    "        data = None\n",
    "\n",
    "#         # Use new database if a new database is given\n",
    "#         if database is not None:\n",
    "#             try:\n",
    "#                 config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "#                 engine = create_engine(config, pool_recycle=300)\n",
    "#             except:\n",
    "#                 logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "#                 return False\n",
    "#         else:\n",
    "#             engine = self.engine\n",
    "\n",
    "#         try:\n",
    "#             data = pd.read_sql(query, engine, **kwargs)\n",
    "#         except exc.ResourceClosedError:\n",
    "#             return True\n",
    "#         except:\n",
    "#             logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "#             params = kwargs['params'] if 'params' in kwargs else None\n",
    "#             return False\n",
    "        print('query', query)\n",
    "        if database is None:\n",
    "            database = 'karvy'\n",
    "        data = None\n",
    "        sql = query\n",
    "        user_ = self.USER\n",
    "        database = self.DATABASE\n",
    "        host_ = self.HOST\n",
    "        password_ = self.PASSWORD\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        # for pos, ind in enumerate(inds):\n",
    "        #     if pos % 2 == 0:\n",
    "        #         sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "        #     else:\n",
    "        #         sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "               \n",
    "        if connection_string:\n",
    "            print('connection string', connection_string)\n",
    "            print('database', database)\n",
    "            print(type(connection_string + database))\n",
    "            print(type(connection_string + database))\n",
    "\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + database)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pymssql.connect(host=host_,database=database,user=user_,password=password_)\n",
    "                else:\n",
    "                    conn = pymssql.connect(host=host_,database=database)\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pymssql.connect(host=host_,database=database)\n",
    "        try:\n",
    "            logging.debug(f'Query: {query}')\n",
    "            curs = conn.cursor(as_dict = True)\n",
    "            params = kwargs.get('params', [])\n",
    "            logging.debug(f'Params: {params}')\n",
    "            curs.execute(sql, params)\n",
    "            print('query executed')\n",
    "            try:\n",
    "                data = curs.fetchall()\n",
    "                data = pd.DataFrame(data)\n",
    "                print(data)\n",
    "            except Exception as e:\n",
    "                logging.debug('Update Query')\n",
    "\n",
    "            #data = pd.read_sql(sql, conn,**kwargs)\n",
    "        except exc.ResourceClosedError:\n",
    "            logging.warning('Query does not have any value to return.')\n",
    "            return True\n",
    "        except (exc.StatementError, OperationalError) as e:\n",
    "            logging.warning(f'Creating new connection. Engine/Connection is probably None. [{e}]')\n",
    "            self.connect()\n",
    "            data = pd.read_sql(query, conn,**kwargs)\n",
    "        except:\n",
    "            logging.exception('Something went wrong executing query. Check trace.')\n",
    "            params = kwargs['params'] if 'params' in kwargs else None\n",
    "            conn.rollback()\n",
    "            return False\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "#         return data.where((pd.notnull(data)), None)\n",
    "        try:\n",
    "            return data.replace({pd.np.nan: None}).set_index('id')\n",
    "        except AttributeError as e:\n",
    "            return True\n",
    "\n",
    "    def insert(self, data, table, **kwargs):\n",
    "        \"\"\"\n",
    "        Write records stored in a DataFrame to a SQL database.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The DataFrame that needs to be write to SQL database.\n",
    "            table (str): The table in which the rcords should be written to.\n",
    "            kwargs: Keyword arguments for pandas to_sql function.\n",
    "                See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html\n",
    "                to know the arguments that can be passed.\n",
    "\n",
    "        Returns:\n",
    "            (bool) True is succesfully inserted, else false.\n",
    "        \"\"\"\n",
    "        logging.info(f'Inserting into `{table}`')\n",
    "       \n",
    "        conn = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={self.HOST};UID={self.USER};PWD={self.PASSWORD};Trusted Connection=yes;DATABASE={self.DATABASE}'\n",
    "\n",
    "       \n",
    "#         conn =  \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=IP_ADDRESS;DATABASE=DataLake;UID=USER;PWD=PASS\"\n",
    "        quoted = quote_plus(conn)\n",
    "        new_con = 'mssql+pyodbc:///?odbc_connect={}'.format(quoted)\n",
    "        self.engine = create_engine(new_con)\n",
    "#         print(self.engine)\n",
    "        try:\n",
    "            data.to_sql(table, self.engine,chunksize = None, **kwargs)\n",
    "            try:\n",
    "                self.execute(f'ALTER TABLE `{table}` ADD PRIMARY KEY (`id`);')\n",
    "            except:\n",
    "                pass\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception('Something went wrong inserting. Check trace.')\n",
    "            return False\n",
    "\n",
    "   \n",
    "   \n",
    "    def insert_(self, data, table, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Write records stored in a DataFrame to a SQL database.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The DataFrame that needs to be write to SQL database.\n",
    "            table (str): The table in which the rcords should be written to.\n",
    "            database (str): The database the table lies in. Leave it none if you\n",
    "                want use database during object creation.\n",
    "            kwargs: Keyword arguments for pandas to_sql function.\n",
    "                See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html\n",
    "                to know the arguments that can be passed.\n",
    "\n",
    "        Returns:\n",
    "            (bool) True is succesfully inserted, else false.\n",
    "        \"\"\"\n",
    "        logging.info(f'Inserting into {table}')\n",
    "\n",
    "        # # Use new database if a new databse is given\n",
    "        # if database is not None:\n",
    "        #     try:\n",
    "        #         config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "        #         engine = create_engine(config, pool_recycle=300)\n",
    "        #     except:\n",
    "        #         logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "        #         return False\n",
    "        # else:\n",
    "        #     engine = self.engine\n",
    "\n",
    "        if database is None:\n",
    "            database = 'karvy'\n",
    "        data = None\n",
    "        sql = query\n",
    "        user_ = self.USER\n",
    "        database = self.DATABASE\n",
    "        host_ = self.HOST\n",
    "        password_ = self.PASSWORD\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        # for pos, ind in enumerate(inds):\n",
    "        #     if pos % 2 == 0:\n",
    "        #         sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "        #     else:\n",
    "        #         sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "               \n",
    "        if connection_string:\n",
    "            print('connection string', connection_string)\n",
    "            print('database', database)\n",
    "            print(type(connection_string + database))\n",
    "            print(type(connection_string + database))\n",
    "\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + database)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pymssql.connect(host=host_,database=database,user=user_,password=password_)\n",
    "                else:\n",
    "                    conn = pymssql.connect(host=host_,database=database)\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pymssql.connect(host=host_,database=database)\n",
    "        try:\n",
    "            logging.debug(f'Query: {query}')\n",
    "            # data.to_sql(table, conn, **kwargs)\n",
    "            curs = conn.cursor(as_dict = True)\n",
    "            curs.execute(sql, tuple(kwargs.get('params', [])))\n",
    "            print('query executed')\n",
    "            try:\n",
    "                data = curs.fetchall()\n",
    "                data = pd.DataFrame(data)\n",
    "                print(data)\n",
    "            except Exception as e:\n",
    "                logging.debug('Update Query')\n",
    "\n",
    "            try:\n",
    "                self.execute(f'ALTER TABLE {table} ADD PRIMARY KEY (id);')\n",
    "            except:\n",
    "                pass\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception('Something went wrong inserting. Check trace.')\n",
    "            return False\n",
    "\n",
    "    def insert_dict(self, data, table):\n",
    "        \"\"\"\n",
    "        Insert dictionary into a SQL database table.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The DataFrame that needs to be write to SQL database.\n",
    "            table (str): The table in which the rcords should be written to.\n",
    "\n",
    "        Returns:\n",
    "            (bool) True is succesfully inserted, else false.\n",
    "        \"\"\"\n",
    "        logging.info(f'Inserting dictionary data into {table}...')\n",
    "        logging.debug(f'Data:\\n{data}')\n",
    "\n",
    "        try:\n",
    "            column_names = []\n",
    "            params = []\n",
    "\n",
    "            for column_name, value in data.items():\n",
    "                column_names.append(f'{column_name}')\n",
    "                params.append(value)\n",
    "\n",
    "            logging.debug(f'Column names: {column_names}')\n",
    "            logging.debug(f'Params: {params}')\n",
    "\n",
    "            columns_string = ', '.join(column_names)\n",
    "            param_placeholders = ', '.join(['%s'] * len(column_names))\n",
    "\n",
    "            query = f'INSERT INTO {table} ({columns_string}) VALUES ({param_placeholders})'\n",
    "\n",
    "            return self.execute(query, params=params)\n",
    "        except:\n",
    "            logging.exception('Error inserting data.')\n",
    "            return False\n",
    "\n",
    "    def update(self, table, update=None, where=None, database=None, force_update=False):\n",
    "        # Use new database if a new databse is given\n",
    "        # if database is not None:\n",
    "        #     try:\n",
    "        #         config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "        #         self.engine = create_engine(config, pool_recycle=300)\n",
    "        #     except:\n",
    "        #         logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "        #         return False\n",
    "\n",
    "        logging.info(f'Updating table: {table}')\n",
    "        logging.info(f'Update data: {update}')\n",
    "        logging.info(f'Where clause data: {where}')\n",
    "        logging.info(f'Force update flag: {force_update}')\n",
    "\n",
    "        try:\n",
    "            set_clause = []\n",
    "            set_value_list = []\n",
    "            where_clause = []\n",
    "            where_value_list = []\n",
    "\n",
    "            if where is not None and where:\n",
    "                for set_column, set_value in update.items():\n",
    "                    set_clause.append(f'{set_column}=%s')\n",
    "                    set_value_list.append(set_value)\n",
    "                set_clause_string = ', '.join(set_clause)\n",
    "            else:\n",
    "                logging.error(f'Update dictionary is None/empty. Must have some update clause.')\n",
    "                return False\n",
    "\n",
    "            if where is not None and where:\n",
    "                for where_column, where_value in where.items():\n",
    "                    where_clause.append(f'{where_column}=%s')\n",
    "                    where_value_list.append(where_value)\n",
    "                where_clause_string = ' AND '.join(where_clause)\n",
    "                query = f'UPDATE {table} SET {set_clause_string} WHERE {where_clause_string}'\n",
    "            else:\n",
    "                if force_update:\n",
    "                    query = f'UPDATE {table} SET {set_clause_string}'\n",
    "                else:\n",
    "                    message = 'Where dictionary is None/empty. If you want to force update every row, pass force_update as True.'\n",
    "                    logging.error(message)\n",
    "                    return False\n",
    "\n",
    "            params = set_value_list + where_value_list\n",
    "            self.execute(query, params=params)\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception('Something went wrong updating. Check trace.')\n",
    "            return False\n",
    "\n",
    "    def get_column_names(self, table, database=None):\n",
    "        \"\"\"\n",
    "        Get all column names from an SQL table.\n",
    "\n",
    "        Args:\n",
    "            table (str): Name of the table from which column names should be extracted.\n",
    "            database (str): Name of the database in which the table lies. Leave\n",
    "                it none if you want use database during object creation.\n",
    "\n",
    "        Returns:\n",
    "            (list) List of headers. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(f'Getting column names of table {table}')\n",
    "            return list(self.execute(f'SELECT * FROM {table}', database))\n",
    "        except:\n",
    "            logging.exception('Something went wrong getting column names. Check trace.')\n",
    "            return\n",
    "\n",
    "    def execute_default_index(self, query, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Executes an SQL query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that needs to be executed.\n",
    "            database (str): Name of the database to execute the query in. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            params (list/tuple/dict): List of parameters to pass to in the query.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data from the executed\n",
    "            query. (None if an error occurs)\n",
    "        \"\"\"\n",
    "\n",
    "        logging.debug(f'Executing `execute` instead of `execute_default_index`')\n",
    "        return self.execute(query, index_col=None, **kwargs)\n",
    "        data = None\n",
    "\n",
    "        # # Use new database if a new databse is given\n",
    "        # if database is not None:\n",
    "        #     try:\n",
    "        #         config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "        #         engine = create_engine(config, pool_recycle=300)\n",
    "        #     except:\n",
    "        #         logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "        #         return False\n",
    "        # else:\n",
    "        #     engine = self.engine\n",
    "\n",
    "        print('query', query)\n",
    "        if database is None:\n",
    "            database = 'karvy'\n",
    "        data = None\n",
    "        sql = query\n",
    "        user_ = self.USER\n",
    "        host_ = self.HOST\n",
    "        database = self.DATABASE\n",
    "        password_ = self.PASSWORD\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        for pos, ind in enumerate(inds):\n",
    "            if pos % 2 == 0:\n",
    "                sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "            else:\n",
    "                sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "               \n",
    "        if connection_string:\n",
    "            print('connection string', connection_string)\n",
    "            print('database', database)\n",
    "            print(type(connection_string + database))\n",
    "            print(type(connection_string + database))\n",
    "\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + database)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pymssql.connect(host=host_,database=database,user=user_,password=password_)\n",
    "                else:\n",
    "                    conn = pymssql.connect(host=host_,database=database)\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pymssql.connect(host=host_,database=database)\n",
    "\n",
    "        try:\n",
    "            logging.debug(f'Query: {query}')\n",
    "            # data.to_sql(table, conn, **kwargs)\n",
    "            curs = conn.cursor(as_dict = True)\n",
    "           \n",
    "            curs.execute(sql, tuple(kwargs.get('params', [])))\n",
    "            print('query executed')\n",
    "            try:\n",
    "                data = curs.fetchall()\n",
    "                data = pd.DataFrame(data)\n",
    "                print(data)\n",
    "            except Exception as e:\n",
    "                logging.debug('Update Query')\n",
    "            # data = pd.read_sql(query, conn, **kwargs)\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "        except exc.ResourceClosedError:\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception(f'Something went wrong while executing query. Check trace.')\n",
    "            params = kwargs['params'] if 'params' in kwargs else None\n",
    "            return False\n",
    "\n",
    "        return data.where((pd.notnull(data)), None).set_index('id')\n",
    "\n",
    "\n",
    "    def get_all(self, table, database=None, discard=None):\n",
    "        \"\"\"\n",
    "        Get all data from an SQL table.\n",
    "\n",
    "        Args:\n",
    "            table (str): Name of the table from which data should be extracted.\n",
    "            database (str): Name of the database in which the table lies. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            discard (list): columns to be excluded while selecting all\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data. (None if an error\n",
    "            occurs)\n",
    "        \"\"\"\n",
    "        logging.info(f'Getting all data from {table}')\n",
    "        if discard:\n",
    "            logging.info(f'Discarding columns {discard}')\n",
    "            columns = list(self.execute_default_index(f'SHOW COLUMNS FROM {table}',database).Field)\n",
    "            columns = [col for col in columns if col not in discard]\n",
    "            columns_str = json.dumps(columns).replace(\"'\",'').replace('\"','')[1:-1]\n",
    "            return self.execute(f'SELECT {columns_str} FROM {table}', database)\n",
    "\n",
    "        return self.execute(f'SELECT * FROM {table}', database)\n",
    "\n",
    "    def get_latest(self, data, group_by_col, sort_col):\n",
    "        \"\"\"\n",
    "        Group data by a column containing repeated values and get latest from it by\n",
    "        taking the latest value based on another column.\n",
    "\n",
    "        Example:\n",
    "        Get the latest products\n",
    "            id     product   date\n",
    "            220    6647     2014-09-01\n",
    "            220    6647     2014-10-16\n",
    "            826    3380     2014-11-11\n",
    "            826    3380     2015-05-19\n",
    "            901    4555     2014-09-01\n",
    "            901    4555     2014-11-01\n",
    "\n",
    "        The function will return\n",
    "            id     product   date\n",
    "            220    6647     2014-10-16\n",
    "            826    3380     2015-05-19\n",
    "            901    4555     2014-11-01\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): Pandas DataFrame to query on.\n",
    "            group_by_col (str): Column containing repeated values.\n",
    "            sort_col (str): Column to identify the latest record.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) Contains the latest records. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info('Grouping data...')\n",
    "            logging.info(f'Data: {data}')\n",
    "            logging.info(f'Group by column: {group_by_col}')\n",
    "            logging.info(f'Sort column: {sort_col}')\n",
    "            return data.sort_values(sort_col).set_index('id').groupby(group_by_col).tail(1)\n",
    "        except KeyError as e:\n",
    "            logging.error(f'Column {e.args[0]} does not exist.')\n",
    "            return None\n",
    "        except:\n",
    "            logging.exception('Something went wrong while grouping data.')\n",
    "            return None\n",
    "\n",
    "db_config = {\n",
    "   'host': '13.233.100.20',\n",
    "   'port': '1433',\n",
    "   'user': 'SA',\n",
    "   'password':'Akhil@Akhil1'\n",
    "}\n",
    "import os\n",
    "os.environ['HOST_IP'] = '13.233.100.20'\n",
    "os.environ['LOCAL_DB_USER']='SA'\n",
    "os.environ['LOCAL_DB_PASSWORD'] = 'Akhil@Akhil1'\n",
    "os.environ['LOCAL_DB_PORT'] = '1433'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:12.910490Z",
     "start_time": "2020-08-25T08:31:12.898964Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def debug_df(df, num=20):\n",
    "    df.printSchema()\n",
    "    df.show(num)\n",
    "    \n",
    "\n",
    "def decrease_date(s, days):\n",
    "    date = datetime.datetime.strptime(s, \"%Y-%m-%d\")\n",
    "    modified_date = date - datetime.timedelta(days=days)\n",
    "    return datetime.datetime.strftime(modified_date, \"%Y-%m-%d\")\n",
    "\n",
    "def read_df(table, columns_to_retrieve, database):\n",
    "    \n",
    "    query = f\"SELECT {','.join(columns_to_retrieve)} from {table}\"\n",
    "    # logging.info(f\"query to be executed is {query}\")\n",
    "    \n",
    "    data = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "            .option(\"query\", query) \\\n",
    "            .option(\"user\", user) \\\n",
    "            .option(\"password\", password) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .load()\n",
    "    return data\n",
    "\n",
    "from datetime import timedelta, date\n",
    "#\n",
    "def daterange(start_date, end_date):\n",
    "    start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield datetime.datetime.strftime(start_date + timedelta(n), \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:13.385413Z",
     "start_time": "2020-08-25T08:31:13.347638Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "# required libraries\n",
    "from pyspark import SparkContext, SparkConf #\n",
    "from pyspark.sql import SparkSession # for dataframe conversions\n",
    "# for type conversions\n",
    "from datetime import datetime\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, udf, sum # col, udf (user defined functions)\n",
    "from pyspark.sql.types import DateType, IntegerType # type\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import trim # for trimming\n",
    "from pyspark.sql.functions import collect_list, sort_array, row_number # for grouping and taking the last/first element\n",
    "from pyspark.sql.functions import *\n",
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import datetime \n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "# intialize spark\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 2*multiprocessing.cpu_count())\n",
    " \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "def populate_db_func(purred: pd.Series, units: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(pd.Series(np.where(purred=='P', units, 0)))\n",
    "\n",
    "def populate_cr_func(purred: pd.Series, units: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(pd.Series(np.where(purred=='R', units, 0)))\n",
    "\n",
    "populate_db = pandas_udf(populate_db_func, returnType=FloatType())\n",
    "populate_cr = pandas_udf(populate_cr_func, returnType=FloatType())    \n",
    "    \n",
    "#Create spark context and sparksession\n",
    "\n",
    "SparkContext.setSystemProperty(\"spark.driver.memory\", \"60g\")\n",
    "SparkContext.setSystemProperty(\"spark.executor.memory\", \"60g\")\n",
    "SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"200g\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "(2*multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:13.732129Z",
     "start_time": "2020-08-25T08:31:13.722136Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# configs\n",
    "import os\n",
    "os.environ['HOST_IP'] = '192.168.15.126'\n",
    "os.environ['LOCAL_DB_USER'] = 'BRS'\n",
    "os.environ['LOCAL_DB_PASSWORD'] = 'Kfintech123$'\n",
    "os.environ['LOCAL_DB_PORT'] = '1433'\n",
    "\n",
    "import os\n",
    "\n",
    "# comment when using the configs from env file\n",
    "default_ip = '13.233.100.20'\n",
    "default_user  = 'SA'\n",
    "default_password = 'Akhil@Akhil1'\n",
    "default_port = '1433'\n",
    "default_tenant_id = 'karvy'\n",
    "\n",
    "# # initializations \n",
    "server = os.environ.get('HOST_IP', default_ip)\n",
    "port = os.environ.get('LOCAL_DB_PORT', default_port)\n",
    "user = os.environ.get('LOCAL_DB_USER', default_user)\n",
    "password = os.environ.get('LOCAL_DB_PASSWORD', default_password)\n",
    "\n",
    "db_config = {\n",
    "   'host': server,\n",
    "   'port': port,\n",
    "   'user': user,\n",
    "   'password':password\n",
    "}\n",
    "\n",
    "\n",
    "def save_metric(date, metric_name, metric_value, fund_name, group_level, table_name, database='IB_Comp_funds'):\n",
    "    db = DB(database, tenant_id='',**db_config)\n",
    "    try:\n",
    "        query = f\"INSERT INTO `karvy_metrics` VALUES ( '{date}','{metric_name}','{metric_value}', '{table_name}', '{group_level}', '{fund_name}')\"\n",
    "        db.execute_(query)\n",
    "    except Exception as e:\n",
    "        logging.error(\"Unable to insert metrics data\")\n",
    "        logging.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:14.750085Z",
     "start_time": "2020-08-25T08:31:14.723074Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 0 ns, total: 8 µs\n",
      "Wall time: 15.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def initialize(date_str, table, database='funds', date_column='BatchCloseDate', tenant_id='karvy',\n",
    "              transaction_status='Active', purred = 'Purred', transaction_type = 'TransactionType',\n",
    "              folio = 'Folio', purchase_units = 'DB_Units', redemption_units = 'Cr_Units',scheme = 'SchemeCode',\n",
    "               plan = 'PlanCode',groupby_level='SP',ter_flag='TerFlag', direct_db=None):\n",
    "    \"\"\"Initialization script which takes in batch_close_date and populates opening balance\n",
    "    Args:\n",
    "        date(str) The date upto which we need to initialize\n",
    "        table(str) \n",
    "        date_column(str)\n",
    "        tenant_id(str)\n",
    "    \n",
    "    Note: batch_close_date is (yyyy-MM-dd) (for ex. 2020-04-30), inclusive \n",
    "    \"\"\"\n",
    "    # fundtable column names\n",
    "    purchase_units = 'DB_Units'\n",
    "    redemption_units = 'Cr_Units'\n",
    "    scheme = 'SchemeCode'\n",
    "    plan = 'PlanCode'\n",
    "    \n",
    "    # configurations we use\n",
    "    batch_close_date = 'batch_close_date'\n",
    "    db_units = 'purchase_units'\n",
    "    cr_units = 'redemption_units'\n",
    "    balance_units = 'balance_units'\n",
    "    day_purchase_units = 'day_pu'\n",
    "    day_redemption_units = 'day_ru'\n",
    "    balance_purchase_units = 'balance_pu'\n",
    "    balance_redemption_units = 'balance_ru'\n",
    "    calculated_date = 'calculated_date'\n",
    "    \n",
    "#     inflow_db_units = 'inflow_purchase_units'\n",
    "#     inflow_cr_units = 'inflow_redemption_units'\n",
    "#     inflow_balance_units = 'inflow_balance_units'\n",
    "#     inflow_day_purchase_units = 'inflow_day_pu'\n",
    "#     inflow_day_redemption_units = 'inflow_day_ru'\n",
    "#     inflow_balance_purchase_units = 'inflow_balance_pu'\n",
    "#     inflow_balance_redemption_units = 'inflow_balance_ru'\n",
    "\n",
    "    \n",
    "#     outflow_db_units = 'outflow_purchase_units'\n",
    "#     outflow_cr_units = 'outflow_redemption_units'\n",
    "#     outlfow_balance_units = 'outlfow_balance_units'\n",
    "#     outlfow_day_purchase_units = 'outlfow_day_pu'\n",
    "#     outlfow_day_redemption_units = 'outlfow_day_ru'\n",
    "#     outlfow_balance_purchase_units = 'outlfow_balance_pu'\n",
    "#     outlfow_balance_redemption_units = 'outlfow_balance_ru'\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    database = direct_db or (f'{tenant_id or default_tenant_id}_{database}')\n",
    "    \n",
    "\n",
    "    # read data\n",
    "    data = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .load()\n",
    "#     data = data.filter(col(scheme) == 'TF')\n",
    "    \n",
    "#     data = data.cache()\n",
    "    total_count = data.count()\n",
    "    # debug_df(data, 10)\n",
    "    \n",
    "    # some preprocessings in the data, additional trimmings etc\n",
    "    data = data.withColumn(transaction_status, upper(trim(col(transaction_status))))\n",
    "    data = data.withColumn(purred, upper(trim(col(purred))))\n",
    "    data = data.withColumn(folio, upper(trim(col(folio))))\n",
    "    data = data.withColumn(scheme, upper(trim(col(scheme))))\n",
    "    data = data.withColumn(plan, upper(trim(col(plan))))\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "#     data = data.filter((col(scheme).isin('IC','HC')))\n",
    "    \n",
    "#     print ('here')\n",
    "    \n",
    "    # cast the date column into dates, as we are concerned only with dates now\n",
    "    data = data.withColumn(date_column, col(date_column).cast('date'))\n",
    "    \n",
    "    # filter the date till the batch_close_date (inclusive)\n",
    "    data = data.filter(col(date_column) <= date_str)\n",
    "        \n",
    "    # filter the data according to rules\n",
    "    data = data.filter((col(date_column).isNotNull()) )\n",
    "    # data = data.filter((col(date_column) != '') ) # this will not work for few types\n",
    "    data = data.filter( ~trim(col(date_column)).cast(\"string\").eqNullSafe(''))\n",
    "    \n",
    "    # do must be rules\n",
    "    data = data.filter( (trim(upper((col(transaction_status))))) == \"Y\")\n",
    "    data = data.filter( (trim(upper(col(purred))) == \"P\") | (trim(upper(col(purred))) == \"R\") )\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "    # our configurations\n",
    "    data = data.withColumn(batch_close_date, data[date_column])\n",
    "    \n",
    "#     print ('pur, redem')\n",
    "    # bring in purchase and redemption units\n",
    "    data = data.withColumn(db_units, when((col(purred) == \"P\"), col(purchase_units)).otherwise(0))\n",
    "    data = data.withColumn(cr_units, when((col(purred) == \"R\"), col(redemption_units)).otherwise(0))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#     print ('1')\n",
    "    # get the group by and window partitions based on partitions\n",
    "    group_by_cols = []\n",
    "    window_partition = []\n",
    "    if groupby_level == 'SP':\n",
    "        window_partition = [scheme, plan]\n",
    "        group_by_cols = [scheme, plan, batch_close_date]\n",
    "        # scheme_plan wise we might need to filter out some transaction types\n",
    "        ignored_tr_types = ['CNI', 'CNO', 'TRMI', \n",
    "                            'TRMO', 'TRFI', 'TRFO', 'PLDO',\n",
    "                            'UPLO', 'DMT', 'RMT', 'CNIR', 'CNOR', 'TRMIR', 'TRMOR',\n",
    "                            'TRFIR', 'TRFOR', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR']\n",
    "        data = data.filter( ~(col(transaction_type).isin(ignored_tr_types)) )\n",
    "    elif groupby_level == 'SPT':\n",
    "        window_partition = [scheme, plan, transaction_type]\n",
    "        group_by_cols  = [scheme, plan, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPF':\n",
    "        window_partition = [scheme, plan, folio]\n",
    "        group_by_cols  = [scheme, plan, folio, batch_close_date]\n",
    "    elif groupby_level == 'SPFB':\n",
    "            window_partition = [scheme, plan, folio, broker]\n",
    "            group_by_cols  = [scheme, plan, folio, broker, batch_close_date]\n",
    "    elif groupby_level == 'SPFT':\n",
    "        window_partition = [scheme, plan, folio, transaction_type]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPFTTer':\n",
    "        window_partition = [scheme, plan, folio, transaction_type, ter_flag]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, ter_flag, batch_close_date]\n",
    "    elif groupby_level == 'SPFTB':\n",
    "        window_partition = [scheme, plan, folio, transaction_type, broker]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, broker, batch_close_date]\n",
    "        \n",
    "        # populate broker code\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # roll up the data\n",
    "    rolledup_data = data.groupBy(group_by_cols)\n",
    "    rolledup_data = rolledup_data.agg({db_units:'sum', cr_units:'sum'})\n",
    "        \n",
    "    rolledup_data = rolledup_data.withColumnRenamed(f\"sum({db_units})\", day_purchase_units).withColumnRenamed(f\"sum({cr_units})\", day_redemption_units)\n",
    "    rolledup_data = rolledup_data.withColumn(balance_purchase_units, sum(col(day_purchase_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    rolledup_data = rolledup_data.withColumn(balance_redemption_units, sum(col(day_redemption_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    rolledup_data = rolledup_data.withColumn(balance_units, (col(balance_purchase_units) - col(balance_redemption_units)))\n",
    "    \n",
    "#     print ('rolling up done')\n",
    "    # get the latest data\n",
    "    latest_data = rolledup_data.filter(col(batch_close_date) <= date_str)\n",
    "    w = Window.partitionBy(window_partition).orderBy(col(batch_close_date).desc())\n",
    "    latest_data = latest_data.withColumn(\"rrn\", row_number().over(w)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    \n",
    "    # maintained the calculated date (the latest data upto the calculated date)\n",
    "    latest_data = latest_data.withColumn(calculated_date, lit(date_str).cast('date'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # store in parquet file for optimization of space and only one file and multi partitions\n",
    "    # but write now store in csv and maintain date wise and colesce one\n",
    "    #latest_data = latest_data.cache()\n",
    "    \n",
    "    \n",
    "#     latest_data.show()\n",
    "    # latest_data.coalesce(1).write.csv(f\"{table}_latest/data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "    latest_data.write.parquet(f\"{table}_latest/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    print (f'inital file on date {date_str} written')\n",
    "    return total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 µs, sys: 0 ns, total: 16 µs\n",
      "Wall time: 23.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def initialize(date_str, table, database='funds', date_column='BatchCloseDate', tenant_id='karvy',\n",
    "              transaction_status='Active', purred = 'Purred', transaction_type = 'TransactionType',\n",
    "              folio = 'Folio', purchase_units = 'DB_Units', redemption_units = 'Cr_Units',scheme = 'SchemeCode',\n",
    "               plan = 'PlanCode',groupby_level='SP',ter_flag='TerFlag', direct_db=None,\n",
    "              broker_column='BrokerARN'):\n",
    "    \"\"\"Initialization script which takes in batch_close_date and populates opening balance\n",
    "    Args:\n",
    "        date(str) The date upto which we need to initialize\n",
    "        table(str) \n",
    "        date_column(str)\n",
    "        tenant_id(str)\n",
    "    \n",
    "    Note: batch_close_date is (yyyy-MM-dd) (for ex. 2020-04-30), inclusive \n",
    "    \"\"\"\n",
    "    # fundtable column names\n",
    "    purchase_units = 'DB_Units'\n",
    "    redemption_units = 'Cr_Units'\n",
    "    scheme = 'SchemeCode'\n",
    "    plan = 'PlanCode'\n",
    "    \n",
    "    # configurations we use\n",
    "    batch_close_date = 'batch_close_date'\n",
    "    db_units = 'purchase_units'\n",
    "    cr_units = 'redemption_units'\n",
    "    balance_units = 'balance_units'\n",
    "    day_purchase_units = 'day_pu'\n",
    "    day_redemption_units = 'day_ru'\n",
    "    balance_purchase_units = 'balance_pu'\n",
    "    balance_redemption_units = 'balance_ru'\n",
    "    calculated_date = 'calculated_date'\n",
    "    \n",
    "#     inflow_db_units = 'inflow_purchase_units'\n",
    "#     inflow_cr_units = 'inflow_redemption_units'\n",
    "#     inflow_balance_units = 'inflow_balance_units'\n",
    "#     inflow_day_purchase_units = 'inflow_day_pu'\n",
    "#     inflow_day_redemption_units = 'inflow_day_ru'\n",
    "#     inflow_balance_purchase_units = 'inflow_balance_pu'\n",
    "#     inflow_balance_redemption_units = 'inflow_balance_ru'\n",
    "\n",
    "    \n",
    "#     outflow_db_units = 'outflow_purchase_units'\n",
    "#     outflow_cr_units = 'outflow_redemption_units'\n",
    "#     outlfow_balance_units = 'outlfow_balance_units'\n",
    "#     outlfow_day_purchase_units = 'outlfow_day_pu'\n",
    "#     outlfow_day_redemption_units = 'outlfow_day_ru'\n",
    "#     outlfow_balance_purchase_units = 'outlfow_balance_pu'\n",
    "#     outlfow_balance_redemption_units = 'outlfow_balance_ru'\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    database = direct_db or (f'{tenant_id or default_tenant_id}_{database}')\n",
    "    \n",
    "\n",
    "    # read data\n",
    "    data = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .load()\n",
    "#     data = data.filter(col(scheme) == 'TF')\n",
    "    \n",
    "#     data = data.cache()\n",
    "    total_count = data.count()\n",
    "    # debug_df(data, 10)\n",
    "    \n",
    "    # some preprocessings in the data, additional trimmings etc\n",
    "    data = data.withColumn(transaction_status, upper(trim(col(transaction_status))))\n",
    "    data = data.withColumn(purred, upper(trim(col(purred))))\n",
    "    data = data.withColumn(folio, upper(trim(col(folio))))\n",
    "    data = data.withColumn(scheme, upper(trim(col(scheme))))\n",
    "    data = data.withColumn(plan, upper(trim(col(plan))))\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "#     data = data.filter((col(scheme).isin('IC','HC')))\n",
    "    \n",
    "#     print ('here')\n",
    "    \n",
    "    # cast the date column into dates, as we are concerned only with dates now\n",
    "    data = data.withColumn(date_column, col(date_column).cast('date'))\n",
    "    \n",
    "    # filter the date till the batch_close_date (inclusive)\n",
    "    data = data.filter(col(date_column) <= date_str)\n",
    "        \n",
    "    # filter the data according to rules\n",
    "    data = data.filter((col(date_column).isNotNull()) )\n",
    "    # data = data.filter((col(date_column) != '') ) # this will not work for few types\n",
    "    data = data.filter( ~trim(col(date_column)).cast(\"string\").eqNullSafe(''))\n",
    "    \n",
    "    # do must be rules\n",
    "    data = data.filter( (trim(upper((col(transaction_status))))) == \"Y\")\n",
    "    data = data.filter( (trim(upper(col(purred))) == \"P\") | (trim(upper(col(purred))) == \"R\") )\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "    # our configurations\n",
    "    data = data.withColumn(batch_close_date, data[date_column])\n",
    "    \n",
    "#     print ('pur, redem')\n",
    "    # bring in purchase and redemption units\n",
    "    data = data.withColumn(db_units, when((col(purred) == \"P\"), col(purchase_units)).otherwise(0))\n",
    "    data = data.withColumn(cr_units, when((col(purred) == \"R\"), col(redemption_units)).otherwise(0))\n",
    "#     data = data.withColumn(db_units, populate_db(col(purred), col(purchase_units)))\n",
    "#     data = data.withColumn(cr_units, populate_cr(col(purred), col(redemption_units)))\n",
    "\n",
    "    \n",
    "    \n",
    "#     print ('1')\n",
    "    # get the group by and window partitions based on partitions\n",
    "    group_by_cols = []\n",
    "    window_partition = []\n",
    "    if groupby_level == 'SP':\n",
    "        window_partition = [scheme, plan]\n",
    "        group_by_cols = [scheme, plan, batch_close_date]\n",
    "        # scheme_plan wise we might need to filter out some transaction types\n",
    "        ignored_tr_types = ['CNI', 'CNO', 'TRMI', \n",
    "                            'TRMO', 'TRFI', 'TRFO', 'PLDO',\n",
    "                            'UPLO', 'DMT', 'RMT', 'CNIR', 'CNOR', 'TRMIR', 'TRMOR',\n",
    "                            'TRFIR', 'TRFOR', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR']\n",
    "        data = data.filter( ~(col(transaction_type).isin(ignored_tr_types)) )\n",
    "    elif groupby_level == 'SPT':\n",
    "        window_partition = [scheme, plan, transaction_type]\n",
    "        group_by_cols  = [scheme, plan, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPF':\n",
    "        window_partition = [scheme, plan, folio]\n",
    "        group_by_cols  = [scheme, plan, folio, batch_close_date]\n",
    "    elif groupby_level == 'SPFB':\n",
    "            window_partition = [scheme, plan, folio, broker]\n",
    "            group_by_cols  = [scheme, plan, folio, broker, batch_close_date]\n",
    "    elif groupby_level == 'SPFT':\n",
    "        window_partition = [scheme, plan, folio, transaction_type]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPFTTer':\n",
    "        window_partition = [scheme, plan, folio, transaction_type, ter_flag]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, ter_flag, batch_close_date]\n",
    "    elif groupby_level == 'SPFTBTer':\n",
    "        window_partition = [scheme, plan, folio, transaction_type, ter_flag, broker]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, ter_flag,broker, batch_close_date]\n",
    "    elif groupby_level == 'SPFTB':\n",
    "        window_partition = [scheme, plan, folio, transaction_type, broker]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, broker, batch_close_date]\n",
    "        \n",
    "        # populate broker code\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # roll up the data\n",
    "    rolledup_data = data.groupBy(group_by_cols)\n",
    "    rolledup_data = rolledup_data.agg({db_units:'sum', cr_units:'sum'})\n",
    "        \n",
    "    rolledup_data = rolledup_data.withColumnRenamed(f\"sum({db_units})\", day_purchase_units).withColumnRenamed(f\"sum({cr_units})\", day_redemption_units)\n",
    "    rolledup_data = rolledup_data.withColumn(balance_purchase_units, sum(col(day_purchase_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    rolledup_data = rolledup_data.withColumn(balance_redemption_units, sum(col(day_redemption_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    rolledup_data = rolledup_data.withColumn(balance_units, (col(balance_purchase_units) - col(balance_redemption_units)))\n",
    "    \n",
    "#     print ('rolling up done')\n",
    "    # get the latest data\n",
    "    latest_data = rolledup_data.filter(col(batch_close_date) <= date_str)\n",
    "    w = Window.partitionBy(window_partition).orderBy(col(batch_close_date).desc())\n",
    "    latest_data = latest_data.withColumn(\"rrn\", row_number().over(w)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    \n",
    "    # maintained the calculated date (the latest data upto the calculated date)\n",
    "    latest_data = latest_data.withColumn(calculated_date, lit(date_str).cast('date'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # store in parquet file for optimization of space and only one file and multi partitions\n",
    "    # but write now store in csv and maintain date wise and colesce one\n",
    "    #latest_data = latest_data.cache()\n",
    "    \n",
    "    \n",
    "#     latest_data.show()\n",
    "    # latest_data.coalesce(1).write.csv(f\"{table}_latest/data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "    latest_data.write.parquet(f\"{table}_latest/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    print (f'inital file on date {date_str} written')\n",
    "    return total_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dialyjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:16.388873Z",
     "start_time": "2020-08-25T08:31:16.352684Z"
    },
    "code_folding": [
     0,
     119,
     126,
     129,
     132,
     141
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 µs, sys: 11 µs, total: 28 µs\n",
      "Wall time: 148 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "def dialy_job(date_str, table='trans116', database='funds', date_column='BatchCloseDate', tenant_id='karvy',\n",
    "              transaction_status='Active', purred = 'Purred', transaction_type = 'TransactionType',\n",
    "              folio = 'Folio', purchase_units = 'DB_Units', redemption_units = 'Cr_Units',scheme = 'SchemeCode',\n",
    "               plan = 'PlanCode',groupby_level='SP', direct_db=None, fn_fromdt = 'fn_fromdt',fn_fromdt_format = 'dd/MM/yyyy',\n",
    "              fn_scheme = 'fn_scheme',fn_plan = 'fn_plan', fn_nav = 'fn_nav', nav_table='nav_master', \n",
    "              scheme_table='scheme_master',scheme_code = 'scheme_code', \n",
    "              plan_code = 'plan_code', category = 'SebiSchemeCategory',\n",
    "              subcategory = 'SebiSchemeSubCategory',nature = 'nature', newmcrid='NewMCRId', ter_flag='TerFlag'\n",
    "             ):\n",
    "    \"\"\"Dialy run this and store the latest data and aum data too\"\"\"\n",
    "    \n",
    "    # inflow outflow\n",
    "    inflow_db_trtypes = ['NEW', 'ADD', 'IPO', 'SIN', 'LTIN', 'LTIA', 'STPN', 'STPA', 'STPI','DIR', 'DSPI', 'SWIN','SWIA']\n",
    "    inflow_cr_trtypes = ['NEWR', 'ADDR', 'IPOR', 'SINR', 'LTINR', 'LTIAR', 'STPNR', \n",
    "                         'STPAR', 'STPIR','DIRR', 'DSPIR', 'SWINR','SWIAR']\n",
    "    \n",
    "    outflow_db_trtypes = ['FULR', 'REDR', 'LTOFR', 'LTOPR', 'STPOR', 'SWDR', 'TRGR', 'SWOPR', 'SWOFR']\n",
    "    outflow_cr_trtypes = ['FUL', 'RED', 'SWD','TRG','LTOF', 'LTOP','STPO', 'SWOP', 'SWOF']\n",
    "    \n",
    "    \n",
    "    \n",
    "    # configurations we use\n",
    "    batch_close_date = 'batch_close_date'\n",
    "    db_units = 'purchase_units'\n",
    "    cr_units = 'redemption_units'\n",
    "    balance_units = 'balance_units'\n",
    "    day_purchase_units = 'day_pu'\n",
    "    day_redemption_units = 'day_ru'\n",
    "    balance_purchase_units = 'balance_pu'\n",
    "    balance_redemption_units = 'balance_ru'\n",
    "    calculated_date = 'calculated_date'\n",
    "    today_pu = 'today_pu'\n",
    "    today_ru = 'today_ru'\n",
    "    effective_nav = 'effective_nav'\n",
    "    aum = 'aum'\n",
    "    aaum = 'aaum'\n",
    "    inflow = 'inflow'\n",
    "    outflow = 'outflow'\n",
    "    inflow_db_units = 'inflow_purchase_units'\n",
    "    inflow_cr_units = 'inflow_redemption_units'\n",
    "    outflow_db_units = 'outflow_purchase_units'\n",
    "    outflow_cr_units = 'outflow_redemption_units'\n",
    "    inflow_units = 'inflow_units'\n",
    "    outflow_units = 'outflow_units'\n",
    "    \n",
    "    # get the latest data from the previously stored file\n",
    "    date_obj = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    day_num = date_obj.day\n",
    "    previous_day = date_obj - datetime.timedelta(1)\n",
    "    previous_day_str = previous_day.strftime('%Y-%m-%d')\n",
    "    latest_data = spark.read.parquet(f\"{table}_latest/data_{groupby_level}_{previous_day_str}.parquet\")\n",
    "\n",
    "    # debug_df(latest_data)\n",
    "    \n",
    "    # get  the todays data\n",
    "    database = direct_db or (f'{tenant_id or default_tenant_id}_{database}')\n",
    "    query = f\"SELECT * from {table} where CAST({date_column} AS DATE)='{date_str}'\"\n",
    "    data  = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "            .option(\"query\", query) \\\n",
    "            .option(\"user\", user) \\\n",
    "            .option(\"password\", password) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .load()\n",
    "#     data = data.filter(col(scheme) == 'TF')\n",
    "    \n",
    "    data = data.cache()\n",
    "    \n",
    "#     data = data.filter((col(scheme)=='IC'))\n",
    "    # latest_data = latest_data.filter((col(scheme) == 'OV') & (col(plan) == 'RG'))\n",
    "    # data = data.filter((col(scheme) == 'OV') & (col(plan) == 'RG'))\n",
    "    # print (data.count())\n",
    "    # debug_df(data)\n",
    "    \n",
    "    # calculate all the steps as in initialization\n",
    "    # some preprocessings in the data, additional trimmings etc\n",
    "    data = data.withColumn(transaction_status, upper(trim(col(transaction_status))))\n",
    "    data = data.withColumn(purred, upper(trim(col(purred))))\n",
    "    data = data.withColumn(folio, upper(trim(col(folio))))\n",
    "    data = data.withColumn(scheme, upper(trim(col(scheme))))\n",
    "    data = data.withColumn(plan, upper(trim(col(plan))))\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "    \n",
    "    day_count = data.count()\n",
    "#     data = data.filter((col(scheme).isin('IC','HC')))\n",
    "    \n",
    "\n",
    "    # cast the date column into dates, as we are concerned only with dates now\n",
    "    data = data.withColumn(date_column, col(date_column).cast('date'))\n",
    "    \n",
    "   \n",
    "    # filter the date till the batch_close_date (inclusive)\n",
    "    # data = data.filter(col(date_column) <= date_str)\n",
    "    \n",
    "        \n",
    "    # filter the data according to rules\n",
    "    data = data.filter((col(date_column).isNotNull()) )\n",
    "    # data = data.filter((col(date_column) != '') ) # this will not work for few types\n",
    "    data = data.filter( ~trim(col(date_column)).cast(\"string\").eqNullSafe(''))\n",
    "    \n",
    "    # do must be rules\n",
    "    data = data.filter( (trim(upper((col(transaction_status))))) == \"Y\")\n",
    "    data = data.filter( (trim(upper(col(purred))) == \"P\") | (trim(upper(col(purred))) == \"R\") )\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "    # our configurations\n",
    "    data = data.withColumn(batch_close_date, data[date_column])\n",
    "    \n",
    "#     data  =data.repartition(16, scheme, plan, folio, transaction_type, ter_flag, batch_close_date)\n",
    "        \n",
    "    # bring in purchase and redemption units\n",
    "    data = data.withColumn(db_units, when((col(purred) == \"P\"), col(purchase_units)).otherwise(0))\n",
    "    data = data.withColumn(cr_units, when((col(purred) == \"R\"), col(redemption_units)).otherwise(0))\n",
    "    \n",
    "    group_by_cols = []\n",
    "    window_partition = []\n",
    "    if groupby_level == 'SP':\n",
    "        window_partition = [scheme, plan]\n",
    "        group_by_cols = [scheme, plan, batch_close_date]\n",
    "        # scheme_plan wise we might need to filter out some transaction types\n",
    "        ignored_tr_types = ['CNI', 'CNO', 'TRMI', 'TRMO', 'TRFI', 'TRFO', 'PLDO', 'UPLO',\n",
    "                            'DMT', 'RMT', 'CNIR', 'CNOR', 'TRMIR', 'TRMOR', 'TRFIR', 'TRFOR', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR']\n",
    "        data = data.filter( ~(col(transaction_type).isin(ignored_tr_types)) )\n",
    "    elif groupby_level == 'SPT':\n",
    "        window_partition = [scheme, plan, transaction_type]\n",
    "        group_by_cols  = [scheme, plan, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPF':\n",
    "        window_partition = [scheme, plan, folio]\n",
    "        group_by_cols  = [scheme, plan, folio, batch_close_date]\n",
    "    elif groupby_level == 'SPFB':\n",
    "            window_partition = [scheme, plan, folio, broker]\n",
    "            group_by_cols  = [scheme, plan, folio, broker, batch_close_date]\n",
    "    elif groupby_level == 'SPFT':\n",
    "        window_partition = [scheme, plan, folio, transaction_type]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPFTTer':\n",
    "        window_partition = [scheme, plan, folio, transaction_type, ter_flag]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, ter_flag, batch_close_date]\n",
    "    elif groupby_level == 'SPFTB':\n",
    "        window_partition = [scheme, plan, folio, transaction_type, broker]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, broker, batch_close_date]\n",
    "    \n",
    "    # roll up the data\n",
    "    rolledup_data = data.groupBy(group_by_cols)\n",
    "    rolledup_data = rolledup_data.agg({db_units:'sum', cr_units:'sum'})\n",
    "        \n",
    "    rolledup_data = rolledup_data.withColumnRenamed(f\"sum({db_units})\", day_purchase_units).withColumnRenamed(f\"sum({cr_units})\", day_redemption_units)\n",
    "\n",
    "    # inflow outflow units\n",
    "    \n",
    "    \n",
    "    latest_data = latest_data.drop(day_purchase_units, day_redemption_units, batch_close_date, balance_units)\n",
    "    latest_data = latest_data.withColumnRenamed(balance_purchase_units, day_purchase_units)\n",
    "    latest_data = latest_data.withColumnRenamed(balance_redemption_units, day_redemption_units)\n",
    "    latest_data = latest_data.withColumnRenamed(calculated_date, batch_close_date)\n",
    "    \n",
    "    # debug_df(rolledup_data)\n",
    "    # debug_df(latest_data)\n",
    "    combined_data = latest_data.union(rolledup_data.select(latest_data.columns))\n",
    "    combined_data = combined_data.cache()\n",
    "    combined_count = combined_data.count()\n",
    "    # debug_df(rolledup_data)\n",
    "    # debug_df(latest_data)\n",
    "    # debug_df(combined_data)\n",
    "    combined_data = combined_data.withColumn(balance_purchase_units, sum(col(day_purchase_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    combined_data = combined_data.withColumn(balance_redemption_units, sum(col(day_redemption_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    combined_data = combined_data.withColumn(balance_units, (col(balance_purchase_units) - col(balance_redemption_units)))\n",
    "    # debug_df(combined_data)\n",
    "    \n",
    "    \n",
    "    # store the latest day data again\n",
    "    # get the latest data\n",
    "    combined_data = combined_data.filter(col(batch_close_date) <= date_str)\n",
    "    w = Window.partitionBy(window_partition).orderBy(col(batch_close_date).desc())\n",
    "    combined_data = combined_data.withColumn(\"rrn\", row_number().over(w)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    \n",
    "    \n",
    "    # maintained the calculated date (the latest data upto the calculated date)\n",
    "    combined_data = combined_data.withColumn(calculated_date, lit(date_str).cast('date'))\n",
    "    \n",
    "    # debug_df(combined_data)\n",
    "    # store in parquet file for optimization of space and only one file and multi partitions\n",
    "    # but write now store in csv and maintain date wise and colesce one\n",
    "    combined_data = combined_data.cache()\n",
    "    # combined_data.coalesce(1).write.csv(f\"{table}_latest/data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "    combined_data.write.parquet(f\"{table}_latest/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    \n",
    "    # join the nav, scheme_master data\n",
    "    nav_data = read_df(nav_table, '*', database)\n",
    "    nav_data = nav_data.withColumnRenamed(fn_scheme, scheme)\n",
    "    nav_data = nav_data.withColumnRenamed(fn_plan, plan)\n",
    "    nav_data = nav_data.withColumn(fn_fromdt, col(fn_fromdt).cast('date'))\n",
    "\n",
    "    scheme_master = read_df(scheme_table, [scheme_code, plan_code, nature, category, subcategory, newmcrid], database)\n",
    "    scheme_master = scheme_master.withColumnRenamed(scheme_code, scheme)\n",
    "    scheme_master = scheme_master.withColumnRenamed(plan_code, plan)\n",
    "\n",
    "    scheme_master_ = scheme_master.dropDuplicates([scheme, plan])\n",
    "    nav_scheme = nav_data.join(scheme_master_, on=[scheme, plan], how='left')\n",
    "    nav_data = nav_scheme\n",
    "    \n",
    "    # debug_df(nav_data)\n",
    "    \n",
    "    # calculate the aum \n",
    "    combined_data = combined_data.withColumn(effective_nav, date_sub(col(calculated_date), 1))\n",
    "    combined_data = combined_data.withColumn(today_pu, when((col(calculated_date) == col(batch_close_date)), col(day_purchase_units)).otherwise(0))\n",
    "    combined_data = combined_data.withColumn(today_ru, when((col(calculated_date) == col(batch_close_date)), col(day_redemption_units)).otherwise(0))\n",
    "    \n",
    "    # inflow outflow addition\n",
    "    inflow_db_condition = col(transaction_type).isin(inflow_db_trtypes)\n",
    "    inflow_cr_condition = col(transaction_type).isin(inflow_cr_trtypes)\n",
    "    combined_data = combined_data.withColumn(inflow_db_units, when(inflow_db_condition, col(today_pu)).otherwise(0))\n",
    "    combined_data = combined_data.withColumn(inflow_cr_units, when(inflow_cr_condition, col(today_ru)).otherwise(0))\n",
    "    \n",
    "    outflow_db_condition = col(transaction_type).isin(outflow_db_trtypes)\n",
    "    outflow_cr_condition = col(transaction_type).isin(outflow_cr_trtypes)\n",
    "    combined_data = combined_data.withColumn(outflow_db_units, when(outflow_db_condition, col(today_pu)).otherwise(0))\n",
    "    combined_data = combined_data.withColumn(outflow_cr_units, when(outflow_cr_condition, col(today_ru)).otherwise(0))\n",
    "    \n",
    "    combined_data = combined_data.withColumn(inflow_units, col(inflow_db_units) - col(inflow_cr_units))\n",
    "    combined_data = combined_data.withColumn(outflow_units, col(outflow_cr_units) - col(outflow_db_units))\n",
    "    \n",
    "    # debug_df(combined_data)\n",
    "    \n",
    "    nav_filteredFT = nav_data.filter(col(fn_fromdt) < date_str)\n",
    "    navw = Window.partitionBy([scheme, plan]).orderBy(col(fn_fromdt).desc())\n",
    "    nav_populate = nav_filteredFT.withColumn(\"rrn\", row_number().over(navw)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    nav_populate = nav_populate.withColumn(calculated_date, lit(date_str))\n",
    "    nav_populate = nav_populate.select([scheme, plan, fn_fromdt, fn_nav, calculated_date, category, subcategory, nature, newmcrid])\n",
    "    \n",
    "#     debug_df(nav_populate)\n",
    "    \n",
    "    joined = combined_data.join(nav_populate, on=[scheme, plan, calculated_date], how='left')\n",
    "    joined = joined.withColumn(aum, col(fn_nav) * col(balance_units))\n",
    "    joined = joined.withColumn(inflow, col(fn_nav) * col(inflow_units))\n",
    "    joined = joined.withColumn(outflow, col(fn_nav) * col(outflow_units))\n",
    "    \n",
    "#     aum_dummy = f'{aum}_d'\n",
    "#     final_joined = joined.withColumn(aum_dummy, col(aum))\n",
    "#     final_joined = final_joined.fillna({aum_dummy: 0})\n",
    "    \n",
    "#     # moving average logic\n",
    "#     print (day_num)\n",
    "#     if day_num == 1:\n",
    "#         final_joined = final_joined.withColumn(f'pre_{aum_dummy}', lit(0))\n",
    "#         final_joined = final_joined.withColumn(aaum, col(aum_dummy))\n",
    "#     elif day_num == 2:\n",
    "#         final_joined = final_joined.withColumn(f'pre_{aum_dummy}', lit(0))\n",
    "#         final_joined = final_joined.withColumn(aaum, col(aum_dummy))\n",
    "#     else:\n",
    "#         previous_day_aum = spark.read.parquet(f\"{table}_dialy/aaum_data_{groupby_level}_{previous_day_str}.parquet\")\n",
    "#         previous_day_aum.withColumnRenamed(aum_dummy, f'pre_{aum_dummy}')\n",
    "#         final_joined = final_joined.join(previous_day_aum.select(window_partition + [f'pre_{aum_dummy}']), on=window_partition, how='left')\n",
    "#         final_joined = final_joined.fillna({f'pre_{aum_dummy}': 0})\n",
    "#         final_joined = final_joined.withColumn(aaum, (col(aum_dummy) + col(f'pre_{aum_dummy}') / day_num))\n",
    "        \n",
    "    \n",
    "#     debug_df(joined)\n",
    "    #joined = joined.cache()\n",
    "    # store the data in the files\n",
    "#     joined.coalesce(1).write.csv(f\"{table}_dialy/data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "    joined.write.parquet(f\"{table}_dialy/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "#     final_joined = final_joined.cache()\n",
    "#     final_joined.coalesce(1).write.csv(f\"{table}_dialy/aaum_data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "#     final_joined.coalesce(1).write.parquet(f\"{table}_dialy/aaum_data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    \n",
    "    # upload the data if needed\n",
    "    # Done\n",
    "    print (f'dialy file on date {date_str} generated')\n",
    "    return day_count, combined_count\n",
    "\n",
    "# dialy_job('2020-05-01', groupby_level='SPFT', table='Trans_116', direct_db='BankRecon', nav_table='fund_navreg_AXA_29072020',\n",
    "#         scheme_table='Fund_Master_AXA_29072020', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory'\n",
    "#         )\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='Trans_128', direct_db='BankRecon', nav_table='fund_navreg_axismf',\n",
    "#         scheme_table='fund_master_axismf', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory'\n",
    "#         )\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='Trans_120', direct_db='BankRecon', nav_table='fund_navreg_invesco',\n",
    "#         scheme_table='fund_master_INVESCO', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory'\n",
    "#         )\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='m_Trans_116', direct_db='BankRecon', nav_table='fund_navreg_AXA_29072020',\n",
    "#         scheme_table='Fund_Master_AXA_29072020', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#         )\n",
    "\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='m_Trans_117', direct_db='BankRecon', nav_table='fund_navreg_MIRAE',\n",
    "#         scheme_table='fund_master_MIRAE', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#         )\n",
    "# print ('done')\n",
    "# import time\n",
    "# for i,ele in enumerate(list(daterange('2020-05-01', '2020-06-02'))):\n",
    "#     s = time.time()\n",
    "#     dialy_job(ele, groupby_level='SPFT', table='m_Trans_117', direct_db='BankRecon', nav_table='fund_navreg_MIRAE',\n",
    "#         scheme_table='fund_master_MIRAE', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#         )\n",
    "#     generate_mcr_report(table=f'm_Trans_{code}', groupby_level='SP', start_date = '2020-05-02', end_date = '2020-06-02')\n",
    "\n",
    "#     print (i, ele, time.time() - s)\n",
    "\n",
    "# day_records, combined_records = dialy_job('2020-04-02', groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "#             scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#              category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 edelwwise\n",
      "inital file on date 2020-03-31 written\n",
      "initialization time 59.900768518447876\n",
      "dialy file on date 2020-04-01 generated\n",
      "     0 2020-04-01 15.705994606018066\n",
      "dialy file on date 2020-04-02 generated\n",
      "     1 2020-04-02 15.222989797592163\n",
      "dialy file on date 2020-04-03 generated\n",
      "     2 2020-04-03 15.330705881118774\n",
      "dialy file on date 2020-04-04 generated\n",
      "     3 2020-04-04 14.719802141189575\n",
      "dialy file on date 2020-04-05 generated\n",
      "     4 2020-04-05 15.081254720687866\n",
      "dialy file on date 2020-04-06 generated\n",
      "     5 2020-04-06 15.553968906402588\n",
      "dialy file on date 2020-04-07 generated\n",
      "     6 2020-04-07 15.012069702148438\n",
      "dialy file on date 2020-04-08 generated\n",
      "     7 2020-04-08 15.375812530517578\n",
      "dialy file on date 2020-04-09 generated\n",
      "     8 2020-04-09 15.738759517669678\n",
      "dialy file on date 2020-04-10 generated\n",
      "     9 2020-04-10 15.067438840866089\n",
      "dialy file on date 2020-04-11 generated\n",
      "     10 2020-04-11 15.436336517333984\n",
      "dialy file on date 2020-04-12 generated\n",
      "     11 2020-04-12 15.421719551086426\n",
      "dialy file on date 2020-04-13 generated\n",
      "     12 2020-04-13 15.500358581542969\n",
      "dialy file on date 2020-04-14 generated\n",
      "     13 2020-04-14 16.053019285202026\n",
      "dialy file on date 2020-04-15 generated\n",
      "     14 2020-04-15 15.464300632476807\n",
      "dialy file on date 2020-04-16 generated\n",
      "     15 2020-04-16 15.383479833602905\n",
      "dialy file on date 2020-04-17 generated\n",
      "     16 2020-04-17 15.219666242599487\n",
      "dialy file on date 2020-04-18 generated\n",
      "     17 2020-04-18 15.001702547073364\n",
      "dialy file on date 2020-04-19 generated\n",
      "     18 2020-04-19 15.44750189781189\n",
      "dialy file on date 2020-04-20 generated\n",
      "     19 2020-04-20 15.967334032058716\n",
      "dialy file on date 2020-04-21 generated\n",
      "     20 2020-04-21 15.730115413665771\n",
      "dialy file on date 2020-04-22 generated\n",
      "     21 2020-04-22 15.462830781936646\n",
      "dialy file on date 2020-04-23 generated\n",
      "     22 2020-04-23 15.390361309051514\n",
      "dialy file on date 2020-04-24 generated\n",
      "     23 2020-04-24 15.453703880310059\n",
      "dialy file on date 2020-04-25 generated\n",
      "     24 2020-04-25 15.58651876449585\n",
      "dialy file on date 2020-04-26 generated\n",
      "     25 2020-04-26 16.1680908203125\n",
      "dialy file on date 2020-04-27 generated\n",
      "     26 2020-04-27 15.39089035987854\n",
      "dialy file on date 2020-04-28 generated\n",
      "     27 2020-04-28 16.94195032119751\n",
      "dialy file on date 2020-04-29 generated\n",
      "     28 2020-04-29 16.322326183319092\n",
      "dialy file on date 2020-04-30 generated\n",
      "     29 2020-04-30 15.046555280685425\n",
      "dialy file on date 2020-05-01 generated\n",
      "     30 2020-05-01 14.870116949081421\n",
      "dialy file on date 2020-05-02 generated\n",
      "     31 2020-05-02 15.069049596786499\n",
      "dialy file on date 2020-05-03 generated\n",
      "     32 2020-05-03 15.093756198883057\n",
      "dialy file on date 2020-05-04 generated\n",
      "     33 2020-05-04 15.349344730377197\n",
      "dialy file on date 2020-05-05 generated\n",
      "     34 2020-05-05 15.949501276016235\n",
      "dialy file on date 2020-05-06 generated\n",
      "     35 2020-05-06 15.088637590408325\n",
      "dialy file on date 2020-05-07 generated\n",
      "     36 2020-05-07 15.214223146438599\n",
      "dialy file on date 2020-05-08 generated\n",
      "     37 2020-05-08 15.382406949996948\n",
      "dialy file on date 2020-05-09 generated\n",
      "     38 2020-05-09 15.243540525436401\n",
      "dialy file on date 2020-05-10 generated\n",
      "     39 2020-05-10 15.020896434783936\n",
      "dialy file on date 2020-05-11 generated\n",
      "     40 2020-05-11 15.13893723487854\n",
      "dialy file on date 2020-05-12 generated\n",
      "     41 2020-05-12 14.889230012893677\n",
      "dialy file on date 2020-05-13 generated\n",
      "     42 2020-05-13 14.439640760421753\n",
      "dialy file on date 2020-05-14 generated\n",
      "     43 2020-05-14 15.270938873291016\n",
      "dialy file on date 2020-05-15 generated\n",
      "     44 2020-05-15 14.872119665145874\n",
      "dialy file on date 2020-05-16 generated\n",
      "     45 2020-05-16 15.637377977371216\n",
      "dialy file on date 2020-05-17 generated\n",
      "     46 2020-05-17 15.0471932888031\n",
      "dialy file on date 2020-05-18 generated\n",
      "     47 2020-05-18 15.398845911026001\n",
      "dialy file on date 2020-05-19 generated\n",
      "     48 2020-05-19 15.056536674499512\n",
      "dialy file on date 2020-05-20 generated\n",
      "     49 2020-05-20 15.284440994262695\n",
      "dialy file on date 2020-05-21 generated\n",
      "     50 2020-05-21 14.95748782157898\n",
      "dialy file on date 2020-05-22 generated\n",
      "     51 2020-05-22 15.287336826324463\n",
      "dialy file on date 2020-05-23 generated\n",
      "     52 2020-05-23 14.938634395599365\n",
      "dialy file on date 2020-05-24 generated\n",
      "     53 2020-05-24 14.935384511947632\n",
      "dialy file on date 2020-05-25 generated\n",
      "     54 2020-05-25 14.891205310821533\n",
      "dialy file on date 2020-05-26 generated\n",
      "     55 2020-05-26 15.253556251525879\n",
      "dialy file on date 2020-05-27 generated\n",
      "     56 2020-05-27 15.300157308578491\n",
      "dialy file on date 2020-05-28 generated\n",
      "     57 2020-05-28 15.522139549255371\n",
      "dialy file on date 2020-05-29 generated\n",
      "     58 2020-05-29 14.793388605117798\n",
      "dialy file on date 2020-05-30 generated\n",
      "     59 2020-05-30 15.116476058959961\n",
      "dialy file on date 2020-05-31 generated\n",
      "     60 2020-05-31 15.414207935333252\n",
      "dialy file on date 2020-06-01 generated\n",
      "     61 2020-06-01 15.090269565582275\n",
      "dialy file on date 2020-06-02 generated\n",
      "     62 2020-06-02 14.822807550430298\n",
      "dialy file on date 2020-06-03 generated\n",
      "     63 2020-06-03 15.03306245803833\n",
      "dialy file on date 2020-06-04 generated\n",
      "     64 2020-06-04 15.051085233688354\n",
      "dialy file on date 2020-06-05 generated\n",
      "     65 2020-06-05 14.95242691040039\n",
      "dialy file on date 2020-06-06 generated\n",
      "     66 2020-06-06 15.208027601242065\n",
      "dialy file on date 2020-06-07 generated\n",
      "     67 2020-06-07 15.06515884399414\n",
      "dialy file on date 2020-06-08 generated\n",
      "     68 2020-06-08 15.72276759147644\n",
      "dialy file on date 2020-06-09 generated\n",
      "     69 2020-06-09 16.124143838882446\n",
      "dialy file on date 2020-06-10 generated\n",
      "     70 2020-06-10 15.525019645690918\n",
      "dialy file on date 2020-06-11 generated\n",
      "     71 2020-06-11 15.494408130645752\n",
      "dialy file on date 2020-06-12 generated\n",
      "     72 2020-06-12 15.774717807769775\n",
      "dialy file on date 2020-06-13 generated\n",
      "     73 2020-06-13 15.373399496078491\n",
      "dialy file on date 2020-06-14 generated\n",
      "     74 2020-06-14 15.592616081237793\n",
      "dialy file on date 2020-06-15 generated\n",
      "     75 2020-06-15 15.783306121826172\n",
      "dialy file on date 2020-06-16 generated\n",
      "     76 2020-06-16 16.41056752204895\n",
      "dialy file on date 2020-06-17 generated\n",
      "     77 2020-06-17 15.992809057235718\n",
      "dialy file on date 2020-06-18 generated\n",
      "     78 2020-06-18 15.284111022949219\n",
      "dialy file on date 2020-06-19 generated\n",
      "     79 2020-06-19 15.070485830307007\n",
      "dialy file on date 2020-06-20 generated\n",
      "     80 2020-06-20 15.442677021026611\n",
      "dialy file on date 2020-06-21 generated\n",
      "     81 2020-06-21 15.90329623222351\n",
      "dialy file on date 2020-06-22 generated\n",
      "     82 2020-06-22 15.63443398475647\n",
      "dialy file on date 2020-06-23 generated\n",
      "     83 2020-06-23 15.538334369659424\n",
      "dialy file on date 2020-06-24 generated\n",
      "     84 2020-06-24 15.727708578109741\n",
      "dialy file on date 2020-06-25 generated\n",
      "     85 2020-06-25 16.28801941871643\n",
      "dialy file on date 2020-06-26 generated\n",
      "     86 2020-06-26 15.758906364440918\n",
      "dialy file on date 2020-06-27 generated\n",
      "     87 2020-06-27 15.572789669036865\n",
      "dialy file on date 2020-06-28 generated\n",
      "     88 2020-06-28 15.055396795272827\n",
      "dialy file on date 2020-06-29 generated\n",
      "     89 2020-06-29 15.417208909988403\n",
      "dialy file on date 2020-06-30 generated\n",
      "     90 2020-06-30 15.483535051345825\n",
      "dialy file on date 2020-07-01 generated\n",
      "     91 2020-07-01 15.13231086730957\n",
      "dialy file on date 2020-07-02 generated\n",
      "     92 2020-07-02 15.569227457046509\n",
      "dialy file on date 2020-07-03 generated\n",
      "     93 2020-07-03 15.532163381576538\n",
      "dialy file on date 2020-07-04 generated\n",
      "     94 2020-07-04 15.658955574035645\n",
      "dialy file on date 2020-07-05 generated\n",
      "     95 2020-07-05 16.16916036605835\n",
      "dialy file on date 2020-07-06 generated\n",
      "     96 2020-07-06 15.413580894470215\n",
      "dialy file on date 2020-07-07 generated\n",
      "     97 2020-07-07 15.587528228759766\n",
      "dialy file on date 2020-07-08 generated\n",
      "     98 2020-07-08 15.275843620300293\n",
      "dialy file on date 2020-07-09 generated\n",
      "     99 2020-07-09 15.62832236289978\n",
      "dialy file on date 2020-07-10 generated\n",
      "     100 2020-07-10 15.356223344802856\n",
      "dialy file on date 2020-07-11 generated\n",
      "     101 2020-07-11 15.78577733039856\n",
      "dialy file on date 2020-07-12 generated\n",
      "     102 2020-07-12 15.690995216369629\n",
      "dialy file on date 2020-07-13 generated\n",
      "     103 2020-07-13 16.163645267486572\n",
      "dialy file on date 2020-07-14 generated\n",
      "     104 2020-07-14 15.933815240859985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialy file on date 2020-07-15 generated\n",
      "     105 2020-07-15 16.05688762664795\n",
      "dialy file on date 2020-07-16 generated\n",
      "     106 2020-07-16 15.625086784362793\n",
      "dialy file on date 2020-07-17 generated\n",
      "     107 2020-07-17 15.835510730743408\n",
      "dialy file on date 2020-07-18 generated\n",
      "     108 2020-07-18 15.804062843322754\n",
      "dialy file on date 2020-07-19 generated\n",
      "     109 2020-07-19 16.01051139831543\n",
      "dialy file on date 2020-07-20 generated\n",
      "     110 2020-07-20 15.69516634941101\n",
      "dialy file on date 2020-07-21 generated\n",
      "     111 2020-07-21 15.998474836349487\n",
      "dialy file on date 2020-07-22 generated\n",
      "     112 2020-07-22 17.99400019645691\n",
      "dialy file on date 2020-07-23 generated\n",
      "     113 2020-07-23 16.65254807472229\n",
      "dialy file on date 2020-07-24 generated\n",
      "     114 2020-07-24 16.738935947418213\n",
      "dialy file on date 2020-07-25 generated\n",
      "     115 2020-07-25 15.935072183609009\n",
      "dialy file on date 2020-07-26 generated\n",
      "     116 2020-07-26 16.425643920898438\n",
      "dialy file on date 2020-07-27 generated\n",
      "     117 2020-07-27 16.636221885681152\n",
      "dialy file on date 2020-07-28 generated\n",
      "     118 2020-07-28 16.516981601715088\n",
      "dialy file on date 2020-07-29 generated\n",
      "     119 2020-07-29 16.500362157821655\n",
      "dialy file on date 2020-07-30 generated\n",
      "     120 2020-07-30 16.222736120224\n",
      "dialy file on date 2020-07-31 generated\n",
      "     121 2020-07-31 16.772446632385254\n",
      "dialy file on date 2020-08-01 generated\n",
      "     122 2020-08-01 16.005974769592285\n",
      "dialy file on date 2020-08-02 generated\n",
      "     123 2020-08-02 16.085573434829712\n",
      "dialy file on date 2020-08-03 generated\n",
      "     124 2020-08-03 16.413301944732666\n",
      "dialy file on date 2020-08-04 generated\n",
      "     125 2020-08-04 16.534802198410034\n",
      "dialy file on date 2020-08-05 generated\n",
      "     126 2020-08-05 16.24715304374695\n",
      "dialy file on date 2020-08-06 generated\n",
      "     127 2020-08-06 16.23611354827881\n",
      "dialy file on date 2020-08-07 generated\n",
      "     128 2020-08-07 16.56627058982849\n",
      "dialy file on date 2020-08-08 generated\n",
      "     129 2020-08-08 16.506346464157104\n",
      "dialy file on date 2020-08-09 generated\n",
      "     130 2020-08-09 16.872217655181885\n",
      "dialy file on date 2020-08-10 generated\n",
      "     131 2020-08-10 16.7664794921875\n",
      "dialy file on date 2020-08-11 generated\n",
      "     132 2020-08-11 17.461987018585205\n",
      "dialy file on date 2020-08-12 generated\n",
      "     133 2020-08-12 17.050023078918457\n",
      "dialy file on date 2020-08-13 generated\n",
      "     134 2020-08-13 16.525525093078613\n",
      "dialy file on date 2020-08-14 generated\n",
      "     135 2020-08-14 17.12021255493164\n",
      "dialy file on date 2020-08-15 generated\n",
      "     136 2020-08-15 17.29893970489502\n",
      "dialy file on date 2020-08-16 generated\n",
      "     137 2020-08-16 17.01011347770691\n",
      "dialy file on date 2020-08-17 generated\n",
      "     138 2020-08-17 16.68229603767395\n",
      "dialy file on date 2020-08-18 generated\n",
      "     139 2020-08-18 18.107266664505005\n",
      "dialy file on date 2020-08-19 generated\n",
      "     140 2020-08-19 16.828526973724365\n",
      "dialy file on date 2020-08-20 generated\n",
      "     141 2020-08-20 23.08294987678528\n",
      "dialy file on date 2020-08-21 generated\n",
      "     142 2020-08-21 18.26700186729431\n",
      "dialy file on date 2020-08-22 generated\n",
      "     143 2020-08-22 17.408600330352783\n",
      "dialy file on date 2020-08-23 generated\n",
      "     144 2020-08-23 17.664201974868774\n",
      "dialy file on date 2020-08-24 generated\n",
      "     145 2020-08-24 17.908140897750854\n",
      "dialy file on date 2020-08-25 generated\n",
      "     146 2020-08-25 17.29504418373108\n",
      "dialy file on date 2020-08-26 generated\n",
      "     147 2020-08-26 18.382290601730347\n",
      "dialy file on date 2020-08-27 generated\n",
      "     148 2020-08-27 18.786951780319214\n",
      "dialy file on date 2020-08-28 generated\n",
      "     149 2020-08-28 18.430410385131836\n",
      "dialy file on date 2020-08-29 generated\n",
      "     150 2020-08-29 17.217907667160034\n",
      "dialy file on date 2020-08-30 generated\n",
      "     151 2020-08-30 18.143737077713013\n",
      "dialy file on date 2020-08-31 generated\n",
      "     152 2020-08-31 17.624600410461426\n",
      "dialy file on date 2020-09-01 generated\n",
      "     153 2020-09-01 18.809248685836792\n",
      "job time is 2453.9083189964294\n",
      "overall time is 2513.8236927986145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "table_codes = {117: 'MIRAE'}\n",
    "\n",
    "table_codes = {\"116\": \"AXA\",\"117\": \"MIRAE\",\"107\": \"BOB\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codes = {\"117\": \"MIRAE\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codess ={130:\"peerless\", 120:\"INVESCO\", }\n",
    "\n",
    "table_codes = {125: 'IBMF', 152: \"ITI\", 123: \"Quantum\",}\n",
    "\n",
    "table_codes = {  107: \"BOB\",135: \"IDBIMF\", 178: \"BNPMF\", 103: \"PMF\"}\n",
    "\n",
    "table_codes = { 103:\"peerless\", 118:\"edelwwise\"}\n",
    "table_codes = {\"129\": \"DLFPramerica\",\"120\": \"INVESCO\"}\n",
    "table_codes = {\"RMF\": \"Reliance\" }\n",
    "table_codes = {\"116\": \"AXA\"}\n",
    "table_codes = {\"135\": 'IDBIMF'}\n",
    "table_codes = {118:\"edelwwise\"}\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "#         try:\n",
    "#             sc.stop()\n",
    "#         except:\n",
    "\n",
    "#             print (\"error no sc\")\n",
    "#         # intialize spark again\n",
    "#         conf = SparkConf()\n",
    "#         conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 16)\n",
    "\n",
    "#         #Create spark context and sparksession\n",
    "#         sc = SparkContext.getOrCreate(conf=conf)\n",
    "#         SparkContext.setSystemProperty(\"spark.driver.memory\", \"60g\")\n",
    "#         SparkContext.setSystemProperty(\"spark.executor.memory\", \"60g\")\n",
    "#         SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "#         SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"100g\")\n",
    "#         spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_trans_{code}'\n",
    "        groupby_level='SPFTTer'\n",
    "        init_date = '2020-03-31'\n",
    "        start_date = '2020-04-01'\n",
    "        end_date = '2020-09-02'\n",
    "#         mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "        save_metric(init_date, 'records_processed', records, name, groupby_level, table)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "        save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level, table)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange(start_date, end_date))):\n",
    "#             try:\n",
    "#                 spark.catalog.clearCache()\n",
    "#             except:\n",
    "#                 pass\n",
    "#             try:\n",
    "#                 sc.stop()\n",
    "#             except:\n",
    "\n",
    "#                 print (\"error no sc\")\n",
    "#             # intialize spark again\n",
    "#             conf = SparkConf()\n",
    "#             conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 16)\n",
    "\n",
    "#             #Create spark context and sparksession\n",
    "#             sc = SparkContext.getOrCreate(conf=conf)\n",
    "#             SparkContext.setSystemProperty(\"spark.driver.memory\", \"120g\")\n",
    "#             SparkContext.setSystemProperty(\"spark.executor.memory\", \"120g\")\n",
    "#             SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "#             SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"100g\")\n",
    "#             spark = SparkSession(sc)\n",
    "            \n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}_{code}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "            save_metric(ele, 'day_records', day_records, name, groupby_level, table)\n",
    "            save_metric(ele, 'combined_records', combined_records, name, groupby_level, table)\n",
    "            save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level, table)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMF Reliance\n",
      "initialization time 4.76837158203125e-06\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "table_codes = {117: 'MIRAE'}\n",
    "\n",
    "table_codes = {\"116\": \"AXA\",\"117\": \"MIRAE\",\"107\": \"BOB\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codes = {\"117\": \"MIRAE\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codess ={130:\"peerless\", 120:\"INVESCO\", }\n",
    "\n",
    "table_codes = {125: 'IBMF', 152: \"ITI\", 123: \"Quantum\",}\n",
    "\n",
    "table_codes = {  107: \"BOB\",135: \"IDBIMF\", 178: \"BNPMF\", 103: \"PMF\"}\n",
    "\n",
    "table_codes = { 103:\"peerless\", 118:\"edelwwise\"}\n",
    "table_codes = {\"129\": \"DLFPramerica\",\"120\": \"INVESCO\"}\n",
    "table_codes = {\"RMF\": \"Reliance\" }\n",
    "# table_codes = {\"116\": \"AXA\"}\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 16)\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"60g\")\n",
    "        SparkContext.setSystemProperty(\"spark.executor.memory\", \"60g\")\n",
    "        SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "        SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"100g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPFTTer'\n",
    "        init_date = '2020-03-31'\n",
    "        start_date = '2020-04-07'\n",
    "        end_date = '2020-09-02'\n",
    "#         mcr_month_date = '2020-05-01'\n",
    "\n",
    "#         records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level, table)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level, table)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange(start_date, end_date))):\n",
    "            try:\n",
    "                spark.catalog.clearCache()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                sc.stop()\n",
    "            except:\n",
    "\n",
    "                print (\"error no sc\")\n",
    "            # intialize spark again\n",
    "            conf = SparkConf()\n",
    "            conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 16)\n",
    "\n",
    "            #Create spark context and sparksession\n",
    "            sc = SparkContext.getOrCreate(conf=conf)\n",
    "            SparkContext.setSystemProperty(\"spark.driver.memory\", \"120g\")\n",
    "            SparkContext.setSystemProperty(\"spark.executor.memory\", \"120g\")\n",
    "            SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "            SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"100g\")\n",
    "            spark = SparkSession(sc)\n",
    "            \n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}_{code}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "            save_metric(ele, 'day_records', day_records, name, groupby_level, table)\n",
    "            save_metric(ele, 'combined_records', combined_records, name, groupby_level, table)\n",
    "            save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level, table)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 AXA\n",
      "inital file on date 2020-03-31 written\n",
      "initialization time 31.472009897232056\n",
      "dialy file on date 2020-04-01 generated\n",
      "     0 2020-04-01 6.777123928070068\n",
      "dialy file on date 2020-04-02 generated\n",
      "     1 2020-04-02 6.985471963882446\n",
      "dialy file on date 2020-04-03 generated\n",
      "     2 2020-04-03 7.0563507080078125\n",
      "dialy file on date 2020-04-04 generated\n",
      "     3 2020-04-04 6.491880178451538\n",
      "dialy file on date 2020-04-05 generated\n",
      "     4 2020-04-05 6.638464450836182\n",
      "dialy file on date 2020-04-06 generated\n",
      "     5 2020-04-06 6.717257261276245\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-111b72fb49b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0mscheme_table\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'fund_master_{name}_{code}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheme_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fm_scheme'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplan_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fm_plan'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fm_nature'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m              \u001b[0mcategory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'fm_SebiSchemeCategory'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubcategory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'fm_SebiSchemeSubCategory'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewmcrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fm_NewMCRId'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mdialy_job\u001b[0;34m(date_str, table, database, date_column, tenant_id, transaction_status, purred, transaction_type, folio, purchase_units, redemption_units, scheme, plan, groupby_level, direct_db, fn_fromdt, fn_fromdt_format, fn_scheme, fn_plan, fn_nav, nav_table, scheme_table, scheme_code, plan_code, category, subcategory, nature, newmcrid, ter_flag)\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36msignal_handler\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msignal_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;31m# see http://stackoverflow.com/questions/23206787/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "table_codes = {117: 'MIRAE'}\n",
    "\n",
    "table_codes = {\"116\": \"AXA\",\"117\": \"MIRAE\",\"107\": \"BOB\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codes = {\"117\": \"MIRAE\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codess ={130:\"peerless\", 120:\"INVESCO\", }\n",
    "\n",
    "table_codes = {125: 'IBMF', 152: \"ITI\", 123: \"Quantum\",}\n",
    "\n",
    "table_codes = {  107: \"BOB\",135: \"IDBIMF\", 178: \"BNPMF\", 103: \"PMF\"}\n",
    "\n",
    "table_codes = { 103:\"peerless\", 118:\"edelwwise\"}\n",
    "table_codes = {\"129\": \"DLFPramerica\",\"120\": \"INVESCO\"}\n",
    "table_codes = {\"RMF\": \"Reliance\" }\n",
    "table_codes = {\"116\": \"AXA\"}\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 16)\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"60g\")\n",
    "        SparkContext.setSystemProperty(\"spark.executor.memory\", \"60g\")\n",
    "        SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "        SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"100g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPFTTer'\n",
    "        init_date = '2020-03-31'\n",
    "        start_date = '2020-04-01'\n",
    "        end_date = '2020-09-02'\n",
    "#         mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "        #save_metric(init_date, 'records_processed', records, name, groupby_level, table)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange(start_date, end_date))):\n",
    "            try:\n",
    "                spark.catalog.clearCache()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                sc.stop()\n",
    "            except:\n",
    "\n",
    "                print (\"error no sc\")\n",
    "            # intialize spark again\n",
    "            conf = SparkConf()\n",
    "            conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 16)\n",
    "\n",
    "            #Create spark context and sparksession\n",
    "            sc = SparkContext.getOrCreate(conf=conf)\n",
    "            SparkContext.setSystemProperty(\"spark.driver.memory\", \"120g\")\n",
    "            SparkContext.setSystemProperty(\"spark.executor.memory\", \"120g\")\n",
    "            SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "            SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"100g\")\n",
    "            spark = SparkSession(sc)\n",
    "            \n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}_{code}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "            #save_metric(ele, 'day_records', day_records, name, groupby_level, table)\n",
    "            #save_metric(ele, 'combined_records', combined_records, name, groupby_level, table)\n",
    "            #save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level, table)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "table_codes = {117: 'MIRAE'}\n",
    "\n",
    "table_codes = {\"116\": \"AXA\",\"117\": \"MIRAE\",\"107\": \"BOB\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codes = {\"117\": \"MIRAE\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codes = {125: 'IBMF', 152: \"ITI\", 123: \"Quantum\",}\n",
    "table_codes = {125:'IBMF', 104:'TARUS', 103:\"peerless\", 123:\"Quantum\", 118:\"edelwwise\"}\n",
    "\n",
    "table_codes = {\"RMF\": \"Reliance\" }\n",
    "\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 16)\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        \n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"60g\")\n",
    "        SparkContext.setSystemProperty(\"spark.executor.memory\", \"60g\")\n",
    "        SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "        SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"200g\")\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPFT'\n",
    "        init_date = '2020-03-31'\n",
    "        start_date = '2020-06-05'\n",
    "        end_date = '2020-08-02'\n",
    "#         mcr_month_date = '2020-05-01'\n",
    "\n",
    "        #records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "        #save_metric(init_date, 'records_processed', records, name, groupby_level, table)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange(start_date, end_date))):\n",
    "            try:\n",
    "                spark.catalog.clearCache()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                sc.stop()\n",
    "            except:\n",
    "\n",
    "                print (\"error no sc\")\n",
    "            # intialize spark again\n",
    "            conf = SparkConf()\n",
    "            conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 16)\n",
    "\n",
    "            #Create spark context and sparksession\n",
    "            \n",
    "            SparkContext.setSystemProperty(\"spark.driver.memory\", \"60g\")\n",
    "            SparkContext.setSystemProperty(\"spark.executor.memory\", \"60g\")\n",
    "            SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "            SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"200g\")\n",
    "            sc = SparkContext.getOrCreate(conf=conf)\n",
    "            spark = SparkSession(sc)\n",
    "            \n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}_{code}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "            save_metric(ele, 'day_records', day_records, name, groupby_level, table)\n",
    "            save_metric(ele, 'combined_records', combined_records, name, groupby_level, table)\n",
    "            save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level, table)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-25T08:32:00.266Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "init_date = '2020-04-01'\n",
    "groupby_level = 'SPFT'\n",
    "table = 'trans116'\n",
    "direct_db = 'kfintech_funds'\n",
    "nav_table = 'nav_master'\n",
    "scheme_table = 'scheme_master'\n",
    "\n",
    "init_date = '2020-04-01'\n",
    "groupby_level = 'SPFT'\n",
    "table = 'trans116'\n",
    "direct_db = 'kfintech_funds'\n",
    "nav_table = 'nav_master'\n",
    "scheme_table = 'scheme_master'\n",
    "\n",
    "initialize(init_date, table=table,direct_db=direct_db, groupby_level=groupby_level)\n",
    "# init_date = '2020-04-30'\n",
    "# initialize(init_date, table=table,direct_db=direct_db, groupby_level=groupby_level)\n",
    "\n",
    "\n",
    "for ele in daterange('2020-04-02', '2020-04-03'):\n",
    "    dialy_job(ele, groupby_level=groupby_level, table=table, direct_db=direct_db, nav_table=nav_table,\n",
    "                scheme_table=scheme_table, scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "                 category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_data = spark.read.parquet(f\"{'m_Trans_116'}_dialy/data_{'SPFTTer'}_{'2020-09-01'}.parquet\")\n",
    "latest_data.coalesce(1).write.csv('axa_ter_flag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:51.310674Z",
     "start_time": "2020-08-25T08:31:26.686999Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_data(start_date, end_date, groupby_level='SPT', table='m_Trans_116'):\n",
    "    \n",
    "    final_data = None\n",
    "\n",
    "    dates_list = list(daterange(start_date, end_date))\n",
    "    for date in dates_list:\n",
    "        date_obj = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "        day_num = date_obj.day\n",
    "        latest_data = spark.read.parquet(f\"{table}_dialy/data_{groupby_level}_{date}.parquet\")\n",
    "        if final_data:\n",
    "            final_data = final_data.union(latest_data)\n",
    "        else:\n",
    "            final_data = latest_data\n",
    "    return final_data\n",
    "\n",
    "def generate_mcr_report(table='m_Trans_116', ignored_tr_types = ['CNI', 'CNO', 'TRMI', 'TRMO', \n",
    "                                                                 'TRFI', 'TRFO', 'PLDO', 'UPLO', 'DMT',\n",
    "                                                                 'RMT', 'CNIR', 'CNOR', 'TRMIR', 'TRMOR',\n",
    "                                                                 'TRFIR', 'TRFOR', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR'],\n",
    "        liquid_fund_tr_types = ['NEW', 'ADD', 'IPO', 'SIN', 'NEWR', 'ADDR', 'IPOR', 'SINR'],\n",
    "                       start_date = '2020-05-02', end_date = '2020-06-02', groupby_level='SPT',\n",
    "                        transaction_type='TransactionType',folio='Folio',folio_ignore_types = ['PLDO', 'UPLO', 'DMT', 'RMT', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR'], \n",
    "                      fn_nav = 'fn_nav', newmcrid='fm_NewMCRId', today_pu = 'today_pu', today_ru = 'today_ru', scheme='SchemeCode', aum='aum', plan='PlanCode'):\n",
    "    \n",
    "    till_but_one_day_data = None\n",
    "    inflow = 'inflow'\n",
    "    outflow = 'outflow'\n",
    "    calculated_date = 'calculated_date'\n",
    "    batch_close_date = 'batch_close_date'\n",
    "    balance_pu = 'balance_pu'\n",
    "    balance_ru = 'balance_ru'\n",
    "    balance_units = 'balance_units'\n",
    "    \n",
    "    \n",
    "    dates_list = list(daterange(start_date, end_date))\n",
    "    \n",
    "    for date in dates_list[:-1]:\n",
    "        date_obj = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "        day_num = date_obj.day\n",
    "        latest_data = spark.read.parquet(f\"{table}_dialy/data_{groupby_level}_{date}.parquet\")\n",
    "#         print (ele, latest_data.count())\n",
    "        if till_but_one_day_data:\n",
    "            till_but_one_day_data = till_but_one_day_data.union(latest_data)\n",
    "        else:\n",
    "            till_but_one_day_data = latest_data\n",
    "            \n",
    "    \n",
    "    date_obj = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    final_day = date_obj + datetime.timedelta(1)\n",
    "    final_day_str = final_day.strftime('%Y-%m-%d')\n",
    "    final_day_data = spark.read.parquet(f\"{table}_dialy/data_{groupby_level}_{final_day_str}.parquet\")\n",
    "    last_but_one_day_data = latest_data\n",
    "\n",
    "    till_but_one_day_data = till_but_one_day_data.filter( ~(col(transaction_type).isin(ignored_tr_types)) ).fillna({today_pu: 0, today_ru: 0, aum: 0, newmcrid: 'Others'})\n",
    "    final_day_data = final_day_data.filter( ~(col(transaction_type).isin(ignored_tr_types)) ).fillna({today_pu: 0, today_ru: 0, aum: 0, newmcrid: 'Others'})\n",
    "    \n",
    "    \n",
    "    all_data = till_but_one_day_data.union(final_day_data)\n",
    "    sp_inf_ouf_data = all_data\n",
    "\n",
    "    liquid_condition = ( (col(newmcrid) == 'A1b') & (col(calculated_date) == final_day_str) & (col(batch_close_date) == final_day_str) & (col(transaction_type).isin(liquid_fund_tr_types)) )\n",
    "    \n",
    "    final_day_data = final_day_data.withColumn(balance_pu,    when(liquid_condition, col(balance_pu) - col(today_pu)).otherwise(col(balance_pu)))\n",
    "    final_day_data = final_day_data.withColumn(balance_ru,    when(liquid_condition, col(balance_ru) - col(today_ru)).otherwise(col(balance_ru)))\n",
    "    final_day_data = final_day_data.withColumn(balance_units, when(liquid_condition, col(balance_pu) - col(balance_ru)).otherwise(col(balance_units)) )\n",
    "    final_day_data = final_day_data.withColumn(aum,           when(liquid_condition,  col(balance_units) * col(fn_nav)).otherwise(col(aum))    )\n",
    "\n",
    "    net_aum = final_day_data.groupby([newmcrid]).agg(sum(aum))\n",
    "    avg_data = till_but_one_day_data.union(final_day_data)\n",
    "#     avg_data = all_data\n",
    "    \n",
    "    # inflow, outflow logic change\n",
    "    sp_data = avg_data\n",
    "    # sp_inf_ouf_data = sp_inf_ouf_data.withColumn(inflow, (col(today_pu)*col(fn_nav)))\n",
    "    # sp_inf_ouf_data = sp_inf_ouf_data.withColumn(outflow, (col(today_ru)*col(fn_nav)))\n",
    "    sp = sp_data.groupby([newmcrid]).agg(countDistinct(scheme),countDistinct(plan))\n",
    "    \n",
    "#     inf_ouf_data = get_data(datetime.datetime(date_obj.year, 4, 2).strftime('%Y-%m-%d'), start_date, groupby_level, table).union(sp_data)\n",
    "#     inf_ouf_data = inf_ouf_data.withColumn(inflow, (col(today_pu)*col(fn_nav)))\n",
    "#     inf_ouf_data = inf_ouf_data.withColumn(outflow, (col(today_ru)*col(fn_nav)))\n",
    "#     inf_ouf_data = inf_ouf_data.groupby([newmcrid]).agg(sum(col(inflow)),sum(col(outflow)))\n",
    "    \n",
    "#     spinout = sp.join(inf_ouf_data, on=[newmcrid], how='left')\n",
    "#     spinout.show()\n",
    "    spinout = sp\n",
    "    # sp_inf_ouf_data = sp_inf_ouf_data.withColumn(inflow, (col(today_pu)*col(fn_nav)))\n",
    "    # sp_inf_ouf_data = sp_inf_ouf_data.withColumn(outflow, (col(today_ru)*col(fn_nav)))\n",
    "\n",
    "    folio_count = avg_data.groupby(folio, scheme, plan, newmcrid).agg(sum('aum')).filter(col('sum(aum)') - 0 > 0.1).groupby(newmcrid).agg(countDistinct(folio))\n",
    "\n",
    "    avg_aum = avg_data.groupby([newmcrid]).agg(sum(aum))\n",
    "    avg_aum = avg_aum.withColumn('avg_aum', col(f'sum({aum})')/(len(list(daterange(start_date, end_date))))).drop(f'sum({aum})')\n",
    "    \n",
    "\n",
    "    mcr_net_aum = spinout.join(net_aum, on=[newmcrid], how='left')\n",
    "    mcr = mcr_net_aum.join(avg_aum, on=[newmcrid], how='left')\n",
    "    mcr = mcr.join(folio_count, on=[newmcrid], how='left')\n",
    "    \n",
    "    # inflow outflow\n",
    "    all_data = get_data('2020-04-02', end_date, groupby_level=groupby_level, table=table)\n",
    "    all_data = all_data.filter(~liquid_condition)\n",
    "    all_data = all_data.fillna({today_pu: 0, today_ru: 0, aum: 0, newmcrid: 'Others', inflow:0, outflow:0})\n",
    "#     all_data = all_data.filter( ~(col(transaction_type).isin(ignored_tr_types)) ).fillna({today_pu: 0, today_ru: 0, aum: 0, newmcrid: 'Others', inflow:0, outflow:0})\n",
    "    inout = all_data.groupby([newmcrid]).agg(sum(inflow), sum(outflow))\n",
    "    mcr = mcr.join(inout, on=[newmcrid], how='left')\n",
    "    \n",
    "#     inout.show(1000)\n",
    "    mcr.show(1000)\n",
    "    \n",
    "    mcr.coalesce(1).write.csv(f\"{table}_mcr/mcr_{groupby_level}_{final_day_str}.csv\",header=True, mode='overwrite')\n",
    "#     mcr.coalesce(1).write.parquet(f\"{table}_mcr/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    \n",
    "    return mcr,avg_data, sp_inf_ouf_data, all_data\n",
    " \n",
    "    \n",
    "table = 'trans116'\n",
    "table = 'm_Trans_116'\n",
    "groupby_level = 'SPFT'\n",
    "start_date = '2020-04-02'\n",
    "end_date = '2020-05-02'\n",
    "mcr, _,_,_ = generate_mcr_report(table=table, groupby_level=groupby_level, start_date=start_date, end_date=end_date)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:05:13.238626Z",
     "start_time": "2020-08-19T07:05:11.045764Z"
    },
    "code_folding": [
     9
    ]
   },
   "outputs": [],
   "source": [
    "table_codes = {117: 'MIRAE'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPT'\n",
    "        init_date = '2020-06-30'\n",
    "        mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange('2020-07-01', '2020-08-02'))):\n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "#             save_metric(ele, 'day_records', day_records, name, groupby_level)\n",
    "#             save_metric(ele, 'combined_records', combined_records, name, groupby_level)\n",
    "#             save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:05:13.924841Z",
     "start_time": "2020-08-19T07:05:13.433907Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "table_codes = {'RMF': 'Reliance'}\n",
    "\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPT'\n",
    "        init_date = '2020-06-30'\n",
    "        mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange('2020-07-01', '2020-08-02'))):\n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "#             save_metric(ele, 'day_records', day_records, name, groupby_level)\n",
    "#             save_metric(ele, 'combined_records', combined_records, name, groupby_level)\n",
    "#             save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T09:10:22.530312Z",
     "start_time": "2020-08-19T09:10:22.524239Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "# datetime.datetime.year('2020-08-04')\n",
    "\n",
    "date_obj = datetime.datetime.strptime('2020-08-05', '%Y-%m-%d')\n",
    "date_obj.year\n",
    "\n",
    "datetime.datetime(date_obj.year, 4, 2).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T17:20:38.744555Z",
     "start_time": "2020-08-19T17:20:38.732643Z"
    }
   },
   "outputs": [],
   "source": [
    "{\"116\": \"AXA\",\n",
    "\"117\": \"MIRAE\",\n",
    "\"107\": \"BOB\",\n",
    "\"120\": \"INVESCO\",\n",
    "\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\n",
    "\"135\": \"IDBIMF\",\n",
    "\"125\": \"IBMF\",\n",
    "\"128\": \"AXISMF\",\n",
    "\"178\": \"BNPMF\",\n",
    "\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\n",
    "\"103\": \"PMF\",\n",
    "\"166\": \"Quant\",\n",
    "\"130\": \"PeerlessMF\",\n",
    "\"104\": \"TAURUS\",\n",
    "\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\n",
    "\"127\": \"MOTILAL\",\n",
    "\"102\": \"LIC\",\n",
    "\"176\": \"SundaramMF\",\n",
    "\"101\": \"canrobeco\",\n",
    "\"129\": \"DLFPramerica\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#### all the script exaaecution for all the funds\n",
    "table_codes = {\n",
    "               102:'LIC',103:'pmf',104:'tarus',105:'JMMF',107:'BOB',108:'uti',\n",
    "               116:'AXA',117:'mirae',118:'edelwwise',120:'invesco',123:'quantum'\n",
    "               ,125:'IBMF',127:'motilal',128:'axismf',130:'peerless',135:'IDBIMF',152:'ITI',166:'quant',\n",
    "               176:'sundaram',178:'BNPMF', 'RMF':'reliance'}\n",
    "table_codes = {104:'taurus',105:'JMMF',107:'BOB',108:'uti',\n",
    "               116:'AXA',117:'mirae',118:'edelwwise',120:'invesco',123:'quantum'\n",
    "               ,125:'IBMF',127:'motilal',128:'axismf',130:'peerless',135:'IDBIMF',152:'ITI',166:'quant',\n",
    "               176:'sundaram',178:'BNPMF', 'RMF':'reliance'}\n",
    "\n",
    "table_codes = {101:'canrobeco',\n",
    "               102:'LIC',103:'pmf',104:'tarus',105:'JMMF',107:'BOB',108:'uti',\n",
    "               116:'AXA',117:'mirae',118:'edelwwise',120:'invesco',123:'quantum'\n",
    "               ,125:'IBMF',127:'motilal',128:'axismf',129:'pgim',130:'peerless',135:'IDBIMF',152:'ITI',166:'quant'\n",
    "               ,178:'BNPMF', 'RMF':'reliance', '129': 'dlfpramerica', 118:'edelweiss', 130:'peerlessMF', '176':'sundaramMF'}\n",
    "table_codes = {116: 'AXA'}\n",
    "\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPFT'\n",
    "        init_date = '2020-04-30'\n",
    "        mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange('2020-05-01', '2020-06-02'))):\n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "            save_metric(ele, 'day_records', day_records, name, groupby_level)\n",
    "            save_metric(ele, 'combined_records', combined_records, name, groupby_level)\n",
    "            save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#### generate mcr for august for all the funds\n",
    "# table_codes = { '129': 'dlfpramerica', 118:'edelweiss', 130:'peerlessMF', '176':'sundaramMF'}\n",
    "\n",
    "table_codes = {101:'canrobeco',\n",
    "               102:'LIC',103:'pmf',104:'tarus',105:'JMMF',107:'BOB',108:'uti',\n",
    "               116:'AXA',117:'mirae',118:'edelwwise',120:'invesco',123:'quantum'\n",
    "               ,125:'IBMF',127:'motilal',128:'axismf',129:'pgim',130:'peerless',135:'IDBIMF',152:'ITI',166:'quant'\n",
    "               ,178:'BNPMF', 'RMF':'reliance', '129': 'dlfpramerica', 118:'edelweiss', 130:'peerlessMF', '176':'sundaramMF'}\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPT'\n",
    "        init_date = '2020-07-01'\n",
    "        mcr_month_date = '2020-07-01'\n",
    "\n",
    "#         records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level)\n",
    "\n",
    "#         print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "#         job_start = time.time()\n",
    "\n",
    "\n",
    "#         for i,ele in enumerate(list(daterange('2020-07-02', '2020-08-02'))):\n",
    "#             s = time.time()\n",
    "#             day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "#             scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#              category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#             )\n",
    "#             save_metric(ele, 'day_records', day_records, name, groupby_level)\n",
    "#             save_metric(ele, 'combined_records', combined_records, name, groupby_level)\n",
    "#             save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level)\n",
    "#             print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "#         s = time.time() \n",
    "        generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-07-02', end_date = '2020-08-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "#         print (f'job time is {time.time() - job_start}')\n",
    "#         print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
