{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T11:57:57.131756Z",
     "start_time": "2020-09-15T11:57:57.105156Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'v'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-ca8831849d75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'v2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplus_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1398\u001b[0m         \"\"\"\n\u001b[1;32m   1399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1401\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[1;32m   1402\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'v'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Use udf to define a row-at-a-time udf\n",
    "@udf('double')\n",
    "# Input/output are both a single double value\n",
    "def plus_one(v):\n",
    "      return v + 1\n",
    "    \n",
    "    \n",
    "\n",
    "df.withColumn('v2', plus_one(df.v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T13:45:41.029963Z",
     "start_time": "2020-09-15T13:45:40.097527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.91 µs\n",
      "0.9056196212768555\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf       \n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "@pandas_udf('long')\n",
    "def pandas_plus_one(s: pd.Series) -> pd.Series:\n",
    "    return s + 1\n",
    "s = time.time()\n",
    "df = spark.range(10000000000000000).select(pandas_plus_one(\"id\"))\n",
    "for i in range(100):\n",
    "    df = spark.range(10000000000000000).select(pandas_plus_one(\"id\"))\n",
    "#      df=   spark.range(10000000000000000).select(col(\"id\")+1)\n",
    "    \n",
    "# spark.range(10000000000000000).select(col(\"id\")+1).show(5)\n",
    "print (time.time()-s)\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T11:25:48.960594Z",
     "start_time": "2020-09-15T11:25:48.943186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Author  Article       x\n",
      "0      P      210  21.000\n",
      "1      R      211  21.000\n",
      "2      P      114  24.000\n",
      "3      R      178  25.231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    P\n",
       "1    S\n",
       "2    P\n",
       "3    S\n",
       "dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author = ['P', 'R', 'P', 'R'] \n",
    "article = [210, 211, 114, 178] \n",
    "  \n",
    "auth_series = pd.Series(author) \n",
    "article_series = pd.Series(article) \n",
    "  \n",
    "frame = { 'Author': auth_series, 'Article': article_series } \n",
    "  \n",
    "result = pd.DataFrame(frame) \n",
    "age = [21, 21, 24, 25.231] \n",
    "  \n",
    "result['x'] = pd.Series(age) \n",
    "  \n",
    "print (result) \n",
    "pd.Series(np.where(result['Author'] == 'P', result['Author'], 'S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T11:25:52.744674Z",
     "start_time": "2020-09-15T11:25:51.737271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "dtype: int64\n",
      "+------+-------+------+\n",
      "|Author|Article|     x|\n",
      "+------+-------+------+\n",
      "|     P|    210|  21.0|\n",
      "|     R|    211|  21.0|\n",
      "|     P|    114|  24.0|\n",
      "|     R|    178|25.231|\n",
      "+------+-------+------+\n",
      "\n",
      "+------+-------+------+---------------------------------------+\n",
      "|Author|Article|     x|populate_db_cr_func(Author, Article, x)|\n",
      "+------+-------+------+---------------------------------------+\n",
      "|     P|    210|  21.0|                                  210.0|\n",
      "|     R|    211|  21.0|                                   21.0|\n",
      "|     P|    114|  24.0|                                  114.0|\n",
      "|     R|    178|25.231|                                 25.231|\n",
      "+------+-------+------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "def populate_db_cr_func(a: pd.Series, b: pd.Series, c:pd.Series) -> pd.Series:\n",
    "    return pd.Series(np.where(a=='P', b, c))\n",
    "\n",
    "populate_db_cr = pandas_udf(populate_db_cr_func, returnType=FloatType())\n",
    "\n",
    "# The function for a pandas_udf should be able to execute with local Pandas data\n",
    "x = pd.Series([1, 2, 3])\n",
    "y = pd.Series([4, 5, 6])\n",
    "\n",
    "print(multiply_func(x, x))\n",
    "\n",
    "# Create a Spark DataFrame, 'spark' is an existing SparkSession\n",
    "df = spark.createDataFrame(result)\n",
    "df.show()\n",
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(df.columns+ [populate_db_cr(col(\"Author\"), col(\"Article\"), col(\"x\"))]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T00:15:24.076933Z",
     "start_time": "2020-09-16T00:15:23.045080Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# db_utils\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import sqlalchemy\n",
    "import os\n",
    "import pyodbc\n",
    "import sys\n",
    "# import pymssql\n",
    "import numpy as np\n",
    "\n",
    "from MySQLdb._exceptions import OperationalError\n",
    "from sqlalchemy import create_engine, exc,event\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "from time import time\n",
    "\n",
    "#connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=192.168.15.126;UID=BRS;PWD=Fint$123;Trusted Connection=yes;DATABASE=\"\n",
    "connection_string = None\n",
    "import logging\n",
    "   \n",
    "# logging = Logging()\n",
    "\n",
    "class DB(object):\n",
    "    def __init__(self, database, host='127.0.0.1', user='root', password='', port='3306', tenant_id=None):\n",
    "        \"\"\"\n",
    "        Initialization of databse object.\n",
    "\n",
    "        Args:\n",
    "            databse (str): The database to connect to.\n",
    "            host (str): Host IP address. For dockerized app, it is the name of\n",
    "                the service set in the compose file.\n",
    "            user (str): Username of MySQL server. (default = 'root')\n",
    "            password (str): Password of MySQL server. For dockerized app, the\n",
    "                password is set in the compose file. (default = '')\n",
    "            port (str): Port number for MySQL. For dockerized app, the port that\n",
    "                is mapped in the compose file. (default = '3306')\n",
    "        \"\"\"\n",
    "\n",
    "        if host in [\"common_db\",\"extraction_db\", \"queue_db\", \"template_db\", \"table_db\", \"stats_db\", \"business_rules_db\", \"reports_db\"]:\n",
    "            self.HOST = os.environ['HOST_IP']\n",
    "            self.USER = os.environ['LOCAL_DB_USER']\n",
    "            self.PASSWORD = os.environ['LOCAL_DB_PASSWORD']\n",
    "            self.PORT = os.environ['LOCAL_DB_PORT']\n",
    "            self.DATABASE = f'{tenant_id}_{database}' if tenant_id is not None and tenant_id else database\n",
    "        else:\n",
    "            self.HOST = os.environ['HOST_IP']\n",
    "            self.USER = os.environ['LOCAL_DB_USER']\n",
    "            self.PASSWORD = os.environ['LOCAL_DB_PASSWORD']\n",
    "            self.PORT = os.environ['LOCAL_DB_PORT']\n",
    "            self.DATABASE = f'{tenant_id}_{database}' if tenant_id is not None and tenant_id else database\n",
    "       \n",
    "        logging.info(f'Host: {self.HOST}')\n",
    "        logging.info(f'User: {self.USER}')\n",
    "        logging.info(f'Password: {self.PASSWORD}')\n",
    "        logging.info(f'Port: {self.PORT}')\n",
    "        logging.info(f'Database: {self.DATABASE}')\n",
    "        # self.connect()\n",
    "    def connect(self, max_retry=5):\n",
    "#         retry = 1\n",
    "\n",
    "#         try:\n",
    "#             start = time()\n",
    "#             logging.debug(f'Making connection to {self.DATABASE}...')\n",
    "#             config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{self.DATABASE}?charset=utf8'\n",
    "#             self.db_ = create_engine(config, connect_args={'connect_timeout': 2}, pool_recycle=300)\n",
    "#             logging.info(f'Engine created for {self.DATABASE}')\n",
    "#             while retry <= max_retry:\n",
    "#                 try:\n",
    "#                     self.engine = self.db_.connect()\n",
    "#                     logging.info(f'Connection established succesfully to {self.DATABASE}! ({round(time() - start, 2)} secs to connect)')\n",
    "#                     break\n",
    "#                 except Exception as e:\n",
    "#                     logging.warning(f'Connection failed. Retrying... ({retry}) [{e}]')\n",
    "#                     retry += 1\n",
    "#                     self.db_.dispose()\n",
    "#         except:\n",
    "#             logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "#             return\n",
    "        data = []\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        print(inds)\n",
    "        for pos, ind in enumerate(inds):\n",
    "            if pos % 2 == 0:\n",
    "                sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "            else:\n",
    "                sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "        if connection_string:\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + self.DATABASE)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pyodbc.connect('DRIVER={' + driver + '};SERVER=' + host_ + ';DATABASE=' + database+ ';UID=' + user_ + ';PWD=' + password_ + ';Trusted Connection=yes;')\n",
    "                else:\n",
    "                    conn = pyodbc.connect('DRIVER={' + driver + '};SERVER=' + host_ + ';DATABASE=' + database + ';Trusted Connection=yes;')\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pyodbc.connect('DRIVER={' + driver + '};SERVER=' + host_ + ';DATABASE=' + database + ';Trusted Connection=yes;')\n",
    "\n",
    "    def convert_to_mssql(self, query):\n",
    "        inds = [i for i in range(len(query)) if query[i] == '`']\n",
    "        for pos, ind in enumerate(inds):\n",
    "            if pos % 2 == 0:\n",
    "                query = query[:ind] + '[' + query[ind+1:]\n",
    "            else:\n",
    "                query = query[:ind] + ']' + query[ind + 1:]\n",
    "       \n",
    "        query = query.replace('%s', '?')\n",
    "\n",
    "        return query\n",
    "\n",
    "    def execute(self, query, database=None, index_col='id', **kwargs):\n",
    "        logging.debug(f'Before converting: {query}')\n",
    "        query = self.convert_to_mssql(query)\n",
    "        logging.debug(f'After converting: {query}')\n",
    "\n",
    "        logging.debug('Connecting to DB')\n",
    "        conn = pyodbc.connect(f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={self.HOST};UID={self.USER};PWD={self.PASSWORD};Trusted Connection=yes;DATABASE={self.DATABASE}', as_dict=True)\n",
    "        logging.debug(f'Connection established with {self.DATABASE}. [{conn}]')\n",
    "        curs = conn.cursor()\n",
    "        logging.debug(f'Cursor object created. [{curs}]')\n",
    "        params = tuple(kwargs.get('params', []))\n",
    "       \n",
    "        logging.debug(f'Params: {params}')\n",
    "        logging.debug(f'Params Type: {type(params)}')\n",
    "        params = [int(i) if isinstance(i, np.int64) else i for i in params]\n",
    "        curs.execute(query, params)\n",
    "        logging.debug(f'Query executed.')\n",
    "       \n",
    "        data = None\n",
    "\n",
    "        try:\n",
    "            logging.debug(f'Fetching all data.')\n",
    "            data = curs.fetchall()\n",
    "            # logging.debug(f'Data fetched: {data}')\n",
    "            columns = [column[0] for column in curs.description]\n",
    "            logging.debug(f'Columns: {columns}')\n",
    "            result = []\n",
    "            for row in data:\n",
    "                result.append(dict(zip(columns, row)))\n",
    "            # logging.debug(f'Zipped result: {result}')\n",
    "            if result:\n",
    "                data = pd.DataFrame(result)\n",
    "            else:\n",
    "                data = pd.DataFrame(columns=columns)\n",
    "            # logging.debug(f'Data to DF: {data}')\n",
    "        except:\n",
    "            logging.debug('Update Query')\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            logging.debug(f'Data is not a DataFrame. Returning True. [{type(data)}]')\n",
    "            return True\n",
    "       \n",
    "        try:\n",
    "            if index_col is not None:\n",
    "                logging.debug(f'Setting ID as index')\n",
    "                return data.where((pd.notnull(data)), None).set_index('id')\n",
    "            else:\n",
    "                return data.where((pd.notnull(data)), None)\n",
    "        except:\n",
    "            logging.exception(f'Failed to set ID as index')\n",
    "            return data.where((pd.notnull(data)), None)\n",
    "\n",
    "    def execute__(self, query, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Executes an SQL query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that needs to be executed.\n",
    "            database (str): Name of the database to execute the query in. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            params (list/tuple/dict): List of parameters to pass to in the query.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data from the executed\n",
    "            query. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        data = None\n",
    "\n",
    "#         # Use new database if a new databse is given\n",
    "#         if database is not None:\n",
    "#             try:\n",
    "#                 config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "#                 engine = create_engine(config, pool_recycle=300)\n",
    "#             except:\n",
    "#                 logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "#                 return False\n",
    "#         else:\n",
    "#             engine = self.engine\n",
    "       \n",
    "        print('query', query)\n",
    "        if database is None:\n",
    "            database = 'karvy'\n",
    "        data = None\n",
    "        sql = query\n",
    "        user_ = self.USER\n",
    "        host_ = self.HOST\n",
    "        database = self.DATABASE\n",
    "        password_ = self.PASSWORD\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        for pos, ind in enumerate(inds):\n",
    "            if pos % 2 == 0:\n",
    "                sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "            else:\n",
    "                sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "               \n",
    "        if connection_string:\n",
    "            print('connection string', connection_string)\n",
    "            print('database', database)\n",
    "            print(type(connection_string + database))\n",
    "            print(type(connection_string + database))\n",
    "\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + database)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pymssql.connect(host=host_,database=database,user=user_,password=password_)\n",
    "                else:\n",
    "                    conn = pymssql.connect(host=host_,database=database)\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pymssql.connect(host=host_,database=database)\n",
    "        try:\n",
    "            logging.debug(f'Query: {query}')\n",
    "            logging.debug(f'Query: {query}')\n",
    "            curs = conn.cursor(as_dict = True)\n",
    "            params = kwargs.get('params', [])\n",
    "            logging.debug(f'Params: {params}')\n",
    "            curs.execute(sql, tuple(params))\n",
    "            print('query executed')\n",
    "            try:\n",
    "                data = curs.fetchall()\n",
    "                data = pd.DataFrame(data)\n",
    "                # print(data)\n",
    "            except Exception as e:\n",
    "                logging.debug('Update Query')\n",
    "            # data = pd.read_sql(sql, conn, index_col='id', **kwargs)\n",
    "        except exc.ResourceClosedError:\n",
    "            logging.warning('Query does not have any value to return.')\n",
    "            return True\n",
    "        except (exc.StatementError, OperationalError) as e:\n",
    "            logging.warning(f'Creating new connection. Engine/Connection is probably None. [{e}]')\n",
    "            self.connect()\n",
    "            data = pd.read_sql(query, self.engine, index_col='id', **kwargs)\n",
    "        except:\n",
    "            logging.exception('Something went wrong executing query. Check trace.')\n",
    "            params = kwargs['params'] if 'params' in kwargs else None\n",
    "            conn.rollback()\n",
    "            return False\n",
    "        conn.close()\n",
    "        return data.where((pd.notnull(data)), None).set_index('id')\n",
    "\n",
    "    def execute_(self, query, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Executes an SQL query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that needs to be executed.\n",
    "            database (str): Name of the database to execute the query in. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            params (list/tuple/dict): List of parameters to pass to in the query.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data from the executed\n",
    "            query. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        logging.debug(f'Executing `execute` instead of `execute_`')\n",
    "        return self.execute(query, index_col=None, **kwargs)\n",
    "       \n",
    "        data = None\n",
    "\n",
    "#         # Use new database if a new database is given\n",
    "#         if database is not None:\n",
    "#             try:\n",
    "#                 config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "#                 engine = create_engine(config, pool_recycle=300)\n",
    "#             except:\n",
    "#                 logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "#                 return False\n",
    "#         else:\n",
    "#             engine = self.engine\n",
    "\n",
    "#         try:\n",
    "#             data = pd.read_sql(query, engine, **kwargs)\n",
    "#         except exc.ResourceClosedError:\n",
    "#             return True\n",
    "#         except:\n",
    "#             logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "#             params = kwargs['params'] if 'params' in kwargs else None\n",
    "#             return False\n",
    "        print('query', query)\n",
    "        if database is None:\n",
    "            database = 'karvy'\n",
    "        data = None\n",
    "        sql = query\n",
    "        user_ = self.USER\n",
    "        database = self.DATABASE\n",
    "        host_ = self.HOST\n",
    "        password_ = self.PASSWORD\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        # for pos, ind in enumerate(inds):\n",
    "        #     if pos % 2 == 0:\n",
    "        #         sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "        #     else:\n",
    "        #         sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "               \n",
    "        if connection_string:\n",
    "            print('connection string', connection_string)\n",
    "            print('database', database)\n",
    "            print(type(connection_string + database))\n",
    "            print(type(connection_string + database))\n",
    "\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + database)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pymssql.connect(host=host_,database=database,user=user_,password=password_)\n",
    "                else:\n",
    "                    conn = pymssql.connect(host=host_,database=database)\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pymssql.connect(host=host_,database=database)\n",
    "        try:\n",
    "            logging.debug(f'Query: {query}')\n",
    "            curs = conn.cursor(as_dict = True)\n",
    "            params = kwargs.get('params', [])\n",
    "            logging.debug(f'Params: {params}')\n",
    "            curs.execute(sql, params)\n",
    "            print('query executed')\n",
    "            try:\n",
    "                data = curs.fetchall()\n",
    "                data = pd.DataFrame(data)\n",
    "                print(data)\n",
    "            except Exception as e:\n",
    "                logging.debug('Update Query')\n",
    "\n",
    "            #data = pd.read_sql(sql, conn,**kwargs)\n",
    "        except exc.ResourceClosedError:\n",
    "            logging.warning('Query does not have any value to return.')\n",
    "            return True\n",
    "        except (exc.StatementError, OperationalError) as e:\n",
    "            logging.warning(f'Creating new connection. Engine/Connection is probably None. [{e}]')\n",
    "            self.connect()\n",
    "            data = pd.read_sql(query, conn,**kwargs)\n",
    "        except:\n",
    "            logging.exception('Something went wrong executing query. Check trace.')\n",
    "            params = kwargs['params'] if 'params' in kwargs else None\n",
    "            conn.rollback()\n",
    "            return False\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "#         return data.where((pd.notnull(data)), None)\n",
    "        try:\n",
    "            return data.replace({pd.np.nan: None}).set_index('id')\n",
    "        except AttributeError as e:\n",
    "            return True\n",
    "\n",
    "    def insert(self, data, table, **kwargs):\n",
    "        \"\"\"\n",
    "        Write records stored in a DataFrame to a SQL database.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The DataFrame that needs to be write to SQL database.\n",
    "            table (str): The table in which the rcords should be written to.\n",
    "            kwargs: Keyword arguments for pandas to_sql function.\n",
    "                See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html\n",
    "                to know the arguments that can be passed.\n",
    "\n",
    "        Returns:\n",
    "            (bool) True is succesfully inserted, else false.\n",
    "        \"\"\"\n",
    "        logging.info(f'Inserting into `{table}`')\n",
    "       \n",
    "        conn = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={self.HOST};UID={self.USER};PWD={self.PASSWORD};Trusted Connection=yes;DATABASE={self.DATABASE}'\n",
    "\n",
    "       \n",
    "#         conn =  \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=IP_ADDRESS;DATABASE=DataLake;UID=USER;PWD=PASS\"\n",
    "        quoted = quote_plus(conn)\n",
    "        new_con = 'mssql+pyodbc:///?odbc_connect={}'.format(quoted)\n",
    "        self.engine = create_engine(new_con)\n",
    "#         print(self.engine)\n",
    "        try:\n",
    "            data.to_sql(table, self.engine,chunksize = None, **kwargs)\n",
    "            try:\n",
    "                self.execute(f'ALTER TABLE `{table}` ADD PRIMARY KEY (`id`);')\n",
    "            except:\n",
    "                pass\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception('Something went wrong inserting. Check trace.')\n",
    "            return False\n",
    "\n",
    "   \n",
    "   \n",
    "    def insert_(self, data, table, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Write records stored in a DataFrame to a SQL database.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The DataFrame that needs to be write to SQL database.\n",
    "            table (str): The table in which the rcords should be written to.\n",
    "            database (str): The database the table lies in. Leave it none if you\n",
    "                want use database during object creation.\n",
    "            kwargs: Keyword arguments for pandas to_sql function.\n",
    "                See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html\n",
    "                to know the arguments that can be passed.\n",
    "\n",
    "        Returns:\n",
    "            (bool) True is succesfully inserted, else false.\n",
    "        \"\"\"\n",
    "        logging.info(f'Inserting into {table}')\n",
    "\n",
    "        # # Use new database if a new databse is given\n",
    "        # if database is not None:\n",
    "        #     try:\n",
    "        #         config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "        #         engine = create_engine(config, pool_recycle=300)\n",
    "        #     except:\n",
    "        #         logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "        #         return False\n",
    "        # else:\n",
    "        #     engine = self.engine\n",
    "\n",
    "        if database is None:\n",
    "            database = 'karvy'\n",
    "        data = None\n",
    "        sql = query\n",
    "        user_ = self.USER\n",
    "        database = self.DATABASE\n",
    "        host_ = self.HOST\n",
    "        password_ = self.PASSWORD\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        # for pos, ind in enumerate(inds):\n",
    "        #     if pos % 2 == 0:\n",
    "        #         sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "        #     else:\n",
    "        #         sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "               \n",
    "        if connection_string:\n",
    "            print('connection string', connection_string)\n",
    "            print('database', database)\n",
    "            print(type(connection_string + database))\n",
    "            print(type(connection_string + database))\n",
    "\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + database)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pymssql.connect(host=host_,database=database,user=user_,password=password_)\n",
    "                else:\n",
    "                    conn = pymssql.connect(host=host_,database=database)\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pymssql.connect(host=host_,database=database)\n",
    "        try:\n",
    "            logging.debug(f'Query: {query}')\n",
    "            # data.to_sql(table, conn, **kwargs)\n",
    "            curs = conn.cursor(as_dict = True)\n",
    "            curs.execute(sql, tuple(kwargs.get('params', [])))\n",
    "            print('query executed')\n",
    "            try:\n",
    "                data = curs.fetchall()\n",
    "                data = pd.DataFrame(data)\n",
    "                print(data)\n",
    "            except Exception as e:\n",
    "                logging.debug('Update Query')\n",
    "\n",
    "            try:\n",
    "                self.execute(f'ALTER TABLE {table} ADD PRIMARY KEY (id);')\n",
    "            except:\n",
    "                pass\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception('Something went wrong inserting. Check trace.')\n",
    "            return False\n",
    "\n",
    "    def insert_dict(self, data, table):\n",
    "        \"\"\"\n",
    "        Insert dictionary into a SQL database table.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The DataFrame that needs to be write to SQL database.\n",
    "            table (str): The table in which the rcords should be written to.\n",
    "\n",
    "        Returns:\n",
    "            (bool) True is succesfully inserted, else false.\n",
    "        \"\"\"\n",
    "        logging.info(f'Inserting dictionary data into {table}...')\n",
    "        logging.debug(f'Data:\\n{data}')\n",
    "\n",
    "        try:\n",
    "            column_names = []\n",
    "            params = []\n",
    "\n",
    "            for column_name, value in data.items():\n",
    "                column_names.append(f'{column_name}')\n",
    "                params.append(value)\n",
    "\n",
    "            logging.debug(f'Column names: {column_names}')\n",
    "            logging.debug(f'Params: {params}')\n",
    "\n",
    "            columns_string = ', '.join(column_names)\n",
    "            param_placeholders = ', '.join(['%s'] * len(column_names))\n",
    "\n",
    "            query = f'INSERT INTO {table} ({columns_string}) VALUES ({param_placeholders})'\n",
    "\n",
    "            return self.execute(query, params=params)\n",
    "        except:\n",
    "            logging.exception('Error inserting data.')\n",
    "            return False\n",
    "\n",
    "    def update(self, table, update=None, where=None, database=None, force_update=False):\n",
    "        # Use new database if a new databse is given\n",
    "        # if database is not None:\n",
    "        #     try:\n",
    "        #         config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "        #         self.engine = create_engine(config, pool_recycle=300)\n",
    "        #     except:\n",
    "        #         logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "        #         return False\n",
    "\n",
    "        logging.info(f'Updating table: {table}')\n",
    "        logging.info(f'Update data: {update}')\n",
    "        logging.info(f'Where clause data: {where}')\n",
    "        logging.info(f'Force update flag: {force_update}')\n",
    "\n",
    "        try:\n",
    "            set_clause = []\n",
    "            set_value_list = []\n",
    "            where_clause = []\n",
    "            where_value_list = []\n",
    "\n",
    "            if where is not None and where:\n",
    "                for set_column, set_value in update.items():\n",
    "                    set_clause.append(f'{set_column}=%s')\n",
    "                    set_value_list.append(set_value)\n",
    "                set_clause_string = ', '.join(set_clause)\n",
    "            else:\n",
    "                logging.error(f'Update dictionary is None/empty. Must have some update clause.')\n",
    "                return False\n",
    "\n",
    "            if where is not None and where:\n",
    "                for where_column, where_value in where.items():\n",
    "                    where_clause.append(f'{where_column}=%s')\n",
    "                    where_value_list.append(where_value)\n",
    "                where_clause_string = ' AND '.join(where_clause)\n",
    "                query = f'UPDATE {table} SET {set_clause_string} WHERE {where_clause_string}'\n",
    "            else:\n",
    "                if force_update:\n",
    "                    query = f'UPDATE {table} SET {set_clause_string}'\n",
    "                else:\n",
    "                    message = 'Where dictionary is None/empty. If you want to force update every row, pass force_update as True.'\n",
    "                    logging.error(message)\n",
    "                    return False\n",
    "\n",
    "            params = set_value_list + where_value_list\n",
    "            self.execute(query, params=params)\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception('Something went wrong updating. Check trace.')\n",
    "            return False\n",
    "\n",
    "    def get_column_names(self, table, database=None):\n",
    "        \"\"\"\n",
    "        Get all column names from an SQL table.\n",
    "\n",
    "        Args:\n",
    "            table (str): Name of the table from which column names should be extracted.\n",
    "            database (str): Name of the database in which the table lies. Leave\n",
    "                it none if you want use database during object creation.\n",
    "\n",
    "        Returns:\n",
    "            (list) List of headers. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(f'Getting column names of table {table}')\n",
    "            return list(self.execute(f'SELECT * FROM {table}', database))\n",
    "        except:\n",
    "            logging.exception('Something went wrong getting column names. Check trace.')\n",
    "            return\n",
    "\n",
    "    def execute_default_index(self, query, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Executes an SQL query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that needs to be executed.\n",
    "            database (str): Name of the database to execute the query in. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            params (list/tuple/dict): List of parameters to pass to in the query.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data from the executed\n",
    "            query. (None if an error occurs)\n",
    "        \"\"\"\n",
    "\n",
    "        logging.debug(f'Executing `execute` instead of `execute_default_index`')\n",
    "        return self.execute(query, index_col=None, **kwargs)\n",
    "        data = None\n",
    "\n",
    "        # # Use new database if a new databse is given\n",
    "        # if database is not None:\n",
    "        #     try:\n",
    "        #         config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "        #         engine = create_engine(config, pool_recycle=300)\n",
    "        #     except:\n",
    "        #         logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "        #         return False\n",
    "        # else:\n",
    "        #     engine = self.engine\n",
    "\n",
    "        print('query', query)\n",
    "        if database is None:\n",
    "            database = 'karvy'\n",
    "        data = None\n",
    "        sql = query\n",
    "        user_ = self.USER\n",
    "        host_ = self.HOST\n",
    "        database = self.DATABASE\n",
    "        password_ = self.PASSWORD\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        for pos, ind in enumerate(inds):\n",
    "            if pos % 2 == 0:\n",
    "                sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "            else:\n",
    "                sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "               \n",
    "        if connection_string:\n",
    "            print('connection string', connection_string)\n",
    "            print('database', database)\n",
    "            print(type(connection_string + database))\n",
    "            print(type(connection_string + database))\n",
    "\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + database)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pymssql.connect(host=host_,database=database,user=user_,password=password_)\n",
    "                else:\n",
    "                    conn = pymssql.connect(host=host_,database=database)\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pymssql.connect(host=host_,database=database)\n",
    "\n",
    "        try:\n",
    "            logging.debug(f'Query: {query}')\n",
    "            # data.to_sql(table, conn, **kwargs)\n",
    "            curs = conn.cursor(as_dict = True)\n",
    "           \n",
    "            curs.execute(sql, tuple(kwargs.get('params', [])))\n",
    "            print('query executed')\n",
    "            try:\n",
    "                data = curs.fetchall()\n",
    "                data = pd.DataFrame(data)\n",
    "                print(data)\n",
    "            except Exception as e:\n",
    "                logging.debug('Update Query')\n",
    "            # data = pd.read_sql(query, conn, **kwargs)\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "        except exc.ResourceClosedError:\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception(f'Something went wrong while executing query. Check trace.')\n",
    "            params = kwargs['params'] if 'params' in kwargs else None\n",
    "            return False\n",
    "\n",
    "        return data.where((pd.notnull(data)), None).set_index('id')\n",
    "\n",
    "\n",
    "    def get_all(self, table, database=None, discard=None):\n",
    "        \"\"\"\n",
    "        Get all data from an SQL table.\n",
    "\n",
    "        Args:\n",
    "            table (str): Name of the table from which data should be extracted.\n",
    "            database (str): Name of the database in which the table lies. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            discard (list): columns to be excluded while selecting all\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data. (None if an error\n",
    "            occurs)\n",
    "        \"\"\"\n",
    "        logging.info(f'Getting all data from {table}')\n",
    "        if discard:\n",
    "            logging.info(f'Discarding columns {discard}')\n",
    "            columns = list(self.execute_default_index(f'SHOW COLUMNS FROM {table}',database).Field)\n",
    "            columns = [col for col in columns if col not in discard]\n",
    "            columns_str = json.dumps(columns).replace(\"'\",'').replace('\"','')[1:-1]\n",
    "            return self.execute(f'SELECT {columns_str} FROM {table}', database)\n",
    "\n",
    "        return self.execute(f'SELECT * FROM {table}', database)\n",
    "\n",
    "    def get_latest(self, data, group_by_col, sort_col):\n",
    "        \"\"\"\n",
    "        Group data by a column containing repeated values and get latest from it by\n",
    "        taking the latest value based on another column.\n",
    "\n",
    "        Example:\n",
    "        Get the latest products\n",
    "            id     product   date\n",
    "            220    6647     2014-09-01\n",
    "            220    6647     2014-10-16\n",
    "            826    3380     2014-11-11\n",
    "            826    3380     2015-05-19\n",
    "            901    4555     2014-09-01\n",
    "            901    4555     2014-11-01\n",
    "\n",
    "        The function will return\n",
    "            id     product   date\n",
    "            220    6647     2014-10-16\n",
    "            826    3380     2015-05-19\n",
    "            901    4555     2014-11-01\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): Pandas DataFrame to query on.\n",
    "            group_by_col (str): Column containing repeated values.\n",
    "            sort_col (str): Column to identify the latest record.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) Contains the latest records. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info('Grouping data...')\n",
    "            logging.info(f'Data: {data}')\n",
    "            logging.info(f'Group by column: {group_by_col}')\n",
    "            logging.info(f'Sort column: {sort_col}')\n",
    "            return data.sort_values(sort_col).set_index('id').groupby(group_by_col).tail(1)\n",
    "        except KeyError as e:\n",
    "            logging.error(f'Column {e.args[0]} does not exist.')\n",
    "            return None\n",
    "        except:\n",
    "            logging.exception('Something went wrong while grouping data.')\n",
    "            return None\n",
    "\n",
    "db_config = {\n",
    "   'host': '13.233.100.20',\n",
    "   'port': '1433',\n",
    "   'user': 'SA',\n",
    "   'password':'Akhil@Akhil1'\n",
    "}\n",
    "import os\n",
    "os.environ['HOST_IP'] = '13.233.100.20'\n",
    "os.environ['LOCAL_DB_USER']='SA'\n",
    "os.environ['LOCAL_DB_PASSWORD'] = 'Akhil@Akhil1'\n",
    "os.environ['LOCAL_DB_PORT'] = '1433'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T00:15:24.619280Z",
     "start_time": "2020-09-16T00:15:24.608387Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def debug_df(df, num=20):\n",
    "    df.printSchema()\n",
    "    df.show(num)\n",
    "    \n",
    "\n",
    "def decrease_date(s, days):\n",
    "    date = datetime.datetime.strptime(s, \"%Y-%m-%d\")\n",
    "    modified_date = date - datetime.timedelta(days=days)\n",
    "    return datetime.datetime.strftime(modified_date, \"%Y-%m-%d\")\n",
    "\n",
    "def read_df(table, columns_to_retrieve, database):\n",
    "    \n",
    "    query = f\"SELECT {','.join(columns_to_retrieve)} from {table}\"\n",
    "    # logging.info(f\"query to be executed is {query}\")\n",
    "    \n",
    "    data = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "            .option(\"query\", query) \\\n",
    "            .option(\"user\", user) \\\n",
    "            .option(\"password\", password) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .load()\n",
    "    return data\n",
    "\n",
    "from datetime import timedelta, date\n",
    "#\n",
    "def daterange(start_date, end_date):\n",
    "    start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield datetime.datetime.strftime(start_date + timedelta(n), \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T00:15:33.219369Z",
     "start_time": "2020-09-16T00:15:25.605227Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "# required libraries\n",
    "from pyspark import SparkContext, SparkConf #\n",
    "from pyspark.sql import SparkSession # for dataframe conversions\n",
    "# for type conversions\n",
    "from datetime import datetime\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, udf, sum # col, udf (user defined functions)\n",
    "from pyspark.sql.types import DateType, IntegerType # type\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import trim # for trimming\n",
    "from pyspark.sql.functions import collect_list, sort_array, row_number # for grouping and taking the last/first element\n",
    "from pyspark.sql.functions import *\n",
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import datetime \n",
    "\n",
    "\n",
    "# intialize spark\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 2)\n",
    "#Create spark context and sparksession\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "def populate_db_func(purred: pd.Series, units: pd.Series) -> pd.Series:\n",
    "    return pd.Series(np.where(purred=='P', units, 0))\n",
    "\n",
    "def populate_cr_func(purred: pd.Series, units: pd.Series) -> pd.Series:\n",
    "    return pd.Series(np.where(purred=='R', units, 0))\n",
    "\n",
    "populate_db = pandas_udf(populate_db_func, returnType=FloatType())\n",
    "populate_cr = pandas_udf(populate_cr_func, returnType=FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T00:15:33.231647Z",
     "start_time": "2020-09-16T00:15:33.221767Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# configs\n",
    "import os\n",
    "# os.environ['HOST_IP'] = '192.168.15.126'\n",
    "# os.environ['LOCAL_DB_USER'] = 'BRS'\n",
    "# os.environ['LOCAL_DB_PASSWORD'] = 'Kfintech123$'\n",
    "# os.environ['LOCAL_DB_PORT'] = '1433'\n",
    "\n",
    "import os\n",
    "\n",
    "# comment when using the configs from env file\n",
    "default_ip = '13.233.100.20'\n",
    "default_user  = 'SA'\n",
    "default_password = 'Akhil@Akhil1'\n",
    "default_port = '1433'\n",
    "default_tenant_id = 'karvy'\n",
    "\n",
    "# # initializations \n",
    "server = os.environ.get('HOST_IP', default_ip)\n",
    "port = os.environ.get('LOCAL_DB_PORT', default_port)\n",
    "user = os.environ.get('LOCAL_DB_USER', default_user)\n",
    "password = os.environ.get('LOCAL_DB_PASSWORD', default_password)\n",
    "\n",
    "db_config = {\n",
    "   'host': server,\n",
    "   'port': port,\n",
    "   'user': user,\n",
    "   'password':password\n",
    "}\n",
    "\n",
    "def save_metric(date, metric_name, metric_value, fund_name, group_level, database='BankRecon'):\n",
    "    db = DB(database, tenant_id='',**db_config)\n",
    "    try:\n",
    "        query = f\"INSERT INTO `karvy_metrics` VALUES ( '{date}','{metric_name}','{metric_value}', '{fund_name}', '{group_level}')\"\n",
    "        db.execute_(query)\n",
    "    except Exception as e:\n",
    "        logging.error(\"Unable to insert metrics data\")\n",
    "        logging.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T00:15:33.258235Z",
     "start_time": "2020-09-16T00:15:33.234392Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def initialize(date_str, table, database='funds', date_column='BatchCloseDate', tenant_id='karvy',\n",
    "              transaction_status='Active', purred = 'Purred', transaction_type = 'TransactionType',\n",
    "              folio = 'Folio', purchase_units = 'DB_Units', redemption_units = 'Cr_Units',scheme = 'SchemeCode',\n",
    "               plan = 'PlanCode',groupby_level='SP', direct_db=None, broker_column='BrokerARN'):\n",
    "    \"\"\"Initialization script which takes in batch_close_date and populates opening balance\n",
    "    Args:\n",
    "        date(str) The date upto which we need to initialize\n",
    "        table(str) \n",
    "        date_column(str)\n",
    "        tenant_id(str)\n",
    "    \n",
    "    Note: batch_close_date is (yyyy-MM-dd) (for ex. 2020-04-30), inclusive \n",
    "    \"\"\"\n",
    "    # fundtable column names\n",
    "    purchase_units = 'DB_Units'\n",
    "    redemption_units = 'Cr_Units'\n",
    "    scheme = 'SchemeCode'\n",
    "    plan = 'PlanCode'\n",
    "    \n",
    "    # configurations we use\n",
    "    batch_close_date = 'batch_close_date'\n",
    "    db_units = 'purchase_units'\n",
    "    cr_units = 'redemption_units'\n",
    "    balance_units = 'balance_units'\n",
    "    day_purchase_units = 'day_pu'\n",
    "    day_redemption_units = 'day_ru'\n",
    "    balance_purchase_units = 'balance_pu'\n",
    "    balance_redemption_units = 'balance_ru'\n",
    "    calculated_date = 'calculated_date'\n",
    "    \n",
    "#     inflow_db_units = 'inflow_purchase_units'\n",
    "#     inflow_cr_units = 'inflow_redemption_units'\n",
    "#     inflow_balance_units = 'inflow_balance_units'\n",
    "#     inflow_day_purchase_units = 'inflow_day_pu'\n",
    "#     inflow_day_redemption_units = 'inflow_day_ru'\n",
    "#     inflow_balance_purchase_units = 'inflow_balance_pu'\n",
    "#     inflow_balance_redemption_units = 'inflow_balance_ru'\n",
    "\n",
    "    \n",
    "#     outflow_db_units = 'outflow_purchase_units'\n",
    "#     outflow_cr_units = 'outflow_redemption_units'\n",
    "#     outlfow_balance_units = 'outlfow_balance_units'\n",
    "#     outlfow_day_purchase_units = 'outlfow_day_pu'\n",
    "#     outlfow_day_redemption_units = 'outlfow_day_ru'\n",
    "#     outlfow_balance_purchase_units = 'outlfow_balance_pu'\n",
    "#     outlfow_balance_redemption_units = 'outlfow_balance_ru'\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    database = direct_db or (f'{tenant_id or default_tenant_id}_{database}')\n",
    "    \n",
    "\n",
    "    # read data\n",
    "    data = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .load()\n",
    "    \n",
    "#     data = data.cache()\n",
    "    total_count = data.count()\n",
    "    # debug_df(data, 10)\n",
    "    \n",
    "    # some preprocessings in the data, additional trimmings etc\n",
    "    data = data.withColumn(transaction_status, upper(trim(col(transaction_status))))\n",
    "    data = data.withColumn(purred, upper(trim(col(purred))))\n",
    "    data = data.withColumn(folio, upper(trim(col(folio))))\n",
    "    data = data.withColumn(scheme, upper(trim(col(scheme))))\n",
    "    data = data.withColumn(plan, upper(trim(col(plan))))\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "#     data = data.filter((col(scheme).isin('IC','HC')))\n",
    "    \n",
    "#     print ('here')\n",
    "    \n",
    "    # cast the date column into dates, as we are concerned only with dates now\n",
    "    data = data.withColumn(date_column, col(date_column).cast('date'))\n",
    "    \n",
    "    # filter the date till the batch_close_date (inclusive)\n",
    "    data = data.filter(col(date_column) <= date_str)\n",
    "    \n",
    "    \n",
    "        \n",
    "    # filter the data according to rules\n",
    "    data = data.filter((col(date_column).isNotNull()) )\n",
    "    # data = data.filter((col(date_column) != '') ) # this will not work for few types\n",
    "    data = data.filter( ~trim(col(date_column)).cast(\"string\").eqNullSafe(''))\n",
    "    \n",
    "    # do must be rules\n",
    "    data = data.filter( (trim(upper((col(transaction_status))))) == \"Y\")\n",
    "    data = data.filter( (trim(upper(col(purred))) == \"P\") | (trim(upper(col(purred))) == \"R\") )\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "    # our configurations\n",
    "    data = data.withColumn(batch_close_date, data[date_column])\n",
    "    \n",
    "    \n",
    "#     broker = 'broker_code'\n",
    "#     # folios having single broker code will remain same\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     # bring in correct broker codes\n",
    "#     # for purchase p they will be same\n",
    "#     data_p = data_folio.filter(col(purred) == 'P')\n",
    "#     data_p = data_p.withColumn(broker, col(broker_column))\n",
    "    \n",
    "#     # for redemptions \n",
    "#     data_r = data_folio.filter(col(purred) == 'R')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     print ('pur, redem')\n",
    "    # bring in purchase and redemption units\n",
    "    data = data.withColumn(db_units, when((col(purred) == \"P\"), col(purchase_units)).otherwise(0))\n",
    "    data = data.withColumn(cr_units, when((col(purred) == \"R\"), col(redemption_units)).otherwise(0))\n",
    "    \n",
    "#     data = data.withColumn(db_units, populate_db(col(purred), col(purchase_units)))\n",
    "#     data = data.withColumn(cr_units, populate_cr(col(purred), col(redemption_units)))\n",
    "\n",
    "    \n",
    "#     print ('1')\n",
    "    # get the group by and window partitions based on partitions\n",
    "    group_by_cols = []\n",
    "    window_partition = []\n",
    "    if groupby_level == 'SP':\n",
    "        window_partition = [scheme, plan]\n",
    "        group_by_cols = [scheme, plan, batch_close_date]\n",
    "        # scheme_plan wise we might need to filter out some transaction types\n",
    "        ignored_tr_types = ['CNI', 'CNO', 'TRMI', \n",
    "                            'TRMO', 'TRFI', 'TRFO', 'PLDO',\n",
    "                            'UPLO', 'DMT', 'RMT', 'CNIR', 'CNOR', 'TRMIR', 'TRMOR',\n",
    "                            'TRFIR', 'TRFOR', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR']\n",
    "        data = data.filter( ~(col(transaction_type).isin(ignored_tr_types)) )\n",
    "    elif groupby_level == 'SPT':\n",
    "        window_partition = [scheme, plan, transaction_type]\n",
    "        group_by_cols  = [scheme, plan, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPF':\n",
    "        window_partition = [scheme, plan, folio]\n",
    "        group_by_cols  = [scheme, plan, folio, batch_close_date]\n",
    "    elif groupby_level == 'SPFB':\n",
    "            window_partition = [scheme, plan, folio, broker]\n",
    "            group_by_cols  = [scheme, plan, folio, broker, batch_close_date]\n",
    "    elif groupby_level == 'SPFT':\n",
    "        window_partition = [scheme, plan, folio, transaction_type]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPFTB':\n",
    "        window_partition = [scheme, plan, folio, transaction_type, broker]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, broker, batch_close_date]\n",
    "        \n",
    "        # populate broker code\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # roll up the data\n",
    "    rolledup_data = data.groupBy(group_by_cols)\n",
    "    rolledup_data = rolledup_data.agg({db_units:'sum', cr_units:'sum'})\n",
    "        \n",
    "    rolledup_data = rolledup_data.withColumnRenamed(f\"sum({db_units})\", day_purchase_units).withColumnRenamed(f\"sum({cr_units})\", day_redemption_units)\n",
    "    rolledup_data = rolledup_data.withColumn(balance_purchase_units, sum(col(day_purchase_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    rolledup_data = rolledup_data.withColumn(balance_redemption_units, sum(col(day_redemption_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    rolledup_data = rolledup_data.withColumn(balance_units, (col(balance_purchase_units) - col(balance_redemption_units)))\n",
    "    \n",
    "#     print ('rolling up done')\n",
    "    # get the latest data\n",
    "    latest_data = rolledup_data.filter(col(batch_close_date) <= date_str)\n",
    "    w = Window.partitionBy(window_partition).orderBy(col(batch_close_date).desc())\n",
    "    latest_data = latest_data.withColumn(\"rrn\", row_number().over(w)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    \n",
    "    # maintained the calculated date (the latest data upto the calculated date)\n",
    "    latest_data = latest_data.withColumn(calculated_date, lit(date_str).cast('date'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # store in parquet file for optimization of space and only one file and multi partitions\n",
    "    # but write now store in csv and maintain date wise and colesce one\n",
    "    latest_data = latest_data.cache()\n",
    "    \n",
    "    \n",
    "#     latest_data.show()\n",
    "    latest_data.coalesce(1).write.csv(f\"{table}_latest/data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "    latest_data.coalesce(1).write.parquet(f\"{table}_latest/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    print (f'inital file on date {date_str} written')\n",
    "    return total_count\n",
    "\n",
    "# s=time.time()\n",
    "# initialize('2020-04-05', table=table,direct_db=direct_db, groupby_level=groupby_level)\n",
    "# print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T00:15:33.283283Z",
     "start_time": "2020-09-16T00:15:33.260571Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 µs, sys: 0 ns, total: 9 µs\n",
      "Wall time: 13.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def initialize(date_str, table, database='funds', date_column='BatchCloseDate', tenant_id='karvy',\n",
    "              transaction_status='Active', purred = 'Purred', transaction_type = 'TransactionType',\n",
    "              folio = 'Folio', purchase_units = 'DB_Units', redemption_units = 'Cr_Units',scheme = 'SchemeCode',\n",
    "               plan = 'PlanCode',groupby_level='SP', direct_db=None, broker_column='BrokerARN'):\n",
    "    \"\"\"Initialization script which takes in batch_close_date and populates opening balance\n",
    "    Args:\n",
    "        date(str) The date upto which we need to initialize\n",
    "        table(str) \n",
    "        date_column(str)\n",
    "        tenant_id(str)\n",
    "    \n",
    "    Note: batch_close_date is (yyyy-MM-dd) (for ex. 2020-04-30), inclusive \n",
    "    \"\"\"\n",
    "    # fundtable column names\n",
    "    purchase_units = 'DB_Units'\n",
    "    redemption_units = 'Cr_Units'\n",
    "    scheme = 'SchemeCode'\n",
    "    plan = 'PlanCode'\n",
    "    \n",
    "    # configurations we use\n",
    "    batch_close_date = 'batch_close_date'\n",
    "    db_units = 'purchase_units'\n",
    "    cr_units = 'redemption_units'\n",
    "    balance_units = 'balance_units'\n",
    "    day_purchase_units = 'day_pu'\n",
    "    day_redemption_units = 'day_ru'\n",
    "    balance_purchase_units = 'balance_pu'\n",
    "    balance_redemption_units = 'balance_ru'\n",
    "    calculated_date = 'calculated_date'\n",
    "    \n",
    "#     inflow_db_units = 'inflow_purchase_units'\n",
    "#     inflow_cr_units = 'inflow_redemption_units'\n",
    "#     inflow_balance_units = 'inflow_balance_units'\n",
    "#     inflow_day_purchase_units = 'inflow_day_pu'\n",
    "#     inflow_day_redemption_units = 'inflow_day_ru'\n",
    "#     inflow_balance_purchase_units = 'inflow_balance_pu'\n",
    "#     inflow_balance_redemption_units = 'inflow_balance_ru'\n",
    "\n",
    "    \n",
    "#     outflow_db_units = 'outflow_purchase_units'\n",
    "#     outflow_cr_units = 'outflow_redemption_units'\n",
    "#     outlfow_balance_units = 'outlfow_balance_units'\n",
    "#     outlfow_day_purchase_units = 'outlfow_day_pu'\n",
    "#     outlfow_day_redemption_units = 'outlfow_day_ru'\n",
    "#     outlfow_balance_purchase_units = 'outlfow_balance_pu'\n",
    "#     outlfow_balance_redemption_units = 'outlfow_balance_ru'\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    database = direct_db or (f'{tenant_id or default_tenant_id}_{database}')\n",
    "    \n",
    "\n",
    "    # read data\n",
    "    data = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .load()\n",
    "    \n",
    "#     data = data.cache()\n",
    "    total_count = data.count()\n",
    "    # debug_df(data, 10)\n",
    "    \n",
    "    # some preprocessings in the data, additional trimmings etc\n",
    "    data = data.withColumn(transaction_status, upper(trim(col(transaction_status))))\n",
    "    data = data.withColumn(purred, upper(trim(col(purred))))\n",
    "    data = data.withColumn(folio, upper(trim(col(folio))))\n",
    "    data = data.withColumn(scheme, upper(trim(col(scheme))))\n",
    "    data = data.withColumn(plan, upper(trim(col(plan))))\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "#     data = data.filter((col(scheme).isin('IC','HC')))\n",
    "    \n",
    "#     print ('here')\n",
    "    \n",
    "    # cast the date column into dates, as we are concerned only with dates now\n",
    "    data = data.withColumn(date_column, col(date_column).cast('date'))\n",
    "    \n",
    "    # filter the date till the batch_close_date (inclusive)\n",
    "    data = data.filter(col(date_column) <= date_str)\n",
    "    \n",
    "    \n",
    "        \n",
    "    # filter the data according to rules\n",
    "    data = data.filter((col(date_column).isNotNull()) )\n",
    "    # data = data.filter((col(date_column) != '') ) # this will not work for few types\n",
    "    data = data.filter( ~trim(col(date_column)).cast(\"string\").eqNullSafe(''))\n",
    "    \n",
    "    # do must be rules\n",
    "    data = data.filter( (trim(upper((col(transaction_status))))) == \"Y\")\n",
    "    data = data.filter( (trim(upper(col(purred))) == \"P\") | (trim(upper(col(purred))) == \"R\") )\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "    # our configurations\n",
    "    data = data.withColumn(batch_close_date, data[date_column])\n",
    "    \n",
    "    \n",
    "#     broker = 'broker_code'\n",
    "#     # folios having single broker code will remain same\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     # bring in correct broker codes\n",
    "#     # for purchase p they will be same\n",
    "#     data_p = data_folio.filter(col(purred) == 'P')\n",
    "#     data_p = data_p.withColumn(broker, col(broker_column))\n",
    "    \n",
    "#     # for redemptions \n",
    "#     data_r = data_folio.filter(col(purred) == 'R')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     print ('pur, redem')\n",
    "    # bring in purchase and redemption units\n",
    "    data = data.withColumn(db_units, when((col(purred) == \"P\"), col(purchase_units)).otherwise(0))\n",
    "    data = data.withColumn(cr_units, when((col(purred) == \"R\"), col(redemption_units)).otherwise(0))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#     print ('1')\n",
    "    # get the group by and window partitions based on partitions\n",
    "    group_by_cols = []\n",
    "    window_partition = []\n",
    "    if groupby_level == 'SP':\n",
    "        window_partition = [scheme, plan]\n",
    "        group_by_cols = [scheme, plan, batch_close_date]\n",
    "        # scheme_plan wise we might need to filter out some transaction types\n",
    "        ignored_tr_types = ['CNI', 'CNO', 'TRMI', \n",
    "                            'TRMO', 'TRFI', 'TRFO', 'PLDO',\n",
    "                            'UPLO', 'DMT', 'RMT', 'CNIR', 'CNOR', 'TRMIR', 'TRMOR',\n",
    "                            'TRFIR', 'TRFOR', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR']\n",
    "        data = data.filter( ~(col(transaction_type).isin(ignored_tr_types)) )\n",
    "    elif groupby_level == 'SPT':\n",
    "        window_partition = [scheme, plan, transaction_type]\n",
    "        group_by_cols  = [scheme, plan, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPF':\n",
    "        window_partition = [scheme, plan, folio]\n",
    "        group_by_cols  = [scheme, plan, folio, batch_close_date]\n",
    "    elif groupby_level == 'SPFB':\n",
    "            window_partition = [scheme, plan, folio, broker]\n",
    "            group_by_cols  = [scheme, plan, folio, broker, batch_close_date]\n",
    "    elif groupby_level == 'SPFT':\n",
    "        window_partition = [scheme, plan, folio, transaction_type]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPFTB':\n",
    "        window_partition = [scheme, plan, folio, transaction_type, broker]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, broker, batch_close_date]\n",
    "        \n",
    "        # populate broker code\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # roll up the data\n",
    "    rolledup_data = data.groupBy(group_by_cols)\n",
    "    rolledup_data = rolledup_data.agg({db_units:'sum', cr_units:'sum'})\n",
    "        \n",
    "    rolledup_data = rolledup_data.withColumnRenamed(f\"sum({db_units})\", day_purchase_units).withColumnRenamed(f\"sum({cr_units})\", day_redemption_units)\n",
    "    rolledup_data = rolledup_data.withColumn(balance_purchase_units, sum(col(day_purchase_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    rolledup_data = rolledup_data.withColumn(balance_redemption_units, sum(col(day_redemption_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    rolledup_data = rolledup_data.withColumn(balance_units, (col(balance_purchase_units) - col(balance_redemption_units)))\n",
    "    \n",
    "#     print ('rolling up done')\n",
    "    # get the latest data\n",
    "    latest_data = rolledup_data.filter(col(batch_close_date) <= date_str)\n",
    "    w = Window.partitionBy(window_partition).orderBy(col(batch_close_date).desc())\n",
    "    latest_data = latest_data.withColumn(\"rrn\", row_number().over(w)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    \n",
    "    # maintained the calculated date (the latest data upto the calculated date)\n",
    "    latest_data = latest_data.withColumn(calculated_date, lit(date_str).cast('date'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # store in parquet file for optimization of space and only one file and multi partitions\n",
    "    # but write now store in csv and maintain date wise and colesce one\n",
    "    latest_data = latest_data.cache()\n",
    "    \n",
    "    \n",
    "#     latest_data.show()\n",
    "    latest_data.coalesce(1).write.csv(f\"{table}_latest/data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "    latest_data.coalesce(1).write.parquet(f\"{table}_latest/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    print (f'inital file on date {date_str} written')\n",
    "    return total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:59:30.495557Z",
     "start_time": "2020-09-16T09:59:30.486448Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "start_date = '2020-04-01'\n",
    "date_str = '2020-05-02'\n",
    "table='m_Trans_116'\n",
    "trans_table = 'trans_116'\n",
    "database='funds'\n",
    "date_column='BatchCloseDate'\n",
    "tenant_id='karvy'\n",
    "transaction_status='Active'\n",
    "purred = 'Purred'\n",
    "transaction_type = 'TransactionType'\n",
    "folio = 'Folio'\n",
    "purchase_units = 'DB_Units'\n",
    "redemption_units = 'Cr_Units'\n",
    "scheme = 'SchemeCode'\n",
    "plan = 'PlanCode'\n",
    "groupby_level='SPFT'\n",
    "direct_db=None\n",
    "fn_fromdt = 'fn_fromdt'\n",
    "fn_fromdt_format = 'dd/MM/yyyy'\n",
    "fn_scheme = 'fn_scheme'\n",
    "fn_plan = 'fn_plan'\n",
    "fn_nav = 'fn_nav'\n",
    "nav_table='nav_master'\n",
    "scheme_table='scheme_master'\n",
    "scheme_code = 'fm_scheme'\n",
    "plan_code = 'fm_plan'\n",
    "category = 'fm_SebiSchemeCategory'\n",
    "subcategory = 'fm_SebiSchemeSubCategory'\n",
    "nature = 'fm_nature'\n",
    "newmcrid='fm_NewMCRId'\n",
    "groupby_level = 'SPFT'\n",
    "direct_db = 'kfintech_funds'\n",
    "broker_column = 'BrokerARN'\n",
    "transaction_no = 'TransactionNo'\n",
    "purchase_transaction_no = 'PurchaseTransactionNo'\n",
    "ihno = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T10:46:17.491612Z",
     "start_time": "2020-09-16T10:46:17.416543Z"
    },
    "code_folding": [
     52,
     59,
     62,
     65,
     68,
     71
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-69-5792b333c225>, line 167)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-69-5792b333c225>\"\u001b[0;36m, line \u001b[0;32m167\u001b[0m\n\u001b[0;31m    roll up the data\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "inflow_db_trtypes = ['NEW', 'ADD', 'IPO', 'SIN', 'LTIN', 'LTIA']\n",
    "inflow_cr_trtypes = ['NEWR', 'ADDR', 'IPOR', 'SINR', 'LTINR', 'LTIAR']\n",
    "\n",
    "outflow_db_trtypes = ['FULR', 'REDR', 'LTOFR', 'LTIAR']\n",
    "outflow_cr_trtypes = ['FUL', 'RED', 'LTOF', 'LTOP']\n",
    "s = time.time()\n",
    "\n",
    "# configurations we use\n",
    "batch_close_date = 'batch_close_date'\n",
    "db_units = 'purchase_units'\n",
    "cr_units = 'redemption_units'\n",
    "balance_units = 'balance_units'\n",
    "day_purchase_units = 'day_pu'\n",
    "day_redemption_units = 'day_ru'\n",
    "balance_purchase_units = 'balance_pu'\n",
    "balance_redemption_units = 'balance_ru'\n",
    "calculated_date = 'calculated_date'\n",
    "today_pu = 'today_pu'\n",
    "today_ru = 'today_ru'\n",
    "effective_nav = 'effective_nav'\n",
    "aum = 'aum'\n",
    "aaum = 'aaum'\n",
    "inflow = 'inflow'\n",
    "outflow = 'outflow'\n",
    "inflow_db_units = 'inflow_purchase_units'\n",
    "inflow_cr_units = 'inflow_redemption_units'\n",
    "outflow_db_units = 'outflow_purchase_units'\n",
    "outflow_cr_units = 'outflow_redemption_units'\n",
    "inflow_units = 'inflow_units'\n",
    "outflow_units = 'outflow_units'\n",
    "\n",
    "\n",
    "# get the latest data from the previously stored file\n",
    "date_obj = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "previous_day = date_obj - datetime.timedelta(1)\n",
    "previous_day_str = previous_day.strftime('%Y-%m-%d')\n",
    "latest_data = spark.read.parquet(f\"{table}_latest/data_{groupby_level}_{previous_day_str}.parquet\")\n",
    "\n",
    "# get  the  data in between\n",
    "database = direct_db or (f'{tenant_id or default_tenant_id}_{database}')\n",
    "query = f\"SELECT * from {table} where CAST({date_column} AS DATE) <'{date_str}' and CAST({date_column} AS DATE) >='{start_date}'\"\n",
    "data  = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "        .option(\"query\", query) \\\n",
    "        .option(\"user\", user) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "        .load()\n",
    "\n",
    "group_by_cols = []\n",
    "window_partition = []\n",
    "if groupby_level == 'SP':\n",
    "    window_partition = [scheme, plan]\n",
    "    group_by_cols = [scheme, plan, batch_close_date]\n",
    "    # scheme_plan wise we might need to filter out some transaction types\n",
    "    ignored_tr_types = ['CNI', 'CNO', 'TRMI', 'TRMO', 'TRFI', 'TRFO', 'PLDO', 'UPLO',\n",
    "                        'DMT', 'RMT', 'CNIR', 'CNOR', 'TRMIR', 'TRMOR', 'TRFIR', 'TRFOR', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR']\n",
    "    data = data.filter( ~(col(transaction_type).isin(ignored_tr_types)) )\n",
    "elif groupby_level == 'SPT':\n",
    "    window_partition = [scheme, plan, transaction_type]\n",
    "    group_by_cols  = [scheme, plan, transaction_type, batch_close_date]\n",
    "elif groupby_level == 'SPF':\n",
    "    window_partition = [scheme, plan, folio]\n",
    "    group_by_cols  = [scheme, plan, folio, batch_close_date]\n",
    "elif groupby_level == 'SPFB':\n",
    "        window_partition = [scheme, plan, folio, broker]\n",
    "        group_by_cols  = [scheme, plan, folio, broker, batch_close_date]\n",
    "elif groupby_level == 'SPFT':\n",
    "    window_partition = [scheme, plan, folio, transaction_type]\n",
    "    group_by_cols = [scheme, plan, folio, transaction_type, batch_close_date]\n",
    "elif groupby_level == 'SPFTB':\n",
    "    window_partition = [scheme, plan, folio, transaction_type, broker]\n",
    "    group_by_cols = [scheme, plan, folio, transaction_type, broker, batch_close_date]\n",
    "\n",
    "\n",
    "# calculate all the steps as in initialization\n",
    "# some preprocessings in the data, additional trimmings etc\n",
    "data = data.withColumn(transaction_status, upper(trim(col(transaction_status))))\n",
    "data = data.withColumn(purred, upper(trim(col(purred))))\n",
    "# data = data.withColumn(folio, upper(trim(col(folio))))\n",
    "data = data.withColumn(scheme, upper(trim(col(scheme))))\n",
    "data = data.withColumn(plan, upper(trim(col(plan))))\n",
    "data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "\n",
    "#     data = data.filter((col(scheme).isin('IC','HC')))\n",
    "# cast the date column into dates, as we are concerned only with dates now\n",
    "data = data.withColumn(date_column, col(date_column).cast('date'))\n",
    "# filter the data according to rules\n",
    "data = data.filter((col(date_column).isNotNull()) )\n",
    "# data = data.filter((col(date_column) != '') ) # this will not work for few types\n",
    "data = data.filter( ~trim(col(date_column)).cast(\"string\").eqNullSafe(''))\n",
    "\n",
    "# our configurations\n",
    "data = data.withColumn(batch_close_date, data[date_column])\n",
    "\n",
    "# latest_data = latest_data.repartition(2,*group_by_cols)\n",
    "# data = data.repartition(2,*group_by_cols)\n",
    "\n",
    "\n",
    "\n",
    "# do must be rules\n",
    "data = data.filter( (trim(upper((col(transaction_status))))) == \"Y\")\n",
    "data = data.filter( (trim(upper(col(purred))) == \"P\") | (trim(upper(col(purred))) == \"R\") )\n",
    "data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "broker = 'broker_code'\n",
    "# folios having single broker code will remain same\n",
    "multi_broker_folio = data.groupby(folio, scheme, plan).count().where(col('count') > 1)\n",
    "single_broker_folio_data = data.join(multi_broker_folio.select(folio), on=[folio], how='left_anti')\n",
    "single_broker_data = single_broker_folio_data.withColumn(broker, col(broker_column))\n",
    "\n",
    "multi_broker_folio_data = data.join(multi_broker_folio.select(folio), on=[folio], how='left_semi')\n",
    "# bring in correct broker codes\n",
    "# for purchase p they will be same\n",
    "data_p = multi_broker_folio_data.filter(col(purred) == 'P')\n",
    "data_p = data_p.withColumn(broker, col(broker_column))\n",
    "\n",
    "# for redemptions \n",
    "data_r = multi_broker_folio_data.filter(col(purred) == 'R')\n",
    "# bring in the redemptions \n",
    "\n",
    "# get the transdata\n",
    "database = direct_db or (f'{tenant_id or default_tenant_id}_{database}')\n",
    "query = f\"SELECT * from {trans_table}\"\n",
    "trans_data  = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "        .option(\"query\", query) \\\n",
    "        .option(\"user\", user) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "        .load()\n",
    "\n",
    "# get the redemptions\n",
    "trans_data = trans_data.join(data_r, on=[scheme, plan, folio, transaction_no], how='left_semi')\n",
    "\n",
    "print ('after filtering')\n",
    "# merge the trans and m_trans\n",
    "grouped = data_p.groupby([transaction_no, broker_column]).count()\n",
    "trans_data = trans_data.drop(date_column)\n",
    "\n",
    "\n",
    "trans_data = trans_data.select(scheme, plan, folio, transaction_no, purchase_transaction_no).join(data_r,\n",
    "                             on=[scheme, plan, folio, transaction_no]).drop(transaction_no)\n",
    "\n",
    "trans_data = trans_data.join(data_p.select(transaction_no, broker), trans_data[purchase_transaction_no] == data_p[transaction_no], how='left').drop(purchase_transaction_no)\n",
    "\n",
    "\n",
    "\n",
    "# bring in purchase and redemption units\n",
    "data = data.withColumn(db_units, when((col(purred) == \"P\"), col(purchase_units)).otherwise(0))\n",
    "data = data.withColumn(cr_units, when((col(purred) == \"R\"), col(redemption_units)).otherwise(0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# roll up the data\n",
    "# rolledup_data = data.groupBy(group_by_cols)\n",
    "# rolledup_data = rolledup_data.agg({db_units:'sum', cr_units:'sum'})\n",
    "\n",
    "# rolledup_data = rolledup_data.withColumnRenamed(f\"sum({db_units})\", day_purchase_units).withColumnRenamed(f\"sum({cr_units})\", day_redemption_units)\n",
    "\n",
    "\n",
    "# latest_data = latest_data.drop(day_purchase_units, day_redemption_units, batch_close_date, balance_units)\n",
    "# latest_data = latest_data.withColumnRenamed(balance_purchase_units, day_purchase_units)\n",
    "# latest_data = latest_data.withColumnRenamed(balance_redemption_units, day_redemption_units)\n",
    "# latest_data = latest_data.withColumnRenamed(calculated_date, batch_close_date)\n",
    "\n",
    "# # debug_df(rolledup_data)\n",
    "# # debug_df(latest_data)\n",
    "# combined_data = latest_data.union(rolledup_data.select(latest_data.columns))\n",
    "\n",
    "# combined_data = combined_data.withColumn(balance_purchase_units, sum(col(day_purchase_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "# combined_data = combined_data.withColumn(balance_redemption_units, sum(col(day_redemption_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "# combined_data = combined_data.withColumn(balance_units, (col(balance_purchase_units) - col(balance_redemption_units)))\n",
    "\n",
    "\n",
    "\n",
    "# # join the nav, scheme_master data\n",
    "# nav_data = read_df(nav_table, '*', database)\n",
    "# nav_data = nav_data.withColumnRenamed(fn_scheme, scheme)\n",
    "# nav_data = nav_data.withColumnRenamed(fn_plan, plan)\n",
    "# nav_data = nav_data.withColumn(fn_fromdt, col(fn_fromdt).cast('date'))\n",
    "\n",
    "# scheme_master = read_df(scheme_table, [scheme_code, plan_code, nature, category, subcategory, newmcrid], database)\n",
    "# scheme_master = scheme_master.withColumnRenamed(scheme_code, scheme)\n",
    "# scheme_master = scheme_master.withColumnRenamed(plan_code, plan)\n",
    "\n",
    "# scheme_master_ = scheme_master.dropDuplicates([scheme, plan])\n",
    "# nav_scheme = nav_data.join(scheme_master_, on=[scheme, plan], how='left')\n",
    "# nav_data = nav_scheme\n",
    "# combined_data = combined_data.cache()\n",
    "# nav_data = nav_data.cache()\n",
    "# final_filtered = None\n",
    "# final_nav = None\n",
    "\n",
    "# date_obj = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "# previous_day = date_obj - datetime.timedelta(1)\n",
    "# previous_day_str = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "# filtered = combined_data.filter(col(batch_close_date) <= date_str)\n",
    "# w = Window.partitionBy(window_partition).orderBy(col(batch_close_date).desc())\n",
    "# row_numbered = filtered.withColumn(\"rrn\", row_number().over(w)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    \n",
    "\n",
    "# row_numbered.coalesce(1).write.parquet(f\"{table}_fastlatest/data_{groupby_level}_{previous_day_str}.parquet\", mode='overwrite')\n",
    "\n",
    "\n",
    "# for day in daterange(start_date, date_str):\n",
    "#     print (day)\n",
    "#     filtered = combined_data.filter(col(batch_close_date) <= day)\n",
    "#     w = Window.partitionBy(window_partition).orderBy(col(batch_close_date).desc())\n",
    "#     row_numbered = filtered.withColumn(\"rrn\", row_number().over(w)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    \n",
    "# #     row_numbered.coalesce(1).write.csv(f\"{table}_fastlatest/data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "# #     row_numbered.coalesce(1).write.parquet(f\"{table}_fastlatest/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    \n",
    "#     combined_data = combined_data.withColumn(calculated_date, lit(day).cast('date'))\n",
    "#     combined_data = combined_data.withColumn(effective_nav, date_sub(col(calculated_date), 1))\n",
    "#     combined_data = combined_data.withColumn(today_pu, when((col(calculated_date) == col(batch_close_date)), col(day_purchase_units)).otherwise(0))\n",
    "#     combined_data = combined_data.withColumn(today_ru, when((col(calculated_date) == col(batch_close_date)), col(day_redemption_units)).otherwise(0))\n",
    "    \n",
    "#     # inflow outflow addition\n",
    "#     inflow_db_condition = col(transaction_type).isin(inflow_db_trtypes)\n",
    "#     inflow_cr_condition = col(transaction_type).isin(inflow_cr_trtypes)\n",
    "#     combined_data = combined_data.withColumn(inflow_db_units, when(inflow_db_condition, col(today_pu)).otherwise(0))\n",
    "#     combined_data = combined_data.withColumn(inflow_cr_units, when(inflow_cr_condition, col(today_ru)).otherwise(0))\n",
    "\n",
    "#     outflow_db_condition = col(transaction_type).isin(outflow_db_trtypes)\n",
    "#     outflow_cr_condition = col(transaction_type).isin(outflow_cr_trtypes)\n",
    "#     combined_data = combined_data.withColumn(outflow_db_units, when(outflow_db_condition, col(today_pu)).otherwise(0))\n",
    "#     combined_data = combined_data.withColumn(outflow_cr_units, when(outflow_cr_condition, col(today_ru)).otherwise(0))\n",
    "\n",
    "#     combined_data = combined_data.withColumn(inflow_units, col(inflow_db_units) - col(inflow_cr_units))\n",
    "#     combined_data = combined_data.withColumn(outflow_units, col(outflow_cr_units) - col(outflow_db_units))\n",
    "    \n",
    "#     if final_filtered:\n",
    "#         final_filtered = final_filtered.union(combined_data)\n",
    "#     else:\n",
    "#         final_filtered = combined_data\n",
    "        \n",
    "#     nav_filteredFT = nav_data.filter(col(fn_fromdt) < day)\n",
    "#     navw = Window.partitionBy([scheme, plan]).orderBy(col(fn_fromdt).desc())\n",
    "#     nav_populate = nav_filteredFT.withColumn(\"rrn\", row_number().over(navw)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "#     nav_populate = nav_populate.withColumn(calculated_date, lit(date_str))\n",
    "#     nav_populate = nav_populate.select([scheme, plan, fn_fromdt, fn_nav, calculated_date, category, subcategory, nature, newmcrid])\n",
    "    \n",
    "    \n",
    "#     if final_nav:\n",
    "#         final_nav = final_nav.union(nav_populate)\n",
    "#     else:\n",
    "#         final_nav = nav_populate\n",
    "        \n",
    "# joined = final_filtered.join(final_nav, on=[scheme, plan, calculated_date], how='left')\n",
    "# joined = joined.withColumn(aum, col(fn_nav) * col(balance_units))\n",
    "# joined = joined.withColumn(inflow, col(fn_nav) * col(inflow_units))\n",
    "# joined = joined.withColumn(outflow, col(fn_nav) * col(outflow_units))\n",
    "\n",
    "\n",
    "# joined.write.parquet(f\"{table}_fastdialy/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "print (f'total time {time.time()-s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T10:09:42.562277Z",
     "start_time": "2020-09-16T10:09:42.556154Z"
    }
   },
   "outputs": [],
   "source": [
    "a= (['Folio', 'SchemeCode', 'PlanCode', 'BrokerARN', 'SubBrokerARN', 'IHNo', 'TransactionNo', 'TransactionType', 'BranchCode', 'S_BranchCode', 'Proxy_BranchCode', 'DB_Units', 'Cr_Units', 'NavDate', 'NAV', 'TransactionDate', 'ProcessDate', 'BatchCloseDate', 'EndorsementDate', 'Active', 'Purred', 'TERFlag', 'F23', 'batch_close_date', 'broker_code'])\n",
    "b=(['SchemeCode', 'PlanCode', 'Folio', 'BrokerARN', 'SubBrokerARN', 'IHNo', 'TransactionType', 'BranchCode', 'S_BranchCode', 'Proxy_BranchCode', 'DB_Units', 'Cr_Units', 'NavDate', 'NAV', 'TransactionDate', 'ProcessDate', 'BatchCloseDate', 'EndorsementDate', 'Active', 'Purred', 'TERFlag', 'F23', 'batch_close_date', 'TransactionNo', 'broker_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T10:11:05.599402Z",
     "start_time": "2020-09-16T10:11:05.577922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Active', 'BatchCloseDate', 'BranchCode', 'BrokerARN', 'Cr_Units', 'DB_Units', 'EndorsementDate', 'F23', 'Folio', 'IHNo', 'NAV', 'NavDate', 'PlanCode', 'ProcessDate', 'Proxy_BranchCode', 'Purred', 'S_BranchCode', 'SchemeCode', 'SubBrokerARN', 'TERFlag', 'TransactionDate', 'TransactionNo', 'TransactionType', 'batch_close_date', 'broker_code']\n",
      "['Active', 'BatchCloseDate', 'BranchCode', 'BrokerARN', 'Cr_Units', 'DB_Units', 'EndorsementDate', 'F23', 'Folio', 'IHNo', 'NAV', 'NavDate', 'PlanCode', 'ProcessDate', 'Proxy_BranchCode', 'Purred', 'S_BranchCode', 'SchemeCode', 'SubBrokerARN', 'TERFlag', 'TransactionDate', 'TransactionNo', 'TransactionType', 'batch_close_date', 'broker_code']\n",
      "0 Active Active True\n",
      "1 BatchCloseDate BatchCloseDate True\n",
      "2 BranchCode BranchCode True\n",
      "3 BrokerARN BrokerARN True\n",
      "4 Cr_Units Cr_Units True\n",
      "5 DB_Units DB_Units True\n",
      "6 EndorsementDate EndorsementDate True\n",
      "7 F23 F23 True\n",
      "8 Folio Folio True\n",
      "9 IHNo IHNo True\n",
      "10 NAV NAV True\n",
      "11 NavDate NavDate True\n",
      "12 PlanCode PlanCode True\n",
      "13 ProcessDate ProcessDate True\n",
      "14 Proxy_BranchCode Proxy_BranchCode True\n",
      "15 Purred Purred True\n",
      "16 S_BranchCode S_BranchCode True\n",
      "17 SchemeCode SchemeCode True\n",
      "18 SubBrokerARN SubBrokerARN True\n",
      "19 TERFlag TERFlag True\n",
      "20 TransactionDate TransactionDate True\n",
      "21 TransactionNo TransactionNo True\n",
      "22 TransactionType TransactionType True\n",
      "23 batch_close_date batch_close_date True\n",
      "24 broker_code broker_code True\n"
     ]
    }
   ],
   "source": [
    "print (sorted(a))\n",
    "print (sorted(b))\n",
    "\n",
    "for i,ele in enumerate(sorted(a)):\n",
    "    print (i,ele, sorted(b)[i], ele==sorted(b)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    "\n",
    "root\n",
    " |-- SchemeCode: string (nullable = true)\n",
    " |-- PlanCode: string (nullable = true)\n",
    " |-- BrokerARN: string (nullable = true)\n",
    " |-- SubBrokerARN: string (nullable = true)\n",
    " |-- IHNo: double (nullable = true)\n",
    " |-- TransactionType: string (nullable = true)\n",
    " |-- BranchCode: string (nullable = true)\n",
    " |-- S_BranchCode: string (nullable = true)\n",
    " |-- Proxy_BranchCode: string (nullable = true)\n",
    " |-- DB_Units: double (nullable = true)\n",
    " |-- Cr_Units: double (nullable = true)\n",
    " |-- NavDate: timestamp (nullable = true)\n",
    " |-- NAV: double (nullable = true)\n",
    " |-- TransactionDate: timestamp (nullable = true)\n",
    " |-- ProcessDate: timestamp (nullable = true)\n",
    " |-- BatchCloseDate: date (nullable = true)\n",
    " |-- EndorsementDate: timestamp (nullable = true)\n",
    " |-- Active: string (nullable = true)\n",
    " |-- Purred: string (nullable = true)\n",
    " |-- TERFlag: string (nullable = true)\n",
    " |-- F23: string (nullable = true)\n",
    " |-- batch_close_date: date (nullable = true)\n",
    " |-- TransactionNo: integer (nullable = true)\n",
    " |-- broker_code: string (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:37:43.567089Z",
     "start_time": "2020-09-16T09:37:43.562898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SchemeCode: string (nullable = true)\n",
      " |-- PlanCode: string (nullable = true)\n",
      " |-- Folio: decimal(15,0) (nullable = true)\n",
      " |-- TransactionNo: integer (nullable = true)\n",
      " |-- BrokerARN: string (nullable = true)\n",
      " |-- SubBrokerARN: string (nullable = true)\n",
      " |-- TransactionType: string (nullable = true)\n",
      " |-- BranchCode: string (nullable = true)\n",
      " |-- DB_Units: double (nullable = true)\n",
      " |-- Cr_Units: double (nullable = true)\n",
      " |-- NAV: double (nullable = true)\n",
      " |-- TransactionDate: timestamp (nullable = true)\n",
      " |-- BatchCloseDate: timestamp (nullable = true)\n",
      " |-- Active: string (nullable = true)\n",
      " |-- Purred: string (nullable = true)\n",
      " |-- PurchaseBranchCode: string (nullable = true)\n",
      " |-- PurchaseTransactionNo: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trans_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T10:46:08.132615Z",
     "start_time": "2020-09-15T10:46:06.932223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----+\n",
      "|       Folio|BrokerARN|count|\n",
      "+------------+---------+-----+\n",
      "|1.23456781E8| ARN-2680|    3|\n",
      "|1.23456783E8| ARN-2682|    2|\n",
      "|1.23456793E8| ARN-2680|    2|\n",
      "+------------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(folio, broker_column).groupby(folio,broker_column).count().where(col('count') > 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T10:46:09.711774Z",
     "start_time": "2020-09-15T10:46:08.139106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------+---------+------------+--------+-------------+---------------+----------+------------+----------------+--------+--------+-------------------+----------------+-------------------+-------------------+--------------+-------------------+------+------+-------+----+----------------+--------------+----------------+\n",
      "|       Folio|SchemeCode|PlanCode|BrokerARN|SubBrokerARN|    IHNo|TransactionNo|TransactionType|BranchCode|S_BranchCode|Proxy_BranchCode|DB_Units|Cr_Units|            NavDate|             NAV|    TransactionDate|        ProcessDate|BatchCloseDate|    EndorsementDate|Active|Purred|TERFlag| F23|batch_close_date|purchase_units|redemption_units|\n",
      "+------------+----------+--------+---------+------------+--------+-------------+---------------+----------+------------+----------------+--------+--------+-------------------+----------------+-------------------+-------------------+--------------+-------------------+------+------+-------+----+----------------+--------------+----------------+\n",
      "|1.23456787E8|        C5|      DQ| ARN-2686|        null|124575.0|      25682.0|            ADD|      GO22|        NULL|            NULL|     2.1|     2.1|2020-04-06 00:00:00|4.49999999999999|2020-04-06 00:00:00|2020-04-06 00:00:00|    2020-04-07|2020-04-06 00:00:00|     Y|     P|   NULL|null|      2020-04-07|           2.1|             0.0|\n",
      "|1.23456789E8|        AF|      DA| ARN-2679|        null|124568.0|      25675.0|            ADD|      GO22|        NULL|            NULL|   45.77|   45.77|2020-04-03 00:00:00|            10.1|2020-04-03 00:00:00|2020-04-03 00:00:00|    2020-04-04|2020-04-03 00:00:00|     Y|     P|   NULL|null|      2020-04-04|         45.77|             0.0|\n",
      "|1.23456792E8|        TF|      DQ| ARN-2679|        null|124579.0|      25686.0|            SIN|      GO22|        NULL|            NULL|   56.32|   56.32|2020-04-07 00:00:00|1.29999999999999|2020-04-07 00:00:00|2020-04-07 00:00:00|    2020-04-08|2020-04-07 00:00:00|     Y|     P|   NULL|null|      2020-04-08|         56.32|             0.0|\n",
      "|1.23456794E8|        TF|      DA| ARN-2681|        null|124581.0|      25688.0|            ADD|      GO22|        NULL|            NULL|  5653.2|  5653.2|2020-04-07 00:00:00|             3.5|2020-04-07 00:00:00|2020-04-07 00:00:00|    2020-04-08|2020-04-07 00:00:00|     Y|     P|   NULL|null|      2020-04-08|        5653.2|             0.0|\n",
      "|1.23456796E8|        SC|      DG| ARN-2683|        null|124583.0|      25690.0|            SIN|      GO22|        NULL|            NULL|   652.3|   652.3|2020-04-08 00:00:00|             2.7|2020-04-08 00:00:00|2020-04-08 00:00:00|    2020-04-09|2020-04-08 00:00:00|     Y|     P|   NULL|null|      2020-04-09|         652.3|             0.0|\n",
      "|1.23456797E8|        S6|      IG| ARN-2684|        null|124584.0|      25691.0|            SIN|      GO22|        NULL|            NULL|    76.4|    76.4|2020-04-08 00:00:00|             2.8|2020-04-08 00:00:00|2020-04-08 00:00:00|    2020-04-09|2020-04-08 00:00:00|     Y|     P|   NULL|null|      2020-04-09|          76.4|             0.0|\n",
      "|1.23456782E8|        AF|      DB| ARN-2681|        null|124570.0|      25677.0|            ADD|      GO22|        NULL|            NULL|   10.36|   10.36|2020-04-05 00:00:00|             8.5|2020-04-05 00:00:00|2020-04-05 00:00:00|    2020-04-06|2020-04-05 00:00:00|     Y|     P|   NULL|null|      2020-04-06|         10.36|             0.0|\n",
      "|1.23456784E8|        C2|      RD| ARN-2683|        null|124572.0|      25679.0|            SIN|      GO22|        NULL|            NULL|    67.9|    67.9|2020-04-05 00:00:00|             6.9|2020-04-05 00:00:00|2020-04-05 00:00:00|    2020-04-06|2020-04-05 00:00:00|     Y|     P|   NULL|null|      2020-04-06|          67.9|             0.0|\n",
      "|1.23456785E8|        C3|      DW| ARN-2684|        null|124573.0|      25680.0|            SIN|      GO22|        NULL|            NULL|    54.8|    54.8|2020-04-05 00:00:00|6.09999999999999|2020-04-05 00:00:00|2020-04-05 00:00:00|    2020-04-06|2020-04-05 00:00:00|     Y|     P|   NULL|null|      2020-04-06|          54.8|             0.0|\n",
      "|1.23456786E8|        C4|      IG| ARN-2685|        null|124574.0|      25681.0|            SIN|      GO22|        NULL|            NULL|     5.3|     5.3|2020-04-06 00:00:00|5.29999999999999|2020-04-06 00:00:00|2020-04-06 00:00:00|    2020-04-07|2020-04-06 00:00:00|     Y|     P|   NULL|null|      2020-04-07|           5.3|             0.0|\n",
      "|1.23456795E8|        TA|      DW| ARN-2682|        null|124582.0|      25689.0|            NEW|      GO22|        NULL|            NULL|    67.3|    67.3|2020-04-08 00:00:00|             2.4|2020-04-08 00:00:00|2020-04-08 00:00:00|    2020-04-09|2020-04-08 00:00:00|     Y|     P|   NULL|null|      2020-04-09|          67.3|             0.0|\n",
      "|1.23456799E8|        S8|      DM| ARN-2686|        null|124586.0|      25693.0|            ADD|      GO22|        NULL|            NULL|    34.6|    34.6|2020-04-08 00:00:00|             2.9|2020-04-08 00:00:00|2020-04-08 00:00:00|    2020-04-09|2020-04-08 00:00:00|     Y|     P|   NULL|null|      2020-04-09|          34.6|             0.0|\n",
      "|1.29736746E8|        AF|      DM| ARN-2678|        null|124567.0|      25674.0|            NEW|      GO22|        NULL|            NULL|   3.567|   3.567|2020-04-02 00:00:00|            10.9|2020-04-02 00:00:00|2020-04-02 00:00:00|    2020-04-03|2020-04-02 00:00:00|     Y|     P|   NULL|null|      2020-04-03|         3.567|             0.0|\n",
      "+------------+----------+--------+---------+------------+--------+-------------+---------------+----------+------------+----------------+--------+--------+-------------------+----------------+-------------------+-------------------+--------------+-------------------+------+------+-------+----+----------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "broker_column = 'BrokerARN'\n",
    "multi_broker_folio = data.select(folio, broker_column).groupby(folio,broker_column).count().where(col('count') > 1)\n",
    "data.join(multi_broker_folio.select('Folio'), on=[folio], how='left_anti').show()\n",
    "\n",
    "# data.join(data.select(folio, broker_column).groupby(folio,broker_column).count().where(col('count') > 1).select('Folio'), on=[folio], how='leftsemi').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T10:46:09.745619Z",
     "start_time": "2020-09-15T10:46:09.713602Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-03d66a37724b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbroker_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'BrokerARN'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroker_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbroker_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'count'\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "broker_column = 'BrokerARN'\n",
    "data.select(folio, broker_column).groupby(folio,broker_column).count().where('count' > 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dialyjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T00:15:45.127690Z",
     "start_time": "2020-09-16T00:15:45.091603Z"
    },
    "code_folding": [
     0,
     110,
     117,
     120,
     123,
     126,
     129
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 2 µs, total: 7 µs\n",
      "Wall time: 9.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def dialy_job(date_str, table='trans116', database='funds', date_column='BatchCloseDate', tenant_id='karvy',\n",
    "              transaction_status='Active', purred = 'Purred', transaction_type = 'TransactionType',\n",
    "              folio = 'Folio', purchase_units = 'DB_Units', redemption_units = 'Cr_Units',scheme = 'SchemeCode',\n",
    "               plan = 'PlanCode',groupby_level='SP', direct_db=None, fn_fromdt = 'fn_fromdt',fn_fromdt_format = 'dd/MM/yyyy',\n",
    "              fn_scheme = 'fn_scheme',fn_plan = 'fn_plan', fn_nav = 'fn_nav', nav_table='nav_master', \n",
    "              scheme_table='scheme_master',scheme_code = 'scheme_code', \n",
    "              plan_code = 'plan_code', category = 'SebiSchemeCategory',\n",
    "              subcategory = 'SebiSchemeSubCategory',nature = 'nature', newmcrid='NewMCRId'\n",
    "             ):\n",
    "    \"\"\"Dialy run this and store the latest data and aum data too\"\"\"\n",
    "    \n",
    "    # inflow outflow\n",
    "    inflow_db_trtypes = ['NEW', 'ADD', 'IPO', 'SIN', 'LTIN', 'LTIA']\n",
    "    inflow_cr_trtypes = ['NEWR', 'ADDR', 'IPOR', 'SINR', 'LTINR', 'LTIAR']\n",
    "    \n",
    "    outflow_db_trtypes = ['FULR', 'REDR', 'LTOFR', 'LTIAR']\n",
    "    outflow_cr_trtypes = ['FUL', 'RED', 'LTOF', 'LTOP']\n",
    "    \n",
    "    \n",
    "    \n",
    "    # configurations we use\n",
    "    batch_close_date = 'batch_close_date'\n",
    "    db_units = 'purchase_units'\n",
    "    cr_units = 'redemption_units'\n",
    "    balance_units = 'balance_units'\n",
    "    day_purchase_units = 'day_pu'\n",
    "    day_redemption_units = 'day_ru'\n",
    "    balance_purchase_units = 'balance_pu'\n",
    "    balance_redemption_units = 'balance_ru'\n",
    "    calculated_date = 'calculated_date'\n",
    "    today_pu = 'today_pu'\n",
    "    today_ru = 'today_ru'\n",
    "    effective_nav = 'effective_nav'\n",
    "    aum = 'aum'\n",
    "    aaum = 'aaum'\n",
    "    inflow = 'inflow'\n",
    "    outflow = 'outflow'\n",
    "    inflow_db_units = 'inflow_purchase_units'\n",
    "    inflow_cr_units = 'inflow_redemption_units'\n",
    "    outflow_db_units = 'outflow_purchase_units'\n",
    "    outflow_cr_units = 'outflow_redemption_units'\n",
    "    inflow_units = 'inflow_units'\n",
    "    outflow_units = 'outflow_units'\n",
    "    \n",
    "    # get the latest data from the previously stored file\n",
    "    date_obj = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    day_num = date_obj.day\n",
    "    previous_day = date_obj - datetime.timedelta(1)\n",
    "    previous_day_str = previous_day.strftime('%Y-%m-%d')\n",
    "    latest_data = spark.read.parquet(f\"{table}_latest/data_{groupby_level}_{previous_day_str}.parquet\")\n",
    "\n",
    "    # debug_df(latest_data)\n",
    "    \n",
    "    # get  the todays data\n",
    "    database = direct_db or (f'{tenant_id or default_tenant_id}_{database}')\n",
    "    query = f\"SELECT * from {table} where CAST({date_column} AS DATE)='{date_str}'\"\n",
    "    data  = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "            .option(\"query\", query) \\\n",
    "            .option(\"user\", user) \\\n",
    "            .option(\"password\", password) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .load()\n",
    "    data = data.cache()\n",
    "    day_count = data.count()\n",
    "#     data = data.filter((col(scheme)=='IC'))\n",
    "    # latest_data = latest_data.filter((col(scheme) == 'OV') & (col(plan) == 'RG'))\n",
    "    # data = data.filter((col(scheme) == 'OV') & (col(plan) == 'RG'))\n",
    "    # print (data.count())\n",
    "    # debug_df(data)\n",
    "    \n",
    "    # calculate all the steps as in initialization\n",
    "    # some preprocessings in the data, additional trimmings etc\n",
    "    data = data.withColumn(transaction_status, upper(trim(col(transaction_status))))\n",
    "    data = data.withColumn(purred, upper(trim(col(purred))))\n",
    "    data = data.withColumn(folio, upper(trim(col(folio))))\n",
    "    data = data.withColumn(scheme, upper(trim(col(scheme))))\n",
    "    data = data.withColumn(plan, upper(trim(col(plan))))\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "#     data = data.filter((col(scheme).isin('IC','HC')))\n",
    "    \n",
    "\n",
    "    # cast the date column into dates, as we are concerned only with dates now\n",
    "    data = data.withColumn(date_column, col(date_column).cast('date'))\n",
    "    \n",
    "    # filter the date till the batch_close_date (inclusive)\n",
    "    data = data.filter(col(date_column) <= date_str)\n",
    "    \n",
    "        \n",
    "    # filter the data according to rules\n",
    "    data = data.filter((col(date_column).isNotNull()) )\n",
    "    # data = data.filter((col(date_column) != '') ) # this will not work for few types\n",
    "    data = data.filter( ~trim(col(date_column)).cast(\"string\").eqNullSafe(''))\n",
    "    \n",
    "    # do must be rules\n",
    "    data = data.filter( (trim(upper((col(transaction_status))))) == \"Y\")\n",
    "    data = data.filter( (trim(upper(col(purred))) == \"P\") | (trim(upper(col(purred))) == \"R\") )\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "    # our configurations\n",
    "    data = data.withColumn(batch_close_date, data[date_column])\n",
    "        \n",
    "    # bring in purchase and redemption units\n",
    "    data = data.withColumn(db_units, when((col(purred) == \"P\"), col(purchase_units)).otherwise(0))\n",
    "    data = data.withColumn(cr_units, when((col(purred) == \"R\"), col(redemption_units)).otherwise(0))\n",
    "    \n",
    "    group_by_cols = []\n",
    "    window_partition = []\n",
    "    if groupby_level == 'SP':\n",
    "        window_partition = [scheme, plan]\n",
    "        group_by_cols = [scheme, plan, batch_close_date]\n",
    "        # scheme_plan wise we might need to filter out some transaction types\n",
    "        ignored_tr_types = ['CNI', 'CNO', 'TRMI', 'TRMO', 'TRFI', 'TRFO', 'PLDO', 'UPLO',\n",
    "                            'DMT', 'RMT', 'CNIR', 'CNOR', 'TRMIR', 'TRMOR', 'TRFIR', 'TRFOR', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR']\n",
    "        data = data.filter( ~(col(transaction_type).isin(ignored_tr_types)) )\n",
    "    elif groupby_level == 'SPT':\n",
    "        window_partition = [scheme, plan, transaction_type]\n",
    "        group_by_cols  = [scheme, plan, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPF':\n",
    "        window_partition = [scheme, plan, folio]\n",
    "        group_by_cols  = [scheme, plan, folio, batch_close_date]\n",
    "    elif groupby_level == 'SPFB':\n",
    "            window_partition = [scheme, plan, folio, broker]\n",
    "            group_by_cols  = [scheme, plan, folio, broker, batch_close_date]\n",
    "    elif groupby_level == 'SPFT':\n",
    "        window_partition = [scheme, plan, folio, transaction_type]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPFTB':\n",
    "        window_partition = [scheme, plan, folio, transaction_type, broker]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, broker, batch_close_date]\n",
    "    \n",
    "    # roll up the data\n",
    "    rolledup_data = data.groupBy(group_by_cols)\n",
    "    rolledup_data = rolledup_data.agg({db_units:'sum', cr_units:'sum'})\n",
    "        \n",
    "    rolledup_data = rolledup_data.withColumnRenamed(f\"sum({db_units})\", day_purchase_units).withColumnRenamed(f\"sum({cr_units})\", day_redemption_units)\n",
    "    # inflow outflow units\n",
    "    \n",
    "    \n",
    "    latest_data = latest_data.drop(day_purchase_units, day_redemption_units, batch_close_date, balance_units)\n",
    "    latest_data = latest_data.withColumnRenamed(balance_purchase_units, day_purchase_units)\n",
    "    latest_data = latest_data.withColumnRenamed(balance_redemption_units, day_redemption_units)\n",
    "    latest_data = latest_data.withColumnRenamed(calculated_date, batch_close_date)\n",
    "    \n",
    "    # debug_df(rolledup_data)\n",
    "    # debug_df(latest_data)\n",
    "    combined_data = latest_data.union(rolledup_data.select(latest_data.columns))\n",
    "    combined_data = combined_data.cache()\n",
    "    combined_count = combined_data.count()\n",
    "    # debug_df(rolledup_data)\n",
    "    # debug_df(latest_data)\n",
    "    # debug_df(combined_data)\n",
    "    combined_data = combined_data.withColumn(balance_purchase_units, sum(col(day_purchase_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    combined_data = combined_data.withColumn(balance_redemption_units, sum(col(day_redemption_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    combined_data = combined_data.withColumn(balance_units, (col(balance_purchase_units) - col(balance_redemption_units)))\n",
    "    # debug_df(combined_data)\n",
    "    \n",
    "    \n",
    "    # store the latest day data again\n",
    "    # get the latest data\n",
    "    combined_data = combined_data.filter(col(batch_close_date) <= date_str)\n",
    "    w = Window.partitionBy(window_partition).orderBy(col(batch_close_date).desc())\n",
    "    combined_data = combined_data.withColumn(\"rrn\", row_number().over(w)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    \n",
    "    \n",
    "    # maintained the calculated date (the latest data upto the calculated date)\n",
    "    combined_data = combined_data.withColumn(calculated_date, lit(date_str).cast('date'))\n",
    "    \n",
    "    # debug_df(combined_data)\n",
    "    # store in parquet file for optimization of space and only one file and multi partitions\n",
    "    # but write now store in csv and maintain date wise and colesce one\n",
    "    combined_data = combined_data.cache()\n",
    "#     combined_data.coalesce(1).write.csv(f\"{table}_latest/data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "    combined_data.write.parquet(f\"{table}_latest/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    \n",
    "    # join the nav, scheme_master data\n",
    "    nav_data = read_df(nav_table, '*', database)\n",
    "    nav_data = nav_data.withColumnRenamed(fn_scheme, scheme)\n",
    "    nav_data = nav_data.withColumnRenamed(fn_plan, plan)\n",
    "    nav_data = nav_data.withColumn(fn_fromdt, col(fn_fromdt).cast('date'))\n",
    "\n",
    "    scheme_master = read_df(scheme_table, [scheme_code, plan_code, nature, category, subcategory, newmcrid], database)\n",
    "    scheme_master = scheme_master.withColumnRenamed(scheme_code, scheme)\n",
    "    scheme_master = scheme_master.withColumnRenamed(plan_code, plan)\n",
    "\n",
    "    scheme_master_ = scheme_master.dropDuplicates([scheme, plan])\n",
    "    nav_scheme = nav_data.join(scheme_master_, on=[scheme, plan], how='left')\n",
    "    nav_data = nav_scheme\n",
    "    \n",
    "    # debug_df(nav_data)\n",
    "    \n",
    "    # calculate the aum \n",
    "    combined_data = combined_data.withColumn(effective_nav, date_sub(col(calculated_date), 1))\n",
    "    combined_data = combined_data.withColumn(today_pu, when((col(calculated_date) == col(batch_close_date)), col(day_purchase_units)).otherwise(0))\n",
    "    combined_data = combined_data.withColumn(today_ru, when((col(calculated_date) == col(batch_close_date)), col(day_redemption_units)).otherwise(0))\n",
    "    \n",
    "    # inflow outflow addition\n",
    "    inflow_db_condition = col(transaction_type).isin(inflow_db_trtypes)\n",
    "    inflow_cr_condition = col(transaction_type).isin(inflow_cr_trtypes)\n",
    "    combined_data = combined_data.withColumn(inflow_db_units, when(inflow_db_condition, col(today_pu)).otherwise(0))\n",
    "    combined_data = combined_data.withColumn(inflow_cr_units, when(inflow_cr_condition, col(today_ru)).otherwise(0))\n",
    "    \n",
    "    outflow_db_condition = col(transaction_type).isin(outflow_db_trtypes)\n",
    "    outflow_cr_condition = col(transaction_type).isin(outflow_cr_trtypes)\n",
    "    combined_data = combined_data.withColumn(outflow_db_units, when(outflow_db_condition, col(today_pu)).otherwise(0))\n",
    "    combined_data = combined_data.withColumn(outflow_cr_units, when(outflow_cr_condition, col(today_ru)).otherwise(0))\n",
    "    \n",
    "    combined_data = combined_data.withColumn(inflow_units, col(inflow_db_units) - col(inflow_cr_units))\n",
    "    combined_data = combined_data.withColumn(outflow_units, col(outflow_cr_units) - col(outflow_db_units))\n",
    "    \n",
    "    # debug_df(combined_data)\n",
    "    \n",
    "    nav_filteredFT = nav_data.filter(col(fn_fromdt) < date_str)\n",
    "    navw = Window.partitionBy([scheme, plan]).orderBy(col(fn_fromdt).desc())\n",
    "    nav_populate = nav_filteredFT.withColumn(\"rrn\", row_number().over(navw)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    nav_populate = nav_populate.withColumn(calculated_date, lit(date_str))\n",
    "    nav_populate = nav_populate.select([scheme, plan, fn_fromdt, fn_nav, calculated_date, category, subcategory, nature, newmcrid])\n",
    "    \n",
    "    # debug_df(nav_populate)\n",
    "    \n",
    "    joined = combined_data.join(nav_populate, on=[scheme, plan, calculated_date], how='left')\n",
    "    joined = joined.withColumn(aum, col(fn_nav) * col(balance_units))\n",
    "    joined = joined.withColumn(inflow, col(fn_nav) * col(inflow_units))\n",
    "    joined = joined.withColumn(outflow, col(fn_nav) * col(outflow_units))\n",
    "    \n",
    "#     aum_dummy = f'{aum}_d'\n",
    "#     final_joined = joined.withColumn(aum_dummy, col(aum))\n",
    "#     final_joined = final_joined.fillna({aum_dummy: 0})\n",
    "    \n",
    "#     # moving average logic\n",
    "#     print (day_num)\n",
    "#     if day_num == 1:\n",
    "#         final_joined = final_joined.withColumn(f'pre_{aum_dummy}', lit(0))\n",
    "#         final_joined = final_joined.withColumn(aaum, col(aum_dummy))\n",
    "#     elif day_num == 2:\n",
    "#         final_joined = final_joined.withColumn(f'pre_{aum_dummy}', lit(0))\n",
    "#         final_joined = final_joined.withColumn(aaum, col(aum_dummy))\n",
    "#     else:\n",
    "#         previous_day_aum = spark.read.parquet(f\"{table}_dialy/aaum_data_{groupby_level}_{previous_day_str}.parquet\")\n",
    "#         previous_day_aum.withColumnRenamed(aum_dummy, f'pre_{aum_dummy}')\n",
    "#         final_joined = final_joined.join(previous_day_aum.select(window_partition + [f'pre_{aum_dummy}']), on=window_partition, how='left')\n",
    "#         final_joined = final_joined.fillna({f'pre_{aum_dummy}': 0})\n",
    "#         final_joined = final_joined.withColumn(aaum, (col(aum_dummy) + col(f'pre_{aum_dummy}') / day_num))\n",
    "        \n",
    "    \n",
    "    # debug_df(joined)\n",
    "#     joined = joined.cache()\n",
    "    # store the data in the files\n",
    "#     joined.coalesce(1).write.csv(f\"{table}_dialy/data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "    joined.write.parquet(f\"{table}_dialy/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "#     final_joined = final_joined.cache()\n",
    "#     final_joined.coalesce(1).write.csv(f\"{table}_dialy/aaum_data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "#     final_joined.coalesce(1).write.parquet(f\"{table}_dialy/aaum_data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    \n",
    "    # upload the data if needed\n",
    "    # Done\n",
    "    print (f'dialy file on date {date_str} generated')\n",
    "    return day_count, combined_count\n",
    "\n",
    "# dialy_job('2020-05-01', groupby_level='SPFT', table='Trans_116', direct_db='BankRecon', nav_table='fund_navreg_AXA_29072020',\n",
    "#         scheme_table='Fund_Master_AXA_29072020', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory'\n",
    "#         )\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='Trans_128', direct_db='BankRecon', nav_table='fund_navreg_axismf',\n",
    "#         scheme_table='fund_master_axismf', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory'\n",
    "#         )\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='Trans_120', direct_db='BankRecon', nav_table='fund_navreg_invesco',\n",
    "#         scheme_table='fund_master_INVESCO', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory'\n",
    "#         )\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='m_Trans_116', direct_db='BankRecon', nav_table='fund_navreg_AXA_29072020',\n",
    "#         scheme_table='Fund_Master_AXA_29072020', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#         )\n",
    "\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='m_Trans_117', direct_db='BankRecon', nav_table='fund_navreg_MIRAE',\n",
    "#         scheme_table='fund_master_MIRAE', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#         )\n",
    "# print ('done')\n",
    "# import time\n",
    "# for i,ele in enumerate(list(daterange('2020-05-01', '2020-06-02'))):\n",
    "#     s = time.time()\n",
    "#     dialy_job(ele, groupby_level='SPFT', table='m_Trans_117', direct_db='BankRecon', nav_table='fund_navreg_MIRAE',\n",
    "#         scheme_table='fund_master_MIRAE', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#         )\n",
    "#     generate_mcr_report(table=f'm_Trans_{code}', groupby_level='SP', start_date = '2020-05-02', end_date = '2020-06-02')\n",
    "\n",
    "#     print (i, ele, time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T22:38:52.266298Z",
     "start_time": "2020-09-15T22:38:52.225021Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 µs, sys: 0 ns, total: 13 µs\n",
      "Wall time: 17.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def dialy_job(date_str, table='trans116', database='funds', date_column='BatchCloseDate', tenant_id='karvy',\n",
    "              transaction_status='Active', purred = 'Purred', transaction_type = 'TransactionType',\n",
    "              folio = 'Folio', purchase_units = 'DB_Units', redemption_units = 'Cr_Units',scheme = 'SchemeCode',\n",
    "               plan = 'PlanCode',groupby_level='SP', direct_db=None, fn_fromdt = 'fn_fromdt',fn_fromdt_format = 'dd/MM/yyyy',\n",
    "              fn_scheme = 'fn_scheme',fn_plan = 'fn_plan', fn_nav = 'fn_nav', nav_table='nav_master', \n",
    "              scheme_table='scheme_master',scheme_code = 'scheme_code', \n",
    "              plan_code = 'plan_code', category = 'SebiSchemeCategory',\n",
    "              subcategory = 'SebiSchemeSubCategory',nature = 'nature', newmcrid='NewMCRId'\n",
    "             ):\n",
    "    \"\"\"Dialy run this and store the latest data and aum data too\"\"\"\n",
    "    \n",
    "    # inflow outflow\n",
    "    inflow_db_trtypes = ['NEW', 'ADD', 'IPO', 'SIN', 'LTIN', 'LTIA']\n",
    "    inflow_cr_trtypes = ['NEWR', 'ADDR', 'IPOR', 'SINR', 'LTINR', 'LTIAR']\n",
    "    \n",
    "    outflow_db_trtypes = ['FULR', 'REDR', 'LTOFR', 'LTIAR']\n",
    "    outflow_cr_trtypes = ['FUL', 'RED', 'LTOF', 'LTOP']\n",
    "    \n",
    "    \n",
    "    \n",
    "    # configurations we use\n",
    "    batch_close_date = 'batch_close_date'\n",
    "    db_units = 'purchase_units'\n",
    "    cr_units = 'redemption_units'\n",
    "    balance_units = 'balance_units'\n",
    "    day_purchase_units = 'day_pu'\n",
    "    day_redemption_units = 'day_ru'\n",
    "    balance_purchase_units = 'balance_pu'\n",
    "    balance_redemption_units = 'balance_ru'\n",
    "    calculated_date = 'calculated_date'\n",
    "    today_pu = 'today_pu'\n",
    "    today_ru = 'today_ru'\n",
    "    effective_nav = 'effective_nav'\n",
    "    aum = 'aum'\n",
    "    aaum = 'aaum'\n",
    "    inflow = 'inflow'\n",
    "    outflow = 'outflow'\n",
    "    inflow_db_units = 'inflow_purchase_units'\n",
    "    inflow_cr_units = 'inflow_redemption_units'\n",
    "    outflow_db_units = 'outflow_purchase_units'\n",
    "    outflow_cr_units = 'outflow_redemption_units'\n",
    "    inflow_units = 'inflow_units'\n",
    "    outflow_units = 'outflow_units'\n",
    "    \n",
    "    # get the latest data from the previously stored file\n",
    "    date_obj = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    day_num = date_obj.day\n",
    "    previous_day = date_obj - datetime.timedelta(1)\n",
    "    previous_day_str = previous_day.strftime('%Y-%m-%d')\n",
    "    latest_data = spark.read.parquet(f\"{table}_latest/data_{groupby_level}_{previous_day_str}.parquet\")\n",
    "\n",
    "    # debug_df(latest_data)\n",
    "    \n",
    "    # get  the todays data\n",
    "    database = direct_db or (f'{tenant_id or default_tenant_id}_{database}')\n",
    "    query = f\"SELECT * from {table} where CAST({date_column} AS DATE)='{date_str}'\"\n",
    "    data  = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "            .option(\"query\", query) \\\n",
    "            .option(\"user\", user) \\\n",
    "            .option(\"password\", password) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .load()\n",
    "    data = data.cache()\n",
    "    day_count = data.count()\n",
    "#     data = data.filter((col(scheme)=='IC'))\n",
    "    # latest_data = latest_data.filter((col(scheme) == 'OV') & (col(plan) == 'RG'))\n",
    "    # data = data.filter((col(scheme) == 'OV') & (col(plan) == 'RG'))\n",
    "    # print (data.count())\n",
    "    # debug_df(data)\n",
    "    \n",
    "    # calculate all the steps as in initialization\n",
    "    # some preprocessings in the data, additional trimmings etc\n",
    "    data = data.withColumn(transaction_status, upper(trim(col(transaction_status))))\n",
    "    data = data.withColumn(purred, upper(trim(col(purred))))\n",
    "    data = data.withColumn(folio, upper(trim(col(folio))))\n",
    "    data = data.withColumn(scheme, upper(trim(col(scheme))))\n",
    "    data = data.withColumn(plan, upper(trim(col(plan))))\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "#     data = data.filter((col(scheme).isin('IC','HC')))\n",
    "    \n",
    "\n",
    "    # cast the date column into dates, as we are concerned only with dates now\n",
    "    data = data.withColumn(date_column, col(date_column).cast('date'))\n",
    "    \n",
    "    # filter the date till the batch_close_date (inclusive)\n",
    "    data = data.filter(col(date_column) <= date_str)\n",
    "    \n",
    "        \n",
    "    # filter the data according to rules\n",
    "    data = data.filter((col(date_column).isNotNull()) )\n",
    "    # data = data.filter((col(date_column) != '') ) # this will not work for few types\n",
    "    data = data.filter( ~trim(col(date_column)).cast(\"string\").eqNullSafe(''))\n",
    "    \n",
    "    # do must be rules\n",
    "    data = data.filter( (trim(upper((col(transaction_status))))) == \"Y\")\n",
    "    data = data.filter( (trim(upper(col(purred))) == \"P\") | (trim(upper(col(purred))) == \"R\") )\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "    # our configurations\n",
    "    data = data.withColumn(batch_close_date, data[date_column])\n",
    "        \n",
    "    # bring in purchase and redemption units\n",
    "    data = data.withColumn(db_units, when((col(purred) == \"P\"), col(purchase_units)).otherwise(0))\n",
    "    data = data.withColumn(cr_units, when((col(purred) == \"R\"), col(redemption_units)).otherwise(0))\n",
    "    \n",
    "    group_by_cols = []\n",
    "    window_partition = []\n",
    "    if groupby_level == 'SP':\n",
    "        window_partition = [scheme, plan]\n",
    "        group_by_cols = [scheme, plan, batch_close_date]\n",
    "        # scheme_plan wise we might need to filter out some transaction types\n",
    "        ignored_tr_types = ['CNI', 'CNO', 'TRMI', 'TRMO', 'TRFI', 'TRFO', 'PLDO', 'UPLO',\n",
    "                            'DMT', 'RMT', 'CNIR', 'CNOR', 'TRMIR', 'TRMOR', 'TRFIR', 'TRFOR', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR']\n",
    "        data = data.filter( ~(col(transaction_type).isin(ignored_tr_types)) )\n",
    "    elif groupby_level == 'SPT':\n",
    "        window_partition = [scheme, plan, transaction_type]\n",
    "        group_by_cols  = [scheme, plan, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPF':\n",
    "        window_partition = [scheme, plan, folio]\n",
    "        group_by_cols  = [scheme, plan, folio, batch_close_date]\n",
    "    elif groupby_level == 'SPFB':\n",
    "            window_partition = [scheme, plan, folio, broker]\n",
    "            group_by_cols  = [scheme, plan, folio, broker, batch_close_date]\n",
    "    elif groupby_level == 'SPFT':\n",
    "        window_partition = [scheme, plan, folio, transaction_type]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPFTB':\n",
    "        window_partition = [scheme, plan, folio, transaction_type, broker]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, broker, batch_close_date]\n",
    "    \n",
    "    # roll up the data\n",
    "    rolledup_data = data.groupBy(group_by_cols)\n",
    "    rolledup_data = rolledup_data.agg({db_units:'sum', cr_units:'sum'})\n",
    "        \n",
    "    rolledup_data = rolledup_data.withColumnRenamed(f\"sum({db_units})\", day_purchase_units).withColumnRenamed(f\"sum({cr_units})\", day_redemption_units)\n",
    "    # inflow outflow units\n",
    "    \n",
    "    \n",
    "#     latest_data.show()\n",
    "#     rolledup_data.show()\n",
    "    back_data = latest_data.join(broadcast(rolledup_data), on=group_by_cols, how='left_anti')\n",
    "#     latest_data.join(rolledup_data, on=group_by_cols, how='left_anti').show()\n",
    "#     latest_data.join(rolledup_data, on=group_by_cols, how='left_semi').show()\n",
    "    \n",
    "    \n",
    "    latest_data = latest_data.join(broadcast(rolledup_data), on=group_by_cols, how='left_semi')\n",
    "    latest_data = latest_data.drop(day_purchase_units, day_redemption_units, batch_close_date, balance_units)\n",
    "    latest_data = latest_data.withColumnRenamed(balance_purchase_units, day_purchase_units)\n",
    "    latest_data = latest_data.withColumnRenamed(balance_redemption_units, day_redemption_units)\n",
    "    latest_data = latest_data.withColumnRenamed(calculated_date, batch_close_date)\n",
    "    \n",
    "    # debug_df(rolledup_data)\n",
    "    # debug_df(latest_data)\n",
    "    combined_data = latest_data.union(rolledup_data.select(latest_data.columns))\n",
    "    \n",
    "    \n",
    "    combined_data = combined_data.cache()\n",
    "    combined_count = combined_data.count()\n",
    "    # debug_df(rolledup_data)\n",
    "    # debug_df(latest_data)\n",
    "    # debug_df(combined_data)\n",
    "    combined_data = combined_data.withColumn(balance_purchase_units, sum(col(day_purchase_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    combined_data = combined_data.withColumn(balance_redemption_units, sum(col(day_redemption_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    combined_data = combined_data.withColumn(balance_units, (col(balance_purchase_units) - col(balance_redemption_units)))\n",
    "    # debug_df(combined_data)\n",
    "    \n",
    "    back_data = back_data.withColumn(day_purchase_units, lit(0))\n",
    "    back_data = back_data.withColumn(day_redemption_units, lit(0))\n",
    "#     back_data = back_data.withColumn(calculated_date, batch_close_date)\n",
    "#     back_data.show()\n",
    "#     combined_data.show()\n",
    "    combined_data = combined_data.union(back_data.select(combined_data.columns))\n",
    "#     combined_data.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # store the latest day data again\n",
    "    # get the latest data\n",
    "    combined_data = combined_data.filter(col(batch_close_date) <= date_str)\n",
    "    w = Window.partitionBy(window_partition).orderBy(col(batch_close_date).desc())\n",
    "    combined_data = combined_data.withColumn(\"rrn\", row_number().over(w)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    \n",
    "    \n",
    "    # maintained the calculated date (the latest data upto the calculated date)\n",
    "    combined_data = combined_data.withColumn(calculated_date, lit(date_str).cast('date'))\n",
    "#     combined_data.show()\n",
    "    \n",
    "    # debug_df(combined_data)\n",
    "    # store in parquet file for optimization of space and only one file and multi partitions\n",
    "    # but write now store in csv and maintain date wise and colesce one\n",
    "    combined_data = combined_data.cache()\n",
    "#     combined_data.coalesce(1).write.csv(f\"{table}_latest/data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "    combined_data.write.parquet(f\"{table}_latest/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    \n",
    "    # join the nav, scheme_master data\n",
    "    nav_data = read_df(nav_table, '*', database)\n",
    "    nav_data = nav_data.withColumnRenamed(fn_scheme, scheme)\n",
    "    nav_data = nav_data.withColumnRenamed(fn_plan, plan)\n",
    "    nav_data = nav_data.withColumn(fn_fromdt, col(fn_fromdt).cast('date'))\n",
    "\n",
    "    scheme_master = read_df(scheme_table, [scheme_code, plan_code, nature, category, subcategory, newmcrid], database)\n",
    "    scheme_master = scheme_master.withColumnRenamed(scheme_code, scheme)\n",
    "    scheme_master = scheme_master.withColumnRenamed(plan_code, plan)\n",
    "\n",
    "    scheme_master_ = scheme_master.dropDuplicates([scheme, plan])\n",
    "    nav_scheme = nav_data.join(scheme_master_, on=[scheme, plan], how='left')\n",
    "    nav_data = nav_scheme\n",
    "    \n",
    "    # debug_df(nav_data)\n",
    "    \n",
    "    # calculate the aum \n",
    "    combined_data = combined_data.withColumn(effective_nav, date_sub(col(calculated_date), 1))\n",
    "    combined_data = combined_data.withColumn(today_pu, when((col(calculated_date) == col(batch_close_date)), col(day_purchase_units)).otherwise(0))\n",
    "    combined_data = combined_data.withColumn(today_ru, when((col(calculated_date) == col(batch_close_date)), col(day_redemption_units)).otherwise(0))\n",
    "    \n",
    "    # inflow outflow addition\n",
    "    inflow_db_condition = col(transaction_type).isin(inflow_db_trtypes)\n",
    "    inflow_cr_condition = col(transaction_type).isin(inflow_cr_trtypes)\n",
    "    combined_data = combined_data.withColumn(inflow_db_units, when(inflow_db_condition, col(today_pu)).otherwise(0))\n",
    "    combined_data = combined_data.withColumn(inflow_cr_units, when(inflow_cr_condition, col(today_ru)).otherwise(0))\n",
    "    \n",
    "    outflow_db_condition = col(transaction_type).isin(outflow_db_trtypes)\n",
    "    outflow_cr_condition = col(transaction_type).isin(outflow_cr_trtypes)\n",
    "    combined_data = combined_data.withColumn(outflow_db_units, when(outflow_db_condition, col(today_pu)).otherwise(0))\n",
    "    combined_data = combined_data.withColumn(outflow_cr_units, when(outflow_cr_condition, col(today_ru)).otherwise(0))\n",
    "    \n",
    "    combined_data = combined_data.withColumn(inflow_units, col(inflow_db_units) - col(inflow_cr_units))\n",
    "    combined_data = combined_data.withColumn(outflow_units, col(outflow_cr_units) - col(outflow_db_units))\n",
    "    \n",
    "    # debug_df(combined_data)\n",
    "    \n",
    "    nav_filteredFT = nav_data.filter(col(fn_fromdt) < date_str)\n",
    "    navw = Window.partitionBy([scheme, plan]).orderBy(col(fn_fromdt).desc())\n",
    "    nav_populate = nav_filteredFT.withColumn(\"rrn\", row_number().over(navw)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    nav_populate = nav_populate.withColumn(calculated_date, lit(date_str))\n",
    "    nav_populate = nav_populate.select([scheme, plan, fn_fromdt, fn_nav, calculated_date, category, subcategory, nature, newmcrid])\n",
    "    \n",
    "    # debug_df(nav_populate)\n",
    "    \n",
    "    joined = combined_data.join(nav_populate, on=[scheme, plan, calculated_date], how='left')\n",
    "    joined = joined.withColumn(aum, col(fn_nav) * col(balance_units))\n",
    "    joined = joined.withColumn(inflow, col(fn_nav) * col(inflow_units))\n",
    "    joined = joined.withColumn(outflow, col(fn_nav) * col(outflow_units))\n",
    "    \n",
    "#     aum_dummy = f'{aum}_d'\n",
    "#     final_joined = joined.withColumn(aum_dummy, col(aum))\n",
    "#     final_joined = final_joined.fillna({aum_dummy: 0})\n",
    "    \n",
    "#     # moving average logic\n",
    "#     print (day_num)\n",
    "#     if day_num == 1:\n",
    "#         final_joined = final_joined.withColumn(f'pre_{aum_dummy}', lit(0))\n",
    "#         final_joined = final_joined.withColumn(aaum, col(aum_dummy))\n",
    "#     elif day_num == 2:\n",
    "#         final_joined = final_joined.withColumn(f'pre_{aum_dummy}', lit(0))\n",
    "#         final_joined = final_joined.withColumn(aaum, col(aum_dummy))\n",
    "#     else:\n",
    "#         previous_day_aum = spark.read.parquet(f\"{table}_dialy/aaum_data_{groupby_level}_{previous_day_str}.parquet\")\n",
    "#         previous_day_aum.withColumnRenamed(aum_dummy, f'pre_{aum_dummy}')\n",
    "#         final_joined = final_joined.join(previous_day_aum.select(window_partition + [f'pre_{aum_dummy}']), on=window_partition, how='left')\n",
    "#         final_joined = final_joined.fillna({f'pre_{aum_dummy}': 0})\n",
    "#         final_joined = final_joined.withColumn(aaum, (col(aum_dummy) + col(f'pre_{aum_dummy}') / day_num))\n",
    "        \n",
    "    \n",
    "    # debug_df(joined)\n",
    "    joined = joined.cache()\n",
    "    # store the data in the files\n",
    "#     joined.coalesce(1).write.csv(f\"{table}_dialy/data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "    joined.write.parquet(f\"{table}_dialy/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "#     final_joined = final_joined.cache()\n",
    "#     final_joined.coalesce(1).write.csv(f\"{table}_dialy/aaum_data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "#     final_joined.coalesce(1).write.parquet(f\"{table}_dialy/aaum_data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    \n",
    "    # upload the data if needed\n",
    "    # Done\n",
    "    print (f'dialy file on date {date_str} generated')\n",
    "    return day_count, combined_count\n",
    "\n",
    "# dialy_job('2020-05-01', groupby_level='SPFT', table='Trans_116', direct_db='BankRecon', nav_table='fund_navreg_AXA_29072020',\n",
    "#         scheme_table='Fund_Master_AXA_29072020', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory'\n",
    "#         )\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='Trans_128', direct_db='BankRecon', nav_table='fund_navreg_axismf',\n",
    "#         scheme_table='fund_master_axismf', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory'\n",
    "#         )\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='Trans_120', direct_db='BankRecon', nav_table='fund_navreg_invesco',\n",
    "#         scheme_table='fund_master_INVESCO', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory'\n",
    "#         )\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='m_Trans_116', direct_db='BankRecon', nav_table='fund_navreg_AXA_29072020',\n",
    "#         scheme_table='Fund_Master_AXA_29072020', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#         )\n",
    "\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='m_Trans_117', direct_db='BankRecon', nav_table='fund_navreg_MIRAE',\n",
    "#         scheme_table='fund_master_MIRAE', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#         )\n",
    "# print ('done')\n",
    "# import time\n",
    "# for i,ele in enumerate(list(daterange('2020-05-01', '2020-06-02'))):\n",
    "#     s = time.time()\n",
    "#     dialy_job(ele, groupby_level='SPFT', table='m_Trans_117', direct_db='BankRecon', nav_table='fund_navreg_MIRAE',\n",
    "#         scheme_table='fund_master_MIRAE', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#         )\n",
    "#     generate_mcr_report(table=f'm_Trans_{code}', groupby_level='SP', start_date = '2020-05-02', end_date = '2020-06-02')\n",
    "\n",
    "#     print (i, ele, time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T07:09:15.258732Z",
     "start_time": "2020-09-16T07:09:13.707500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inital file on date 2020-03-31 written\n",
      "init time is 1.5460426807403564\n"
     ]
    }
   ],
   "source": [
    "init_date = '2020-03-31'\n",
    "groupby_level = 'SPFT'\n",
    "table = 'm_Trans_116'\n",
    "direct_db = 'kfintech_funds'\n",
    "nav_table = 'nav_master'\n",
    "scheme_table = 'scheme_master'\n",
    "s = time.time()\n",
    "initialize(init_date, table=table,direct_db=direct_db, groupby_level=groupby_level)\n",
    "print (f'init time is {time.time()-s}')\n",
    "# init_date = '2020-03-30'\n",
    "# initialize(init_date, table=table,direct_db=direct_db, groupby_level=groupby_level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T00:17:10.978437Z",
     "start_time": "2020-09-16T00:15:49.876116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inital file on date 2020-03-31 written\n",
      "init time is 12.266433715820312\n",
      "dialy file on date 2020-04-01 generated\n",
      "2020-04-01 time is 6.248687505722046\n",
      "dialy file on date 2020-04-02 generated\n",
      "2020-04-02 time is 3.534027338027954\n",
      "dialy file on date 2020-04-03 generated\n",
      "2020-04-03 time is 3.3817849159240723\n",
      "dialy file on date 2020-04-04 generated\n",
      "2020-04-04 time is 2.6859920024871826\n",
      "dialy file on date 2020-04-05 generated\n",
      "2020-04-05 time is 2.4296092987060547\n",
      "dialy file on date 2020-04-06 generated\n",
      "2020-04-06 time is 2.675478935241699\n",
      "dialy file on date 2020-04-07 generated\n",
      "2020-04-07 time is 2.2134010791778564\n",
      "dialy file on date 2020-04-08 generated\n",
      "2020-04-08 time is 2.2975497245788574\n",
      "dialy file on date 2020-04-09 generated\n",
      "2020-04-09 time is 2.5125327110290527\n",
      "dialy file on date 2020-04-10 generated\n",
      "2020-04-10 time is 2.250476837158203\n",
      "dialy file on date 2020-04-11 generated\n",
      "2020-04-11 time is 2.2093355655670166\n",
      "dialy file on date 2020-04-12 generated\n",
      "2020-04-12 time is 2.1668529510498047\n",
      "dialy file on date 2020-04-13 generated\n",
      "2020-04-13 time is 2.0496394634246826\n",
      "dialy file on date 2020-04-14 generated\n",
      "2020-04-14 time is 2.086313247680664\n",
      "dialy file on date 2020-04-15 generated\n",
      "2020-04-15 time is 2.2821550369262695\n",
      "dialy file on date 2020-04-16 generated\n",
      "2020-04-16 time is 2.1312034130096436\n",
      "dialy file on date 2020-04-17 generated\n",
      "2020-04-17 time is 2.27518367767334\n",
      "dialy file on date 2020-04-18 generated\n",
      "2020-04-18 time is 2.0347561836242676\n",
      "dialy file on date 2020-04-19 generated\n",
      "2020-04-19 time is 2.0315043926239014\n",
      "dialy file on date 2020-04-20 generated\n",
      "2020-04-20 time is 1.9802780151367188\n",
      "dialy file on date 2020-04-21 generated\n",
      "2020-04-21 time is 1.9025189876556396\n",
      "dialy file on date 2020-04-22 generated\n",
      "2020-04-22 time is 1.9359993934631348\n",
      "dialy file on date 2020-04-23 generated\n",
      "2020-04-23 time is 1.8653252124786377\n",
      "dialy file on date 2020-04-24 generated\n",
      "2020-04-24 time is 2.1732866764068604\n",
      "dialy file on date 2020-04-25 generated\n",
      "2020-04-25 time is 1.889486312866211\n",
      "dialy file on date 2020-04-26 generated\n",
      "2020-04-26 time is 1.9420137405395508\n",
      "dialy file on date 2020-04-27 generated\n",
      "2020-04-27 time is 1.8160760402679443\n",
      "dialy file on date 2020-04-28 generated\n",
      "2020-04-28 time is 1.8763444423675537\n",
      "dialy file on date 2020-04-29 generated\n",
      "2020-04-29 time is 1.9436242580413818\n",
      "total time 81.0918231010437\n"
     ]
    }
   ],
   "source": [
    "init_date = '2020-03-31'\n",
    "groupby_level = 'SPFT'\n",
    "table = 'trans116'\n",
    "direct_db = 'kfintech_funds'\n",
    "nav_table = 'nav_master'\n",
    "scheme_table = 'scheme_master'\n",
    "s = time.time()\n",
    "initialize(init_date, table=table,direct_db=direct_db, groupby_level=groupby_level)\n",
    "print (f'init time is {time.time()-s}')\n",
    "# init_date = '2020-03-30'\n",
    "# initialize(init_date, table=table,direct_db=direct_db, groupby_level=groupby_level)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for ele in daterange('2020-04-01', '2020-04-30'):\n",
    "    i = time.time()\n",
    "    dialy_job(ele, groupby_level=groupby_level, table=table, direct_db=direct_db, nav_table=nav_table,\n",
    "                scheme_table=scheme_table, scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "                 category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "                )\n",
    "    print (f'{ele} time is {time.time()-i}')\n",
    "    \n",
    "print (f'total time {time.time()-s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T10:43:35.421200Z",
     "start_time": "2020-08-26T10:42:50.068245Z"
    },
    "code_folding": [
     0,
     15
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+---------------+------------------+------------+------------------+------------------+------------+\n",
      "|fm_NewMCRId|count(SchemeCode)|count(PlanCode)|       sum(inflow)|sum(outflow)|          sum(aum)|           avg_aum|count(Folio)|\n",
      "+-----------+-----------------+---------------+------------------+------------+------------------+------------------+------------+\n",
      "|       BIii|                1|              1|  9.44999999999998|         0.0|  9.44999999999998| 9.449999999999983|           1|\n",
      "|     AIII28|                4|              4|            2960.7|     522.522|2438.1779999999994|2438.1779999999994|           4|\n",
      "|     AIII27|                4|              4|2236.9900000000002|         0.0|2236.9900000000002|           2236.99|           4|\n",
      "|     Others|                2|              5|              null|        null|               0.0|               0.0|        null|\n",
      "+-----------+-----------------+---------------+------------------+------------+------------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_data(start_date, end_date, groupby_level='SPT', table='m_Trans_116'):\n",
    "    \n",
    "    final_data = None\n",
    "\n",
    "    dates_list = list(daterange(start_date, end_date))\n",
    "    for date in dates_list:\n",
    "        date_obj = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "        day_num = date_obj.day\n",
    "        latest_data = spark.read.parquet(f\"{table}_dialy/data_{groupby_level}_{date}.parquet\")\n",
    "        if final_data:\n",
    "            final_data = final_data.union(latest_data)\n",
    "        else:\n",
    "            final_data = latest_data\n",
    "    return final_data\n",
    "\n",
    "def generate_mcr_report(table='m_Trans_116', ignored_tr_types = ['CNI', 'CNO', 'TRMI', 'TRMO', \n",
    "                                                                 'TRFI', 'TRFO', 'PLDO', 'UPLO', 'DMT',\n",
    "                                                                 'RMT', 'CNIR', 'CNOR', 'TRMIR', 'TRMOR',\n",
    "                                                                 'TRFIR', 'TRFOR', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR'],\n",
    "        liquid_fund_tr_types = ['NEW', 'ADD', 'IPO', 'SIN', 'NEWR', 'ADDR', 'IPOR', 'SINR'],\n",
    "                       start_date = '2020-05-02', end_date = '2020-06-02', groupby_level='SPT',\n",
    "                        transaction_type='TransactionType',folio='Folio',folio_ignore_types = ['PLDO', 'UPLO', 'DMT', 'RMT', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR'], \n",
    "                      fn_nav = 'fn_nav', newmcrid='fm_NewMCRId', today_pu = 'today_pu', today_ru = 'today_ru', scheme='SchemeCode', aum='aum', plan='PlanCode'):\n",
    "    \n",
    "    till_but_one_day_data = None\n",
    "    inflow = 'inflow'\n",
    "    outflow = 'outflow'\n",
    "    calculated_date = 'calculated_date'\n",
    "    batch_close_date = 'batch_close_date'\n",
    "    balance_pu = 'balance_pu'\n",
    "    balance_ru = 'balance_ru'\n",
    "    balance_units = 'balance_units'\n",
    "    \n",
    "    \n",
    "    dates_list = list(daterange(start_date, end_date))\n",
    "    \n",
    "    for date in dates_list[:-1]:\n",
    "        date_obj = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "        day_num = date_obj.day\n",
    "        latest_data = spark.read.parquet(f\"{table}_dialy/data_{groupby_level}_{date}.parquet\")\n",
    "#         print (ele, latest_data.count())\n",
    "        if till_but_one_day_data:\n",
    "            till_but_one_day_data = till_but_one_day_data.union(latest_data)\n",
    "        else:\n",
    "            till_but_one_day_data = latest_data\n",
    "            \n",
    "    \n",
    "    date_obj = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    final_day = date_obj + datetime.timedelta(1)\n",
    "    final_day_str = final_day.strftime('%Y-%m-%d')\n",
    "    final_day_data = spark.read.parquet(f\"{table}_dialy/data_{groupby_level}_{final_day_str}.parquet\")\n",
    "    last_but_one_day_data = latest_data\n",
    "\n",
    "    till_but_one_day_data = till_but_one_day_data.filter( ~(col(transaction_type).isin(ignored_tr_types)) ).fillna({today_pu: 0, today_ru: 0, aum: 0, newmcrid: 'Others'})\n",
    "    final_day_data = final_day_data.filter( ~(col(transaction_type).isin(ignored_tr_types)) ).fillna({today_pu: 0, today_ru: 0, aum: 0, newmcrid: 'Others'})\n",
    "    \n",
    "    \n",
    "    all_data = till_but_one_day_data.union(final_day_data)\n",
    "    sp_inf_ouf_data = all_data\n",
    "\n",
    "    liquid_condition = ( (col(newmcrid) == 'A1b') & (col(calculated_date) == final_day_str) & (col(batch_close_date) == final_day_str) & (col(transaction_type).isin(liquid_fund_tr_types)) )\n",
    "    \n",
    "    final_day_data = final_day_data.withColumn(balance_pu,    when(liquid_condition, col(balance_pu) - col(today_pu)).otherwise(col(balance_pu)))\n",
    "    final_day_data = final_day_data.withColumn(balance_ru,    when(liquid_condition, col(balance_ru) - col(today_ru)).otherwise(col(balance_ru)))\n",
    "    final_day_data = final_day_data.withColumn(balance_units, when(liquid_condition, col(balance_pu) - col(balance_ru)).otherwise(col(balance_units)) )\n",
    "    final_day_data = final_day_data.withColumn(aum,           when(liquid_condition,  col(balance_units) * col(fn_nav)).otherwise(col(aum))    )\n",
    "\n",
    "    net_aum = final_day_data.groupby([newmcrid]).agg(sum(aum))\n",
    "    avg_data = till_but_one_day_data.union(final_day_data)\n",
    "#     avg_data = all_data\n",
    "    \n",
    "    # inflow, outflow logic change\n",
    "    sp_data = avg_data\n",
    "    # sp_inf_ouf_data = sp_inf_ouf_data.withColumn(inflow, (col(today_pu)*col(fn_nav)))\n",
    "    # sp_inf_ouf_data = sp_inf_ouf_data.withColumn(outflow, (col(today_ru)*col(fn_nav)))\n",
    "    sp = sp_data.groupby([newmcrid]).agg(countDistinct(scheme),countDistinct(plan))\n",
    "    \n",
    "    inf_ouf_data = get_data(datetime.datetime(date_obj.year, 4, 2).strftime('%Y-%m-%d'), start_date, groupby_level, table).union(sp_data)\n",
    "    inf_ouf_data = inf_ouf_data.withColumn(inflow, (col(today_pu)*col(fn_nav)))\n",
    "    inf_ouf_data = inf_ouf_data.withColumn(outflow, (col(today_ru)*col(fn_nav)))\n",
    "    inf_ouf_data = inf_ouf_data.groupby([newmcrid]).agg(sum(col(inflow)),sum(col(outflow)))\n",
    "    \n",
    "    spinout = sp.join(inf_ouf_data, on=[newmcrid], how='left')\n",
    "#     spinout.show()\n",
    "    # sp_inf_ouf_data = sp_inf_ouf_data.withColumn(inflow, (col(today_pu)*col(fn_nav)))\n",
    "    # sp_inf_ouf_data = sp_inf_ouf_data.withColumn(outflow, (col(today_ru)*col(fn_nav)))\n",
    "\n",
    "    folio_count = avg_data.groupby(folio, scheme, plan, newmcrid).agg(sum('aum')).filter(col('sum(aum)') - 0 > 0.1).groupby(newmcrid).agg(countDistinct(folio))\n",
    "\n",
    "    avg_aum = avg_data.groupby([newmcrid]).agg(sum(aum))\n",
    "    avg_aum = avg_aum.withColumn('avg_aum', col(f'sum({aum})')/(len(list(daterange(start_date, end_date))))).drop(f'sum({aum})')\n",
    "    \n",
    "\n",
    "    mcr_net_aum = spinout.join(net_aum, on=[newmcrid], how='left')\n",
    "    mcr = mcr_net_aum.join(avg_aum, on=[newmcrid], how='left')\n",
    "    mcr = mcr.join(folio_count, on=[newmcrid], how='left')\n",
    "\n",
    "\n",
    "    mcr.show()\n",
    "    \n",
    "    mcr.coalesce(1).write.csv(f\"{table}_mcr/mcr_{groupby_level}_{final_day_str}.csv\",header=True, mode='overwrite')\n",
    "#     mcr.coalesce(1).write.parquet(f\"{table}_mcr/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    \n",
    "    return mcr,avg_data, sp_inf_ouf_data, all_data\n",
    " \n",
    "    \n",
    "table = 'trans116'\n",
    "groupby_level = 'SPFT'\n",
    "start_date = '2020-05-02'\n",
    "end_date = '2020-06-02'\n",
    "mcr, _,_,_ = generate_mcr_report(table=table, groupby_level=groupby_level, start_date=start_date, end_date=end_date)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:05:13.238626Z",
     "start_time": "2020-08-19T07:05:11.045764Z"
    },
    "code_folding": [
     9
    ]
   },
   "source": [
    "#### table_codes = {117: 'MIRAE'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPT'\n",
    "        init_date = '2020-06-30'\n",
    "        mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange('2020-07-01', '2020-08-02'))):\n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "#             save_metric(ele, 'day_records', day_records, name, groupby_level)\n",
    "#             save_metric(ele, 'combined_records', combined_records, name, groupby_level)\n",
    "#             save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:05:13.924841Z",
     "start_time": "2020-08-19T07:05:13.433907Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMF Reliance\n",
      "None\n",
      "An error occurred while calling o101.load.\n",
      ": com.microsoft.sqlserver.jdbc.SQLServerException: Cannot open database \"BankRecon\" requested by the login. The login failed. ClientConnectionId:21ea2dce-924a-45bd-ae0b-951a417920a0\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:262)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:258)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:104)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:5036)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:3668)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.access$000(SQLServerConnection.java:94)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:3627)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7194)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:2935)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2456)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2103)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:1950)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1162)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:735)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:339)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:203)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-8-651d4dd48d65>\", line 35, in <module>\n",
      "    records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
      "  File \"<timed exec>\", line 32, in initialize\n",
      "  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 184, in load\n",
      "    return self._df(self._jreader.load())\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 131, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o101.load.\n",
      ": com.microsoft.sqlserver.jdbc.SQLServerException: Cannot open database \"BankRecon\" requested by the login. The login failed. ClientConnectionId:21ea2dce-924a-45bd-ae0b-951a417920a0\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:262)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:258)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:104)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:5036)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:3668)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.access$000(SQLServerConnection.java:94)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:3627)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7194)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:2935)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2456)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2103)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:1950)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1162)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:735)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:339)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:203)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_codes = {'RMF': 'Reliance'}\n",
    "\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPT'\n",
    "        init_date = '2020-06-30'\n",
    "        mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange('2020-07-01', '2020-08-02'))):\n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "#             save_metric(ele, 'day_records', day_records, name, groupby_level)\n",
    "#             save_metric(ele, 'combined_records', combined_records, name, groupby_level)\n",
    "#             save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T09:10:22.530312Z",
     "start_time": "2020-08-19T09:10:22.524239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-04-02'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "# datetime.datetime.year('2020-08-04')\n",
    "\n",
    "date_obj = datetime.datetime.strptime('2020-08-05', '%Y-%m-%d')\n",
    "date_obj.year\n",
    "\n",
    "datetime.datetime(date_obj.year, 4, 2).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T17:20:38.744555Z",
     "start_time": "2020-08-19T17:20:38.732643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'116': 'AXA',\n",
       " '117': 'MIRAE',\n",
       " '107': 'BOB',\n",
       " '120': 'INVESCO',\n",
       " 'RMF': 'Reliance',\n",
       " '118': 'Edelweiss',\n",
       " '135': 'IDBIMF',\n",
       " '125': 'IBMF',\n",
       " '128': 'AXISMF',\n",
       " '178': 'BNPMF',\n",
       " '152': 'ITI',\n",
       " '105': 'JMMF',\n",
       " '103': 'PMF',\n",
       " '166': 'Quant',\n",
       " '130': 'PeerlessMF',\n",
       " '104': 'TAURUS',\n",
       " '108': 'UTI',\n",
       " '123': 'Quantum',\n",
       " '127': 'MOTILAL',\n",
       " '102': 'LIC',\n",
       " '176': 'SundaramMF',\n",
       " '101': 'canrobeco',\n",
       " '129': 'DLFPramerica'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"116\": \"AXA\",\n",
    "\"117\": \"MIRAE\",\n",
    "\"107\": \"BOB\",\n",
    "\"120\": \"INVESCO\",\n",
    "\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\n",
    "\"135\": \"IDBIMF\",\n",
    "\"125\": \"IBMF\",\n",
    "\"128\": \"AXISMF\",\n",
    "\"178\": \"BNPMF\",\n",
    "\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\n",
    "\"103\": \"PMF\",\n",
    "\"166\": \"Quant\",\n",
    "\"130\": \"PeerlessMF\",\n",
    "\"104\": \"TAURUS\",\n",
    "\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\n",
    "\"127\": \"MOTILAL\",\n",
    "\"102\": \"LIC\",\n",
    "\"176\": \"SundaramMF\",\n",
    "\"101\": \"canrobeco\",\n",
    "\"129\": \"DLFPramerica\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#### all the script exaaecution for all the funds\n",
    "table_codes = {\n",
    "               102:'LIC',103:'pmf',104:'tarus',105:'JMMF',107:'BOB',108:'uti',\n",
    "               116:'AXA',117:'mirae',118:'edelwwise',120:'invesco',123:'quantum'\n",
    "               ,125:'IBMF',127:'motilal',128:'axismf',130:'peerless',135:'IDBIMF',152:'ITI',166:'quant',\n",
    "               176:'sundaram',178:'BNPMF', 'RMF':'reliance'}\n",
    "table_codes = {104:'taurus',105:'JMMF',107:'BOB',108:'uti',\n",
    "               116:'AXA',117:'mirae',118:'edelwwise',120:'invesco',123:'quantum'\n",
    "               ,125:'IBMF',127:'motilal',128:'axismf',130:'peerless',135:'IDBIMF',152:'ITI',166:'quant',\n",
    "               176:'sundaram',178:'BNPMF', 'RMF':'reliance'}\n",
    "\n",
    "table_codes = {101:'canrobeco',\n",
    "               102:'LIC',103:'pmf',104:'tarus',105:'JMMF',107:'BOB',108:'uti',\n",
    "               116:'AXA',117:'mirae',118:'edelwwise',120:'invesco',123:'quantum'\n",
    "               ,125:'IBMF',127:'motilal',128:'axismf',129:'pgim',130:'peerless',135:'IDBIMF',152:'ITI',166:'quant'\n",
    "               ,178:'BNPMF', 'RMF':'reliance', '129': 'dlfpramerica', 118:'edelweiss', 130:'peerlessMF', '176':'sundaramMF'}\n",
    "table_codes = {116: 'AXA'}\n",
    "\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPFT'\n",
    "        init_date = '2020-04-30'\n",
    "        mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange('2020-05-01', '2020-06-02'))):\n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "            save_metric(ele, 'day_records', day_records, name, groupby_level)\n",
    "            save_metric(ele, 'combined_records', combined_records, name, groupby_level)\n",
    "            save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#### generate mcr for august for all the funds\n",
    "# table_codes = { '129': 'dlfpramerica', 118:'edelweiss', 130:'peerlessMF', '176':'sundaramMF'}\n",
    "\n",
    "table_codes = {101:'canrobeco',\n",
    "               102:'LIC',103:'pmf',104:'tarus',105:'JMMF',107:'BOB',108:'uti',\n",
    "               116:'AXA',117:'mirae',118:'edelwwise',120:'invesco',123:'quantum'\n",
    "               ,125:'IBMF',127:'motilal',128:'axismf',129:'pgim',130:'peerless',135:'IDBIMF',152:'ITI',166:'quant'\n",
    "               ,178:'BNPMF', 'RMF':'reliance', '129': 'dlfpramerica', 118:'edelweiss', 130:'peerlessMF', '176':'sundaramMF'}\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPT'\n",
    "        init_date = '2020-07-01'\n",
    "        mcr_month_date = '2020-07-01'\n",
    "\n",
    "#         records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level)\n",
    "\n",
    "#         print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "#         job_start = time.time()\n",
    "\n",
    "\n",
    "#         for i,ele in enumerate(list(daterange('2020-07-02', '2020-08-02'))):\n",
    "#             s = time.time()\n",
    "#             day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "#             scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#              category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#             )\n",
    "#             save_metric(ele, 'day_records', day_records, name, groupby_level)\n",
    "#             save_metric(ele, 'combined_records', combined_records, name, groupby_level)\n",
    "#             save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level)\n",
    "#             print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "#         s = time.time() \n",
    "        generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-07-02', end_date = '2020-08-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "#         print (f'job time is {time.time() - job_start}')\n",
    "#         print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
