{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:12.676366Z",
     "start_time": "2020-08-25T08:31:12.527984Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# db_utils\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import sqlalchemy\n",
    "import os\n",
    "import pyodbc\n",
    "import sys\n",
    "# import pymssql\n",
    "import numpy as np\n",
    "\n",
    "from MySQLdb._exceptions import OperationalError\n",
    "from sqlalchemy import create_engine, exc,event\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "from time import time\n",
    "\n",
    "#connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=192.168.15.126;UID=BRS;PWD=Fint$123;Trusted Connection=yes;DATABASE=\"\n",
    "connection_string = None\n",
    "import logging\n",
    "   \n",
    "# logging = Logging()\n",
    "\n",
    "class DB(object):\n",
    "    def __init__(self, database, host='127.0.0.1', user='root', password='', port='3306', tenant_id=None):\n",
    "        \"\"\"\n",
    "        Initialization of databse object.\n",
    "\n",
    "        Args:\n",
    "            databse (str): The database to connect to.\n",
    "            host (str): Host IP address. For dockerized app, it is the name of\n",
    "                the service set in the compose file.\n",
    "            user (str): Username of MySQL server. (default = 'root')\n",
    "            password (str): Password of MySQL server. For dockerized app, the\n",
    "                password is set in the compose file. (default = '')\n",
    "            port (str): Port number for MySQL. For dockerized app, the port that\n",
    "                is mapped in the compose file. (default = '3306')\n",
    "        \"\"\"\n",
    "\n",
    "        if host in [\"common_db\",\"extraction_db\", \"queue_db\", \"template_db\", \"table_db\", \"stats_db\", \"business_rules_db\", \"reports_db\"]:\n",
    "            self.HOST = os.environ['HOST_IP']\n",
    "            self.USER = os.environ['LOCAL_DB_USER']\n",
    "            self.PASSWORD = os.environ['LOCAL_DB_PASSWORD']\n",
    "            self.PORT = os.environ['LOCAL_DB_PORT']\n",
    "            self.DATABASE = f'{tenant_id}_{database}' if tenant_id is not None and tenant_id else database\n",
    "        else:\n",
    "            self.HOST = os.environ['HOST_IP']\n",
    "            self.USER = os.environ['LOCAL_DB_USER']\n",
    "            self.PASSWORD = os.environ['LOCAL_DB_PASSWORD']\n",
    "            self.PORT = os.environ['LOCAL_DB_PORT']\n",
    "            self.DATABASE = f'{tenant_id}_{database}' if tenant_id is not None and tenant_id else database\n",
    "       \n",
    "        logging.info(f'Host: {self.HOST}')\n",
    "        logging.info(f'User: {self.USER}')\n",
    "        logging.info(f'Password: {self.PASSWORD}')\n",
    "        logging.info(f'Port: {self.PORT}')\n",
    "        logging.info(f'Database: {self.DATABASE}')\n",
    "        # self.connect()\n",
    "    def connect(self, max_retry=5):\n",
    "#         retry = 1\n",
    "\n",
    "#         try:\n",
    "#             start = time()\n",
    "#             logging.debug(f'Making connection to {self.DATABASE}...')\n",
    "#             config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{self.DATABASE}?charset=utf8'\n",
    "#             self.db_ = create_engine(config, connect_args={'connect_timeout': 2}, pool_recycle=300)\n",
    "#             logging.info(f'Engine created for {self.DATABASE}')\n",
    "#             while retry <= max_retry:\n",
    "#                 try:\n",
    "#                     self.engine = self.db_.connect()\n",
    "#                     logging.info(f'Connection established succesfully to {self.DATABASE}! ({round(time() - start, 2)} secs to connect)')\n",
    "#                     break\n",
    "#                 except Exception as e:\n",
    "#                     logging.warning(f'Connection failed. Retrying... ({retry}) [{e}]')\n",
    "#                     retry += 1\n",
    "#                     self.db_.dispose()\n",
    "#         except:\n",
    "#             logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "#             return\n",
    "        data = []\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        print(inds)\n",
    "        for pos, ind in enumerate(inds):\n",
    "            if pos % 2 == 0:\n",
    "                sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "            else:\n",
    "                sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "        if connection_string:\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + self.DATABASE)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pyodbc.connect('DRIVER={' + driver + '};SERVER=' + host_ + ';DATABASE=' + database+ ';UID=' + user_ + ';PWD=' + password_ + ';Trusted Connection=yes;')\n",
    "                else:\n",
    "                    conn = pyodbc.connect('DRIVER={' + driver + '};SERVER=' + host_ + ';DATABASE=' + database + ';Trusted Connection=yes;')\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pyodbc.connect('DRIVER={' + driver + '};SERVER=' + host_ + ';DATABASE=' + database + ';Trusted Connection=yes;')\n",
    "\n",
    "    def convert_to_mssql(self, query):\n",
    "        inds = [i for i in range(len(query)) if query[i] == '`']\n",
    "        for pos, ind in enumerate(inds):\n",
    "            if pos % 2 == 0:\n",
    "                query = query[:ind] + '[' + query[ind+1:]\n",
    "            else:\n",
    "                query = query[:ind] + ']' + query[ind + 1:]\n",
    "       \n",
    "        query = query.replace('%s', '?')\n",
    "\n",
    "        return query\n",
    "\n",
    "    def execute(self, query, database=None, index_col='id', **kwargs):\n",
    "        logging.debug(f'Before converting: {query}')\n",
    "        query = self.convert_to_mssql(query)\n",
    "        logging.debug(f'After converting: {query}')\n",
    "\n",
    "        logging.debug('Connecting to DB')\n",
    "        conn = pyodbc.connect(f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={self.HOST};UID={self.USER};PWD={self.PASSWORD};Trusted Connection=yes;DATABASE={self.DATABASE}', as_dict=True)\n",
    "        logging.debug(f'Connection established with {self.DATABASE}. [{conn}]')\n",
    "        curs = conn.cursor()\n",
    "        logging.debug(f'Cursor object created. [{curs}]')\n",
    "        params = tuple(kwargs.get('params', []))\n",
    "       \n",
    "        logging.debug(f'Params: {params}')\n",
    "        logging.debug(f'Params Type: {type(params)}')\n",
    "        params = [int(i) if isinstance(i, np.int64) else i for i in params]\n",
    "        curs.execute(query, params)\n",
    "        logging.debug(f'Query executed.')\n",
    "       \n",
    "        data = None\n",
    "\n",
    "        try:\n",
    "            logging.debug(f'Fetching all data.')\n",
    "            data = curs.fetchall()\n",
    "            # logging.debug(f'Data fetched: {data}')\n",
    "            columns = [column[0] for column in curs.description]\n",
    "            logging.debug(f'Columns: {columns}')\n",
    "            result = []\n",
    "            for row in data:\n",
    "                result.append(dict(zip(columns, row)))\n",
    "            # logging.debug(f'Zipped result: {result}')\n",
    "            if result:\n",
    "                data = pd.DataFrame(result)\n",
    "            else:\n",
    "                data = pd.DataFrame(columns=columns)\n",
    "            # logging.debug(f'Data to DF: {data}')\n",
    "        except:\n",
    "            logging.debug('Update Query')\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            logging.debug(f'Data is not a DataFrame. Returning True. [{type(data)}]')\n",
    "            return True\n",
    "       \n",
    "        try:\n",
    "            if index_col is not None:\n",
    "                logging.debug(f'Setting ID as index')\n",
    "                return data.where((pd.notnull(data)), None).set_index('id')\n",
    "            else:\n",
    "                return data.where((pd.notnull(data)), None)\n",
    "        except:\n",
    "            logging.exception(f'Failed to set ID as index')\n",
    "            return data.where((pd.notnull(data)), None)\n",
    "\n",
    "    def execute__(self, query, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Executes an SQL query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that needs to be executed.\n",
    "            database (str): Name of the database to execute the query in. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            params (list/tuple/dict): List of parameters to pass to in the query.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data from the executed\n",
    "            query. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        data = None\n",
    "\n",
    "#         # Use new database if a new databse is given\n",
    "#         if database is not None:\n",
    "#             try:\n",
    "#                 config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "#                 engine = create_engine(config, pool_recycle=300)\n",
    "#             except:\n",
    "#                 logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "#                 return False\n",
    "#         else:\n",
    "#             engine = self.engine\n",
    "       \n",
    "        print('query', query)\n",
    "        if database is None:\n",
    "            database = 'karvy'\n",
    "        data = None\n",
    "        sql = query\n",
    "        user_ = self.USER\n",
    "        host_ = self.HOST\n",
    "        database = self.DATABASE\n",
    "        password_ = self.PASSWORD\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        for pos, ind in enumerate(inds):\n",
    "            if pos % 2 == 0:\n",
    "                sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "            else:\n",
    "                sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "               \n",
    "        if connection_string:\n",
    "            print('connection string', connection_string)\n",
    "            print('database', database)\n",
    "            print(type(connection_string + database))\n",
    "            print(type(connection_string + database))\n",
    "\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + database)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pymssql.connect(host=host_,database=database,user=user_,password=password_)\n",
    "                else:\n",
    "                    conn = pymssql.connect(host=host_,database=database)\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pymssql.connect(host=host_,database=database)\n",
    "        try:\n",
    "            logging.debug(f'Query: {query}')\n",
    "            logging.debug(f'Query: {query}')\n",
    "            curs = conn.cursor(as_dict = True)\n",
    "            params = kwargs.get('params', [])\n",
    "            logging.debug(f'Params: {params}')\n",
    "            curs.execute(sql, tuple(params))\n",
    "            print('query executed')\n",
    "            try:\n",
    "                data = curs.fetchall()\n",
    "                data = pd.DataFrame(data)\n",
    "                # print(data)\n",
    "            except Exception as e:\n",
    "                logging.debug('Update Query')\n",
    "            # data = pd.read_sql(sql, conn, index_col='id', **kwargs)\n",
    "        except exc.ResourceClosedError:\n",
    "            logging.warning('Query does not have any value to return.')\n",
    "            return True\n",
    "        except (exc.StatementError, OperationalError) as e:\n",
    "            logging.warning(f'Creating new connection. Engine/Connection is probably None. [{e}]')\n",
    "            self.connect()\n",
    "            data = pd.read_sql(query, self.engine, index_col='id', **kwargs)\n",
    "        except:\n",
    "            logging.exception('Something went wrong executing query. Check trace.')\n",
    "            params = kwargs['params'] if 'params' in kwargs else None\n",
    "            conn.rollback()\n",
    "            return False\n",
    "        conn.close()\n",
    "        return data.where((pd.notnull(data)), None).set_index('id')\n",
    "\n",
    "    def execute_(self, query, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Executes an SQL query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that needs to be executed.\n",
    "            database (str): Name of the database to execute the query in. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            params (list/tuple/dict): List of parameters to pass to in the query.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data from the executed\n",
    "            query. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        logging.debug(f'Executing `execute` instead of `execute_`')\n",
    "        return self.execute(query, index_col=None, **kwargs)\n",
    "       \n",
    "        data = None\n",
    "\n",
    "#         # Use new database if a new database is given\n",
    "#         if database is not None:\n",
    "#             try:\n",
    "#                 config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "#                 engine = create_engine(config, pool_recycle=300)\n",
    "#             except:\n",
    "#                 logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "#                 return False\n",
    "#         else:\n",
    "#             engine = self.engine\n",
    "\n",
    "#         try:\n",
    "#             data = pd.read_sql(query, engine, **kwargs)\n",
    "#         except exc.ResourceClosedError:\n",
    "#             return True\n",
    "#         except:\n",
    "#             logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "#             params = kwargs['params'] if 'params' in kwargs else None\n",
    "#             return False\n",
    "        print('query', query)\n",
    "        if database is None:\n",
    "            database = 'karvy'\n",
    "        data = None\n",
    "        sql = query\n",
    "        user_ = self.USER\n",
    "        database = self.DATABASE\n",
    "        host_ = self.HOST\n",
    "        password_ = self.PASSWORD\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        # for pos, ind in enumerate(inds):\n",
    "        #     if pos % 2 == 0:\n",
    "        #         sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "        #     else:\n",
    "        #         sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "               \n",
    "        if connection_string:\n",
    "            print('connection string', connection_string)\n",
    "            print('database', database)\n",
    "            print(type(connection_string + database))\n",
    "            print(type(connection_string + database))\n",
    "\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + database)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pymssql.connect(host=host_,database=database,user=user_,password=password_)\n",
    "                else:\n",
    "                    conn = pymssql.connect(host=host_,database=database)\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pymssql.connect(host=host_,database=database)\n",
    "        try:\n",
    "            logging.debug(f'Query: {query}')\n",
    "            curs = conn.cursor(as_dict = True)\n",
    "            params = kwargs.get('params', [])\n",
    "            logging.debug(f'Params: {params}')\n",
    "            curs.execute(sql, params)\n",
    "            print('query executed')\n",
    "            try:\n",
    "                data = curs.fetchall()\n",
    "                data = pd.DataFrame(data)\n",
    "                print(data)\n",
    "            except Exception as e:\n",
    "                logging.debug('Update Query')\n",
    "\n",
    "            #data = pd.read_sql(sql, conn,**kwargs)\n",
    "        except exc.ResourceClosedError:\n",
    "            logging.warning('Query does not have any value to return.')\n",
    "            return True\n",
    "        except (exc.StatementError, OperationalError) as e:\n",
    "            logging.warning(f'Creating new connection. Engine/Connection is probably None. [{e}]')\n",
    "            self.connect()\n",
    "            data = pd.read_sql(query, conn,**kwargs)\n",
    "        except:\n",
    "            logging.exception('Something went wrong executing query. Check trace.')\n",
    "            params = kwargs['params'] if 'params' in kwargs else None\n",
    "            conn.rollback()\n",
    "            return False\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "#         return data.where((pd.notnull(data)), None)\n",
    "        try:\n",
    "            return data.replace({pd.np.nan: None}).set_index('id')\n",
    "        except AttributeError as e:\n",
    "            return True\n",
    "\n",
    "    def insert(self, data, table, **kwargs):\n",
    "        \"\"\"\n",
    "        Write records stored in a DataFrame to a SQL database.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The DataFrame that needs to be write to SQL database.\n",
    "            table (str): The table in which the rcords should be written to.\n",
    "            kwargs: Keyword arguments for pandas to_sql function.\n",
    "                See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html\n",
    "                to know the arguments that can be passed.\n",
    "\n",
    "        Returns:\n",
    "            (bool) True is succesfully inserted, else false.\n",
    "        \"\"\"\n",
    "        logging.info(f'Inserting into `{table}`')\n",
    "       \n",
    "        conn = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={self.HOST};UID={self.USER};PWD={self.PASSWORD};Trusted Connection=yes;DATABASE={self.DATABASE}'\n",
    "\n",
    "       \n",
    "#         conn =  \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=IP_ADDRESS;DATABASE=DataLake;UID=USER;PWD=PASS\"\n",
    "        quoted = quote_plus(conn)\n",
    "        new_con = 'mssql+pyodbc:///?odbc_connect={}'.format(quoted)\n",
    "        self.engine = create_engine(new_con)\n",
    "#         print(self.engine)\n",
    "        try:\n",
    "            data.to_sql(table, self.engine,chunksize = None, **kwargs)\n",
    "            try:\n",
    "                self.execute(f'ALTER TABLE `{table}` ADD PRIMARY KEY (`id`);')\n",
    "            except:\n",
    "                pass\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception('Something went wrong inserting. Check trace.')\n",
    "            return False\n",
    "\n",
    "   \n",
    "   \n",
    "    def insert_(self, data, table, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Write records stored in a DataFrame to a SQL database.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The DataFrame that needs to be write to SQL database.\n",
    "            table (str): The table in which the rcords should be written to.\n",
    "            database (str): The database the table lies in. Leave it none if you\n",
    "                want use database during object creation.\n",
    "            kwargs: Keyword arguments for pandas to_sql function.\n",
    "                See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html\n",
    "                to know the arguments that can be passed.\n",
    "\n",
    "        Returns:\n",
    "            (bool) True is succesfully inserted, else false.\n",
    "        \"\"\"\n",
    "        logging.info(f'Inserting into {table}')\n",
    "\n",
    "        # # Use new database if a new databse is given\n",
    "        # if database is not None:\n",
    "        #     try:\n",
    "        #         config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "        #         engine = create_engine(config, pool_recycle=300)\n",
    "        #     except:\n",
    "        #         logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "        #         return False\n",
    "        # else:\n",
    "        #     engine = self.engine\n",
    "\n",
    "        if database is None:\n",
    "            database = 'karvy'\n",
    "        data = None\n",
    "        sql = query\n",
    "        user_ = self.USER\n",
    "        database = self.DATABASE\n",
    "        host_ = self.HOST\n",
    "        password_ = self.PASSWORD\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        # for pos, ind in enumerate(inds):\n",
    "        #     if pos % 2 == 0:\n",
    "        #         sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "        #     else:\n",
    "        #         sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "               \n",
    "        if connection_string:\n",
    "            print('connection string', connection_string)\n",
    "            print('database', database)\n",
    "            print(type(connection_string + database))\n",
    "            print(type(connection_string + database))\n",
    "\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + database)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pymssql.connect(host=host_,database=database,user=user_,password=password_)\n",
    "                else:\n",
    "                    conn = pymssql.connect(host=host_,database=database)\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pymssql.connect(host=host_,database=database)\n",
    "        try:\n",
    "            logging.debug(f'Query: {query}')\n",
    "            # data.to_sql(table, conn, **kwargs)\n",
    "            curs = conn.cursor(as_dict = True)\n",
    "            curs.execute(sql, tuple(kwargs.get('params', [])))\n",
    "            print('query executed')\n",
    "            try:\n",
    "                data = curs.fetchall()\n",
    "                data = pd.DataFrame(data)\n",
    "                print(data)\n",
    "            except Exception as e:\n",
    "                logging.debug('Update Query')\n",
    "\n",
    "            try:\n",
    "                self.execute(f'ALTER TABLE {table} ADD PRIMARY KEY (id);')\n",
    "            except:\n",
    "                pass\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception('Something went wrong inserting. Check trace.')\n",
    "            return False\n",
    "\n",
    "    def insert_dict(self, data, table):\n",
    "        \"\"\"\n",
    "        Insert dictionary into a SQL database table.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The DataFrame that needs to be write to SQL database.\n",
    "            table (str): The table in which the rcords should be written to.\n",
    "\n",
    "        Returns:\n",
    "            (bool) True is succesfully inserted, else false.\n",
    "        \"\"\"\n",
    "        logging.info(f'Inserting dictionary data into {table}...')\n",
    "        logging.debug(f'Data:\\n{data}')\n",
    "\n",
    "        try:\n",
    "            column_names = []\n",
    "            params = []\n",
    "\n",
    "            for column_name, value in data.items():\n",
    "                column_names.append(f'{column_name}')\n",
    "                params.append(value)\n",
    "\n",
    "            logging.debug(f'Column names: {column_names}')\n",
    "            logging.debug(f'Params: {params}')\n",
    "\n",
    "            columns_string = ', '.join(column_names)\n",
    "            param_placeholders = ', '.join(['%s'] * len(column_names))\n",
    "\n",
    "            query = f'INSERT INTO {table} ({columns_string}) VALUES ({param_placeholders})'\n",
    "\n",
    "            return self.execute(query, params=params)\n",
    "        except:\n",
    "            logging.exception('Error inserting data.')\n",
    "            return False\n",
    "\n",
    "    def update(self, table, update=None, where=None, database=None, force_update=False):\n",
    "        # Use new database if a new databse is given\n",
    "        # if database is not None:\n",
    "        #     try:\n",
    "        #         config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "        #         self.engine = create_engine(config, pool_recycle=300)\n",
    "        #     except:\n",
    "        #         logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "        #         return False\n",
    "\n",
    "        logging.info(f'Updating table: {table}')\n",
    "        logging.info(f'Update data: {update}')\n",
    "        logging.info(f'Where clause data: {where}')\n",
    "        logging.info(f'Force update flag: {force_update}')\n",
    "\n",
    "        try:\n",
    "            set_clause = []\n",
    "            set_value_list = []\n",
    "            where_clause = []\n",
    "            where_value_list = []\n",
    "\n",
    "            if where is not None and where:\n",
    "                for set_column, set_value in update.items():\n",
    "                    set_clause.append(f'{set_column}=%s')\n",
    "                    set_value_list.append(set_value)\n",
    "                set_clause_string = ', '.join(set_clause)\n",
    "            else:\n",
    "                logging.error(f'Update dictionary is None/empty. Must have some update clause.')\n",
    "                return False\n",
    "\n",
    "            if where is not None and where:\n",
    "                for where_column, where_value in where.items():\n",
    "                    where_clause.append(f'{where_column}=%s')\n",
    "                    where_value_list.append(where_value)\n",
    "                where_clause_string = ' AND '.join(where_clause)\n",
    "                query = f'UPDATE {table} SET {set_clause_string} WHERE {where_clause_string}'\n",
    "            else:\n",
    "                if force_update:\n",
    "                    query = f'UPDATE {table} SET {set_clause_string}'\n",
    "                else:\n",
    "                    message = 'Where dictionary is None/empty. If you want to force update every row, pass force_update as True.'\n",
    "                    logging.error(message)\n",
    "                    return False\n",
    "\n",
    "            params = set_value_list + where_value_list\n",
    "            self.execute(query, params=params)\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception('Something went wrong updating. Check trace.')\n",
    "            return False\n",
    "\n",
    "    def get_column_names(self, table, database=None):\n",
    "        \"\"\"\n",
    "        Get all column names from an SQL table.\n",
    "\n",
    "        Args:\n",
    "            table (str): Name of the table from which column names should be extracted.\n",
    "            database (str): Name of the database in which the table lies. Leave\n",
    "                it none if you want use database during object creation.\n",
    "\n",
    "        Returns:\n",
    "            (list) List of headers. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(f'Getting column names of table {table}')\n",
    "            return list(self.execute(f'SELECT * FROM {table}', database))\n",
    "        except:\n",
    "            logging.exception('Something went wrong getting column names. Check trace.')\n",
    "            return\n",
    "\n",
    "    def execute_default_index(self, query, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Executes an SQL query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that needs to be executed.\n",
    "            database (str): Name of the database to execute the query in. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            params (list/tuple/dict): List of parameters to pass to in the query.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data from the executed\n",
    "            query. (None if an error occurs)\n",
    "        \"\"\"\n",
    "\n",
    "        logging.debug(f'Executing `execute` instead of `execute_default_index`')\n",
    "        return self.execute(query, index_col=None, **kwargs)\n",
    "        data = None\n",
    "\n",
    "        # # Use new database if a new databse is given\n",
    "        # if database is not None:\n",
    "        #     try:\n",
    "        #         config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "        #         engine = create_engine(config, pool_recycle=300)\n",
    "        #     except:\n",
    "        #         logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "        #         return False\n",
    "        # else:\n",
    "        #     engine = self.engine\n",
    "\n",
    "        print('query', query)\n",
    "        if database is None:\n",
    "            database = 'karvy'\n",
    "        data = None\n",
    "        sql = query\n",
    "        user_ = self.USER\n",
    "        host_ = self.HOST\n",
    "        database = self.DATABASE\n",
    "        password_ = self.PASSWORD\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        for pos, ind in enumerate(inds):\n",
    "            if pos % 2 == 0:\n",
    "                sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "            else:\n",
    "                sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "               \n",
    "        if connection_string:\n",
    "            print('connection string', connection_string)\n",
    "            print('database', database)\n",
    "            print(type(connection_string + database))\n",
    "            print(type(connection_string + database))\n",
    "\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + database)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pymssql.connect(host=host_,database=database,user=user_,password=password_)\n",
    "                else:\n",
    "                    conn = pymssql.connect(host=host_,database=database)\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pymssql.connect(host=host_,database=database)\n",
    "\n",
    "        try:\n",
    "            logging.debug(f'Query: {query}')\n",
    "            # data.to_sql(table, conn, **kwargs)\n",
    "            curs = conn.cursor(as_dict = True)\n",
    "           \n",
    "            curs.execute(sql, tuple(kwargs.get('params', [])))\n",
    "            print('query executed')\n",
    "            try:\n",
    "                data = curs.fetchall()\n",
    "                data = pd.DataFrame(data)\n",
    "                print(data)\n",
    "            except Exception as e:\n",
    "                logging.debug('Update Query')\n",
    "            # data = pd.read_sql(query, conn, **kwargs)\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "        except exc.ResourceClosedError:\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception(f'Something went wrong while executing query. Check trace.')\n",
    "            params = kwargs['params'] if 'params' in kwargs else None\n",
    "            return False\n",
    "\n",
    "        return data.where((pd.notnull(data)), None).set_index('id')\n",
    "\n",
    "\n",
    "    def get_all(self, table, database=None, discard=None):\n",
    "        \"\"\"\n",
    "        Get all data from an SQL table.\n",
    "\n",
    "        Args:\n",
    "            table (str): Name of the table from which data should be extracted.\n",
    "            database (str): Name of the database in which the table lies. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            discard (list): columns to be excluded while selecting all\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data. (None if an error\n",
    "            occurs)\n",
    "        \"\"\"\n",
    "        logging.info(f'Getting all data from {table}')\n",
    "        if discard:\n",
    "            logging.info(f'Discarding columns {discard}')\n",
    "            columns = list(self.execute_default_index(f'SHOW COLUMNS FROM {table}',database).Field)\n",
    "            columns = [col for col in columns if col not in discard]\n",
    "            columns_str = json.dumps(columns).replace(\"'\",'').replace('\"','')[1:-1]\n",
    "            return self.execute(f'SELECT {columns_str} FROM {table}', database)\n",
    "\n",
    "        return self.execute(f'SELECT * FROM {table}', database)\n",
    "\n",
    "    def get_latest(self, data, group_by_col, sort_col):\n",
    "        \"\"\"\n",
    "        Group data by a column containing repeated values and get latest from it by\n",
    "        taking the latest value based on another column.\n",
    "\n",
    "        Example:\n",
    "        Get the latest products\n",
    "            id     product   date\n",
    "            220    6647     2014-09-01\n",
    "            220    6647     2014-10-16\n",
    "            826    3380     2014-11-11\n",
    "            826    3380     2015-05-19\n",
    "            901    4555     2014-09-01\n",
    "            901    4555     2014-11-01\n",
    "\n",
    "        The function will return\n",
    "            id     product   date\n",
    "            220    6647     2014-10-16\n",
    "            826    3380     2015-05-19\n",
    "            901    4555     2014-11-01\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): Pandas DataFrame to query on.\n",
    "            group_by_col (str): Column containing repeated values.\n",
    "            sort_col (str): Column to identify the latest record.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) Contains the latest records. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info('Grouping data...')\n",
    "            logging.info(f'Data: {data}')\n",
    "            logging.info(f'Group by column: {group_by_col}')\n",
    "            logging.info(f'Sort column: {sort_col}')\n",
    "            return data.sort_values(sort_col).set_index('id').groupby(group_by_col).tail(1)\n",
    "        except KeyError as e:\n",
    "            logging.error(f'Column {e.args[0]} does not exist.')\n",
    "            return None\n",
    "        except:\n",
    "            logging.exception('Something went wrong while grouping data.')\n",
    "            return None\n",
    "\n",
    "db_config = {\n",
    "   'host': '13.233.100.20',\n",
    "   'port': '1433',\n",
    "   'user': 'SA',\n",
    "   'password':'Akhil@Akhil1'\n",
    "}\n",
    "import os\n",
    "os.environ['HOST_IP'] = '13.233.100.20'\n",
    "os.environ['LOCAL_DB_USER']='SA'\n",
    "os.environ['LOCAL_DB_PASSWORD'] = 'Akhil@Akhil1'\n",
    "os.environ['LOCAL_DB_PORT'] = '1433'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:12.910490Z",
     "start_time": "2020-08-25T08:31:12.898964Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def debug_df(df, num=20):\n",
    "    df.printSchema()\n",
    "    df.show(num)\n",
    "    \n",
    "\n",
    "def decrease_date(s, days):\n",
    "    date = datetime.datetime.strptime(s, \"%Y-%m-%d\")\n",
    "    modified_date = date - datetime.timedelta(days=days)\n",
    "    return datetime.datetime.strftime(modified_date, \"%Y-%m-%d\")\n",
    "\n",
    "def read_df(table, columns_to_retrieve, database):\n",
    "    \n",
    "    query = f\"SELECT {','.join(columns_to_retrieve)} from {table}\"\n",
    "    # logging.info(f\"query to be executed is {query}\")\n",
    "    \n",
    "    data = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "            .option(\"query\", query) \\\n",
    "            .option(\"user\", user) \\\n",
    "            .option(\"password\", password) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .load()\n",
    "    return data\n",
    "\n",
    "from datetime import timedelta, date\n",
    "#\n",
    "def daterange(start_date, end_date):\n",
    "    start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield datetime.datetime.strftime(start_date + timedelta(n), \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:13.385413Z",
     "start_time": "2020-08-25T08:31:13.347638Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "# required libraries\n",
    "from pyspark import SparkContext, SparkConf #\n",
    "from pyspark.sql import SparkSession # for dataframe conversions\n",
    "# for type conversions\n",
    "from datetime import datetime\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, udf, sum # col, udf (user defined functions)\n",
    "from pyspark.sql.types import DateType, IntegerType # type\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import trim # for trimming\n",
    "from pyspark.sql.functions import collect_list, sort_array, row_number # for grouping and taking the last/first element\n",
    "from pyspark.sql.functions import *\n",
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import datetime \n",
    "\n",
    "\n",
    "# intialize spark\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    " \n",
    "#Create spark context and sparksession\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "SparkContext.setSystemProperty(\"spark.driver.memory\", \"50g\")\n",
    "SparkContext.setSystemProperty(\"spark.executor.memory\", \"50g\")\n",
    "SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"50g\")\n",
    "\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:13.732129Z",
     "start_time": "2020-08-25T08:31:13.722136Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# configs\n",
    "import os\n",
    "os.environ['HOST_IP'] = '192.168.15.126'\n",
    "os.environ['LOCAL_DB_USER'] = 'BRS'\n",
    "os.environ['LOCAL_DB_PASSWORD'] = 'Kfintech123$'\n",
    "os.environ['LOCAL_DB_PORT'] = '1433'\n",
    "\n",
    "import os\n",
    "\n",
    "# comment when using the configs from env file\n",
    "default_ip = '13.233.100.20'\n",
    "default_user  = 'SA'\n",
    "default_password = 'Akhil@Akhil1'\n",
    "default_port = '1433'\n",
    "default_tenant_id = 'karvy'\n",
    "\n",
    "# # initializations \n",
    "server = os.environ.get('HOST_IP', default_ip)\n",
    "port = os.environ.get('LOCAL_DB_PORT', default_port)\n",
    "user = os.environ.get('LOCAL_DB_USER', default_user)\n",
    "password = os.environ.get('LOCAL_DB_PASSWORD', default_password)\n",
    "\n",
    "db_config = {\n",
    "   'host': server,\n",
    "   'port': port,\n",
    "   'user': user,\n",
    "   'password':password\n",
    "}\n",
    "\n",
    "def save_metric(date, metric_name, metric_value, fund_name, group_level, table_name, database='IB_Comp_funds'):\n",
    "    db = DB(database, tenant_id='',**db_config)\n",
    "    try:\n",
    "        query = f\"INSERT INTO `karvy_metrics` VALUES ( '{date}','{metric_name}','{metric_value}', '{table_name}', '{group_level}', '{fund_name}')\"\n",
    "        db.execute_(query)\n",
    "    except Exception as e:\n",
    "        logging.error(\"Unable to insert metrics data\")\n",
    "        logging.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:14.750085Z",
     "start_time": "2020-08-25T08:31:14.723074Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 µs, sys: 5 µs, total: 15 µs\n",
      "Wall time: 35.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def initialize(date_str, table, database='funds', date_column='BatchCloseDate', tenant_id='karvy',\n",
    "              transaction_status='Active', purred = 'Purred', transaction_type = 'TransactionType',\n",
    "              folio = 'Folio', purchase_units = 'DB_Units', redemption_units = 'Cr_Units',scheme = 'SchemeCode',\n",
    "               plan = 'PlanCode',groupby_level='SP', direct_db=None):\n",
    "    \"\"\"Initialization script which takes in batch_close_date and populates opening balance\n",
    "    Args:\n",
    "        date(str) The date upto which we need to initialize\n",
    "        table(str) \n",
    "        date_column(str)\n",
    "        tenant_id(str)\n",
    "    \n",
    "    Note: batch_close_date is (yyyy-MM-dd) (for ex. 2020-04-30), inclusive \n",
    "    \"\"\"\n",
    "    # fundtable column names\n",
    "    purchase_units = 'DB_Units'\n",
    "    redemption_units = 'Cr_Units'\n",
    "    scheme = 'SchemeCode'\n",
    "    plan = 'PlanCode'\n",
    "    \n",
    "    # configurations we use\n",
    "    batch_close_date = 'batch_close_date'\n",
    "    db_units = 'purchase_units'\n",
    "    cr_units = 'redemption_units'\n",
    "    balance_units = 'balance_units'\n",
    "    day_purchase_units = 'day_pu'\n",
    "    day_redemption_units = 'day_ru'\n",
    "    balance_purchase_units = 'balance_pu'\n",
    "    balance_redemption_units = 'balance_ru'\n",
    "    calculated_date = 'calculated_date'\n",
    "    \n",
    "#     inflow_db_units = 'inflow_purchase_units'\n",
    "#     inflow_cr_units = 'inflow_redemption_units'\n",
    "#     inflow_balance_units = 'inflow_balance_units'\n",
    "#     inflow_day_purchase_units = 'inflow_day_pu'\n",
    "#     inflow_day_redemption_units = 'inflow_day_ru'\n",
    "#     inflow_balance_purchase_units = 'inflow_balance_pu'\n",
    "#     inflow_balance_redemption_units = 'inflow_balance_ru'\n",
    "\n",
    "    \n",
    "#     outflow_db_units = 'outflow_purchase_units'\n",
    "#     outflow_cr_units = 'outflow_redemption_units'\n",
    "#     outlfow_balance_units = 'outlfow_balance_units'\n",
    "#     outlfow_day_purchase_units = 'outlfow_day_pu'\n",
    "#     outlfow_day_redemption_units = 'outlfow_day_ru'\n",
    "#     outlfow_balance_purchase_units = 'outlfow_balance_pu'\n",
    "#     outlfow_balance_redemption_units = 'outlfow_balance_ru'\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    database = direct_db or (f'{tenant_id or default_tenant_id}_{database}')\n",
    "    \n",
    "\n",
    "    # read data\n",
    "    data = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .load()\n",
    "#     data = data.filter(col(scheme) == 'TF')\n",
    "    \n",
    "#     data = data.cache()\n",
    "    total_count = data.count()\n",
    "    # debug_df(data, 10)\n",
    "    \n",
    "    # some preprocessings in the data, additional trimmings etc\n",
    "    data = data.withColumn(transaction_status, upper(trim(col(transaction_status))))\n",
    "    data = data.withColumn(purred, upper(trim(col(purred))))\n",
    "    data = data.withColumn(folio, upper(trim(col(folio))))\n",
    "    data = data.withColumn(scheme, upper(trim(col(scheme))))\n",
    "    data = data.withColumn(plan, upper(trim(col(plan))))\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "#     data = data.filter((col(scheme).isin('IC','HC')))\n",
    "    \n",
    "#     print ('here')\n",
    "    \n",
    "    # cast the date column into dates, as we are concerned only with dates now\n",
    "    data = data.withColumn(date_column, col(date_column).cast('date'))\n",
    "    \n",
    "    # filter the date till the batch_close_date (inclusive)\n",
    "    data = data.filter(col(date_column) <= date_str)\n",
    "        \n",
    "    # filter the data according to rules\n",
    "    data = data.filter((col(date_column).isNotNull()) )\n",
    "    # data = data.filter((col(date_column) != '') ) # this will not work for few types\n",
    "    data = data.filter( ~trim(col(date_column)).cast(\"string\").eqNullSafe(''))\n",
    "    \n",
    "    # do must be rules\n",
    "    data = data.filter( (trim(upper((col(transaction_status))))) == \"Y\")\n",
    "    data = data.filter( (trim(upper(col(purred))) == \"P\") | (trim(upper(col(purred))) == \"R\") )\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "    # our configurations\n",
    "    data = data.withColumn(batch_close_date, data[date_column])\n",
    "    \n",
    "#     print ('pur, redem')\n",
    "    # bring in purchase and redemption units\n",
    "    data = data.withColumn(db_units, when((col(purred) == \"P\"), col(purchase_units)).otherwise(0))\n",
    "    data = data.withColumn(cr_units, when((col(purred) == \"R\"), col(redemption_units)).otherwise(0))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#     print ('1')\n",
    "    # get the group by and window partitions based on partitions\n",
    "    group_by_cols = []\n",
    "    window_partition = []\n",
    "    if groupby_level == 'SP':\n",
    "        window_partition = [scheme, plan]\n",
    "        group_by_cols = [scheme, plan, batch_close_date]\n",
    "        # scheme_plan wise we might need to filter out some transaction types\n",
    "        ignored_tr_types = ['CNI', 'CNO', 'TRMI', \n",
    "                            'TRMO', 'TRFI', 'TRFO', 'PLDO',\n",
    "                            'UPLO', 'DMT', 'RMT', 'CNIR', 'CNOR', 'TRMIR', 'TRMOR',\n",
    "                            'TRFIR', 'TRFOR', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR']\n",
    "        data = data.filter( ~(col(transaction_type).isin(ignored_tr_types)) )\n",
    "    elif groupby_level == 'SPT':\n",
    "        window_partition = [scheme, plan, transaction_type]\n",
    "        group_by_cols  = [scheme, plan, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPF':\n",
    "        window_partition = [scheme, plan, folio]\n",
    "        group_by_cols  = [scheme, plan, folio, batch_close_date]\n",
    "    elif groupby_level == 'SPFB':\n",
    "            window_partition = [scheme, plan, folio, broker]\n",
    "            group_by_cols  = [scheme, plan, folio, broker, batch_close_date]\n",
    "    elif groupby_level == 'SPFT':\n",
    "        window_partition = [scheme, plan, folio, transaction_type]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPFTB':\n",
    "        window_partition = [scheme, plan, folio, transaction_type, broker]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, broker, batch_close_date]\n",
    "        \n",
    "        # populate broker code\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # roll up the data\n",
    "    rolledup_data = data.groupBy(group_by_cols)\n",
    "    rolledup_data = rolledup_data.agg({db_units:'sum', cr_units:'sum'})\n",
    "        \n",
    "    rolledup_data = rolledup_data.withColumnRenamed(f\"sum({db_units})\", day_purchase_units).withColumnRenamed(f\"sum({cr_units})\", day_redemption_units)\n",
    "    rolledup_data = rolledup_data.withColumn(balance_purchase_units, sum(col(day_purchase_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    rolledup_data = rolledup_data.withColumn(balance_redemption_units, sum(col(day_redemption_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    rolledup_data = rolledup_data.withColumn(balance_units, (col(balance_purchase_units) - col(balance_redemption_units)))\n",
    "    \n",
    "#     print ('rolling up done')\n",
    "    # get the latest data\n",
    "    latest_data = rolledup_data.filter(col(batch_close_date) <= date_str)\n",
    "    w = Window.partitionBy(window_partition).orderBy(col(batch_close_date).desc())\n",
    "    latest_data = latest_data.withColumn(\"rrn\", row_number().over(w)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    \n",
    "    # maintained the calculated date (the latest data upto the calculated date)\n",
    "    latest_data = latest_data.withColumn(calculated_date, lit(date_str).cast('date'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # store in parquet file for optimization of space and only one file and multi partitions\n",
    "    # but write now store in csv and maintain date wise and colesce one\n",
    "    latest_data = latest_data.cache()\n",
    "    \n",
    "    \n",
    "#     latest_data.show()\n",
    "    # latest_data.coalesce(1).write.csv(f\"{table}_latest/data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "    latest_data.coalesce(1).write.parquet(f\"{table}_latest/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    print (f'inital file on date {date_str} written')\n",
    "    return total_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dialyjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:16.388873Z",
     "start_time": "2020-08-25T08:31:16.352684Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 µs, sys: 6 µs, total: 18 µs\n",
      "Wall time: 27.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def dialy_job(date_str, table='trans116', database='funds', date_column='BatchCloseDate', tenant_id='karvy',\n",
    "              transaction_status='Active', purred = 'Purred', transaction_type = 'TransactionType',\n",
    "              folio = 'Folio', purchase_units = 'DB_Units', redemption_units = 'Cr_Units',scheme = 'SchemeCode',\n",
    "               plan = 'PlanCode',groupby_level='SP', direct_db=None, fn_fromdt = 'fn_fromdt',fn_fromdt_format = 'dd/MM/yyyy',\n",
    "              fn_scheme = 'fn_scheme',fn_plan = 'fn_plan', fn_nav = 'fn_nav', nav_table='nav_master', \n",
    "              scheme_table='scheme_master',scheme_code = 'scheme_code', \n",
    "              plan_code = 'plan_code', category = 'SebiSchemeCategory',\n",
    "              subcategory = 'SebiSchemeSubCategory',nature = 'nature', newmcrid='NewMCRId'\n",
    "             ):\n",
    "    \"\"\"Dialy run this and store the latest data and aum data too\"\"\"\n",
    "    \n",
    "    # inflow outflow\n",
    "    inflow_db_trtypes = ['NEW', 'ADD', 'IPO', 'SIN', 'LTIN', 'LTIA', 'STPN', 'STPA', 'STPI','DIR', 'DSPI', 'SWIN','SWIA']\n",
    "    inflow_cr_trtypes = ['NEWR', 'ADDR', 'IPOR', 'SINR', 'LTINR', 'LTIAR', 'STPNR', 'STPAR', 'STPIR','DIRR', 'DSPIR', 'SWINR','SWIAR']\n",
    "    \n",
    "    outflow_db_trtypes = ['FULR', 'REDR', 'LTOFR', 'LTOPR', 'STPOR', 'SWDR', 'TRGR', 'SWOPR', 'SWOFR']\n",
    "    outflow_cr_trtypes = ['FUL', 'RED', 'SWD','TRG','LTOF', 'LTOP','STPO', 'SWOP', 'SWOF']\n",
    "    \n",
    "    \n",
    "    \n",
    "    # configurations we use\n",
    "    batch_close_date = 'batch_close_date'\n",
    "    db_units = 'purchase_units'\n",
    "    cr_units = 'redemption_units'\n",
    "    balance_units = 'balance_units'\n",
    "    day_purchase_units = 'day_pu'\n",
    "    day_redemption_units = 'day_ru'\n",
    "    balance_purchase_units = 'balance_pu'\n",
    "    balance_redemption_units = 'balance_ru'\n",
    "    calculated_date = 'calculated_date'\n",
    "    today_pu = 'today_pu'\n",
    "    today_ru = 'today_ru'\n",
    "    effective_nav = 'effective_nav'\n",
    "    aum = 'aum'\n",
    "    aaum = 'aaum'\n",
    "    inflow = 'inflow'\n",
    "    outflow = 'outflow'\n",
    "    inflow_db_units = 'inflow_purchase_units'\n",
    "    inflow_cr_units = 'inflow_redemption_units'\n",
    "    outflow_db_units = 'outflow_purchase_units'\n",
    "    outflow_cr_units = 'outflow_redemption_units'\n",
    "    inflow_units = 'inflow_units'\n",
    "    outflow_units = 'outflow_units'\n",
    "    \n",
    "    # get the latest data from the previously stored file\n",
    "    date_obj = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    day_num = date_obj.day\n",
    "    previous_day = date_obj - datetime.timedelta(1)\n",
    "    previous_day_str = previous_day.strftime('%Y-%m-%d')\n",
    "    latest_data = spark.read.parquet(f\"{table}_latest/data_{groupby_level}_{previous_day_str}.parquet\")\n",
    "\n",
    "    # debug_df(latest_data)\n",
    "    \n",
    "    # get  the todays data\n",
    "    database = direct_db or (f'{tenant_id or default_tenant_id}_{database}')\n",
    "    query = f\"SELECT * from {table} where CAST({date_column} AS DATE)='{date_str}'\"\n",
    "    data  = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "            .option(\"query\", query) \\\n",
    "            .option(\"user\", user) \\\n",
    "            .option(\"password\", password) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .load()\n",
    "#     data = data.filter(col(scheme) == 'TF')\n",
    "    data = data.cache()\n",
    "    day_count = data.count()\n",
    "#     data = data.filter((col(scheme)=='IC'))\n",
    "    # latest_data = latest_data.filter((col(scheme) == 'OV') & (col(plan) == 'RG'))\n",
    "    # data = data.filter((col(scheme) == 'OV') & (col(plan) == 'RG'))\n",
    "    # print (data.count())\n",
    "    # debug_df(data)\n",
    "    \n",
    "    # calculate all the steps as in initialization\n",
    "    # some preprocessings in the data, additional trimmings etc\n",
    "    data = data.withColumn(transaction_status, upper(trim(col(transaction_status))))\n",
    "    data = data.withColumn(purred, upper(trim(col(purred))))\n",
    "    data = data.withColumn(folio, upper(trim(col(folio))))\n",
    "    data = data.withColumn(scheme, upper(trim(col(scheme))))\n",
    "    data = data.withColumn(plan, upper(trim(col(plan))))\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "#     data = data.filter((col(scheme).isin('IC','HC')))\n",
    "    \n",
    "\n",
    "    # cast the date column into dates, as we are concerned only with dates now\n",
    "    data = data.withColumn(date_column, col(date_column).cast('date'))\n",
    "    \n",
    "    # filter the date till the batch_close_date (inclusive)\n",
    "    data = data.filter(col(date_column) <= date_str)\n",
    "    \n",
    "        \n",
    "    # filter the data according to rules\n",
    "    data = data.filter((col(date_column).isNotNull()) )\n",
    "    # data = data.filter((col(date_column) != '') ) # this will not work for few types\n",
    "    data = data.filter( ~trim(col(date_column)).cast(\"string\").eqNullSafe(''))\n",
    "    \n",
    "    # do must be rules\n",
    "    data = data.filter( (trim(upper((col(transaction_status))))) == \"Y\")\n",
    "    data = data.filter( (trim(upper(col(purred))) == \"P\") | (trim(upper(col(purred))) == \"R\") )\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "    # our configurations\n",
    "    data = data.withColumn(batch_close_date, data[date_column])\n",
    "        \n",
    "    # bring in purchase and redemption units\n",
    "    data = data.withColumn(db_units, when((col(purred) == \"P\"), col(purchase_units)).otherwise(0))\n",
    "    data = data.withColumn(cr_units, when((col(purred) == \"R\"), col(redemption_units)).otherwise(0))\n",
    "    \n",
    "    group_by_cols = []\n",
    "    window_partition = []\n",
    "    if groupby_level == 'SP':\n",
    "        window_partition = [scheme, plan]\n",
    "        group_by_cols = [scheme, plan, batch_close_date]\n",
    "        # scheme_plan wise we might need to filter out some transaction types\n",
    "        ignored_tr_types = ['CNI', 'CNO', 'TRMI', 'TRMO', 'TRFI', 'TRFO', 'PLDO', 'UPLO',\n",
    "                            'DMT', 'RMT', 'CNIR', 'CNOR', 'TRMIR', 'TRMOR', 'TRFIR', 'TRFOR', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR']\n",
    "        data = data.filter( ~(col(transaction_type).isin(ignored_tr_types)) )\n",
    "    elif groupby_level == 'SPT':\n",
    "        window_partition = [scheme, plan, transaction_type]\n",
    "        group_by_cols  = [scheme, plan, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPF':\n",
    "        window_partition = [scheme, plan, folio]\n",
    "        group_by_cols  = [scheme, plan, folio, batch_close_date]\n",
    "    elif groupby_level == 'SPFB':\n",
    "            window_partition = [scheme, plan, folio, broker]\n",
    "            group_by_cols  = [scheme, plan, folio, broker, batch_close_date]\n",
    "    elif groupby_level == 'SPFT':\n",
    "        window_partition = [scheme, plan, folio, transaction_type]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPFTB':\n",
    "        window_partition = [scheme, plan, folio, transaction_type, broker]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, broker, batch_close_date]\n",
    "    \n",
    "    # roll up the data\n",
    "    rolledup_data = data.groupBy(group_by_cols)\n",
    "    rolledup_data = rolledup_data.agg({db_units:'sum', cr_units:'sum'})\n",
    "        \n",
    "    rolledup_data = rolledup_data.withColumnRenamed(f\"sum({db_units})\", day_purchase_units).withColumnRenamed(f\"sum({cr_units})\", day_redemption_units)\n",
    "\n",
    "    # inflow outflow units\n",
    "    \n",
    "    \n",
    "    latest_data = latest_data.drop(day_purchase_units, day_redemption_units, batch_close_date, balance_units)\n",
    "    latest_data = latest_data.withColumnRenamed(balance_purchase_units, day_purchase_units)\n",
    "    latest_data = latest_data.withColumnRenamed(balance_redemption_units, day_redemption_units)\n",
    "    latest_data = latest_data.withColumnRenamed(calculated_date, batch_close_date)\n",
    "    \n",
    "    # debug_df(rolledup_data)\n",
    "    # debug_df(latest_data)\n",
    "    combined_data = latest_data.union(rolledup_data.select(latest_data.columns))\n",
    "    combined_data = combined_data.cache()\n",
    "    combined_count = combined_data.count()\n",
    "    # debug_df(rolledup_data)\n",
    "    # debug_df(latest_data)\n",
    "    # debug_df(combined_data)\n",
    "    combined_data = combined_data.withColumn(balance_purchase_units, sum(col(day_purchase_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    combined_data = combined_data.withColumn(balance_redemption_units, sum(col(day_redemption_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    combined_data = combined_data.withColumn(balance_units, (col(balance_purchase_units) - col(balance_redemption_units)))\n",
    "    # debug_df(combined_data)\n",
    "    \n",
    "    \n",
    "    # store the latest day data again\n",
    "    # get the latest data\n",
    "    combined_data = combined_data.filter(col(batch_close_date) <= date_str)\n",
    "    w = Window.partitionBy(window_partition).orderBy(col(batch_close_date).desc())\n",
    "    combined_data = combined_data.withColumn(\"rrn\", row_number().over(w)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    \n",
    "    \n",
    "    # maintained the calculated date (the latest data upto the calculated date)\n",
    "    combined_data = combined_data.withColumn(calculated_date, lit(date_str).cast('date'))\n",
    "    \n",
    "    # debug_df(combined_data)\n",
    "    # store in parquet file for optimization of space and only one file and multi partitions\n",
    "    # but write now store in csv and maintain date wise and colesce one\n",
    "    combined_data = combined_data.cache()\n",
    "    # combined_data.coalesce(1).write.csv(f\"{table}_latest/data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "    combined_data.coalesce(1).write.parquet(f\"{table}_latest/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    \n",
    "    # join the nav, scheme_master data\n",
    "    nav_data = read_df(nav_table, '*', database)\n",
    "    nav_data = nav_data.withColumnRenamed(fn_scheme, scheme)\n",
    "    nav_data = nav_data.withColumnRenamed(fn_plan, plan)\n",
    "    nav_data = nav_data.withColumn(fn_fromdt, col(fn_fromdt).cast('date'))\n",
    "\n",
    "    scheme_master = read_df(scheme_table, [scheme_code, plan_code, nature, category, subcategory, newmcrid], database)\n",
    "    scheme_master = scheme_master.withColumnRenamed(scheme_code, scheme)\n",
    "    scheme_master = scheme_master.withColumnRenamed(plan_code, plan)\n",
    "\n",
    "    scheme_master_ = scheme_master.dropDuplicates([scheme, plan])\n",
    "    nav_scheme = nav_data.join(scheme_master_, on=[scheme, plan], how='left')\n",
    "    nav_data = nav_scheme\n",
    "    \n",
    "    # debug_df(nav_data)\n",
    "    \n",
    "    # calculate the aum \n",
    "    combined_data = combined_data.withColumn(effective_nav, date_sub(col(calculated_date), 1))\n",
    "    combined_data = combined_data.withColumn(today_pu, when((col(calculated_date) == col(batch_close_date)), col(day_purchase_units)).otherwise(0))\n",
    "    combined_data = combined_data.withColumn(today_ru, when((col(calculated_date) == col(batch_close_date)), col(day_redemption_units)).otherwise(0))\n",
    "    \n",
    "    # inflow outflow addition\n",
    "    inflow_db_condition = col(transaction_type).isin(inflow_db_trtypes)\n",
    "    inflow_cr_condition = col(transaction_type).isin(inflow_cr_trtypes)\n",
    "    combined_data = combined_data.withColumn(inflow_db_units, when(inflow_db_condition, col(today_pu)).otherwise(0))\n",
    "    combined_data = combined_data.withColumn(inflow_cr_units, when(inflow_cr_condition, col(today_ru)).otherwise(0))\n",
    "    \n",
    "    outflow_db_condition = col(transaction_type).isin(outflow_db_trtypes)\n",
    "    outflow_cr_condition = col(transaction_type).isin(outflow_cr_trtypes)\n",
    "    combined_data = combined_data.withColumn(outflow_db_units, when(outflow_db_condition, col(today_pu)).otherwise(0))\n",
    "    combined_data = combined_data.withColumn(outflow_cr_units, when(outflow_cr_condition, col(today_ru)).otherwise(0))\n",
    "    \n",
    "    combined_data = combined_data.withColumn(inflow_units, col(inflow_db_units) - col(inflow_cr_units))\n",
    "    combined_data = combined_data.withColumn(outflow_units, col(outflow_cr_units) - col(outflow_db_units))\n",
    "    \n",
    "    # debug_df(combined_data)\n",
    "    \n",
    "    nav_filteredFT = nav_data.filter(col(fn_fromdt) < date_str)\n",
    "    navw = Window.partitionBy([scheme, plan]).orderBy(col(fn_fromdt).desc())\n",
    "    nav_populate = nav_filteredFT.withColumn(\"rrn\", row_number().over(navw)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    nav_populate = nav_populate.withColumn(calculated_date, lit(date_str))\n",
    "    nav_populate = nav_populate.select([scheme, plan, fn_fromdt, fn_nav, calculated_date, category, subcategory, nature, newmcrid])\n",
    "    \n",
    "#     debug_df(nav_populate)\n",
    "    \n",
    "    joined = combined_data.join(nav_populate, on=[scheme, plan, calculated_date], how='left')\n",
    "    joined = joined.withColumn(aum, col(fn_nav) * col(balance_units))\n",
    "    joined = joined.withColumn(inflow, col(fn_nav) * col(inflow_units))\n",
    "    joined = joined.withColumn(outflow, col(fn_nav) * col(outflow_units))\n",
    "    \n",
    "#     aum_dummy = f'{aum}_d'\n",
    "#     final_joined = joined.withColumn(aum_dummy, col(aum))\n",
    "#     final_joined = final_joined.fillna({aum_dummy: 0})\n",
    "    \n",
    "#     # moving average logic\n",
    "#     print (day_num)\n",
    "#     if day_num == 1:\n",
    "#         final_joined = final_joined.withColumn(f'pre_{aum_dummy}', lit(0))\n",
    "#         final_joined = final_joined.withColumn(aaum, col(aum_dummy))\n",
    "#     elif day_num == 2:\n",
    "#         final_joined = final_joined.withColumn(f'pre_{aum_dummy}', lit(0))\n",
    "#         final_joined = final_joined.withColumn(aaum, col(aum_dummy))\n",
    "#     else:\n",
    "#         previous_day_aum = spark.read.parquet(f\"{table}_dialy/aaum_data_{groupby_level}_{previous_day_str}.parquet\")\n",
    "#         previous_day_aum.withColumnRenamed(aum_dummy, f'pre_{aum_dummy}')\n",
    "#         final_joined = final_joined.join(previous_day_aum.select(window_partition + [f'pre_{aum_dummy}']), on=window_partition, how='left')\n",
    "#         final_joined = final_joined.fillna({f'pre_{aum_dummy}': 0})\n",
    "#         final_joined = final_joined.withColumn(aaum, (col(aum_dummy) + col(f'pre_{aum_dummy}') / day_num))\n",
    "        \n",
    "    \n",
    "#     debug_df(joined)\n",
    "    joined = joined.cache()\n",
    "    # store the data in the files\n",
    "    # joined.coalesce(1).write.csv(f\"{table}_dialy/data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "    joined.coalesce(1).write.parquet(f\"{table}_dialy/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "#     final_joined = final_joined.cache()\n",
    "#     final_joined.coalesce(1).write.csv(f\"{table}_dialy/aaum_data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "#     final_joined.coalesce(1).write.parquet(f\"{table}_dialy/aaum_data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    \n",
    "    # upload the data if needed\n",
    "    # Done\n",
    "    print (f'dialy file on date {date_str} generated')\n",
    "    return day_count, combined_count\n",
    "\n",
    "# dialy_job('2020-05-01', groupby_level='SPFT', table='Trans_116', direct_db='BankRecon', nav_table='fund_navreg_AXA_29072020',\n",
    "#         scheme_table='Fund_Master_AXA_29072020', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory'\n",
    "#         )\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='Trans_128', direct_db='BankRecon', nav_table='fund_navreg_axismf',\n",
    "#         scheme_table='fund_master_axismf', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory'\n",
    "#         )\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='Trans_120', direct_db='BankRecon', nav_table='fund_navreg_invesco',\n",
    "#         scheme_table='fund_master_INVESCO', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory'\n",
    "#         )\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='m_Trans_116', direct_db='BankRecon', nav_table='fund_navreg_AXA_29072020',\n",
    "#         scheme_table='Fund_Master_AXA_29072020', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#         )\n",
    "\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='m_Trans_117', direct_db='BankRecon', nav_table='fund_navreg_MIRAE',\n",
    "#         scheme_table='fund_master_MIRAE', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#         )\n",
    "# print ('done')\n",
    "# import time\n",
    "# for i,ele in enumerate(list(daterange('2020-05-01', '2020-06-02'))):\n",
    "#     s = time.time()\n",
    "#     dialy_job(ele, groupby_level='SPFT', table='m_Trans_117', direct_db='BankRecon', nav_table='fund_navreg_MIRAE',\n",
    "#         scheme_table='fund_master_MIRAE', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#         )\n",
    "#     generate_mcr_report(table=f'm_Trans_{code}', groupby_level='SP', start_date = '2020-05-02', end_date = '2020-06-02')\n",
    "\n",
    "#     print (i, ele, time.time() - s)\n",
    "\n",
    "# day_records, combined_records = dialy_job('2020-04-02', groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "#             scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#              category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     59,
     61,
     63,
     65
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 AXA\n",
      "inital file on date 2020-04-01 written\n",
      "initialization time 142.65521883964539\n",
      "dialy file on date 2020-04-02 generated\n",
      "     0 2020-04-02 51.00508499145508\n",
      "dialy file on date 2020-04-03 generated\n",
      "     1 2020-04-03 42.02682065963745\n",
      "dialy file on date 2020-04-04 generated\n",
      "     2 2020-04-04 48.786314964294434\n",
      "dialy file on date 2020-04-05 generated\n",
      "     3 2020-04-05 49.88373780250549\n",
      "dialy file on date 2020-04-06 generated\n",
      "     4 2020-04-06 57.76590895652771\n",
      "dialy file on date 2020-04-07 generated\n",
      "     5 2020-04-07 56.68015456199646\n",
      "dialy file on date 2020-04-08 generated\n",
      "     6 2020-04-08 53.55712080001831\n",
      "dialy file on date 2020-04-09 generated\n",
      "     7 2020-04-09 52.94791889190674\n",
      "dialy file on date 2020-04-10 generated\n",
      "     8 2020-04-10 53.853893518447876\n",
      "dialy file on date 2020-04-11 generated\n",
      "     9 2020-04-11 44.45889067649841\n",
      "dialy file on date 2020-04-12 generated\n",
      "     10 2020-04-12 40.29715323448181\n",
      "dialy file on date 2020-04-13 generated\n",
      "     11 2020-04-13 46.44366526603699\n",
      "dialy file on date 2020-04-14 generated\n",
      "     12 2020-04-14 45.906644344329834\n",
      "dialy file on date 2020-04-15 generated\n",
      "     13 2020-04-15 52.71201682090759\n",
      "dialy file on date 2020-04-16 generated\n",
      "     14 2020-04-16 51.44400882720947\n",
      "dialy file on date 2020-04-17 generated\n",
      "     15 2020-04-17 52.67105269432068\n",
      "dialy file on date 2020-04-18 generated\n",
      "     16 2020-04-18 47.78026509284973\n",
      "dialy file on date 2020-04-19 generated\n",
      "     17 2020-04-19 50.37606358528137\n",
      "dialy file on date 2020-04-20 generated\n",
      "     18 2020-04-20 45.85931897163391\n",
      "dialy file on date 2020-04-21 generated\n",
      "     19 2020-04-21 54.82415699958801\n",
      "dialy file on date 2020-04-22 generated\n",
      "     20 2020-04-22 55.577486991882324\n",
      "dialy file on date 2020-04-23 generated\n",
      "     21 2020-04-23 54.28824257850647\n",
      "dialy file on date 2020-04-24 generated\n",
      "     22 2020-04-24 56.01084923744202\n",
      "dialy file on date 2020-04-25 generated\n",
      "     23 2020-04-25 54.40263509750366\n",
      "dialy file on date 2020-04-26 generated\n",
      "     24 2020-04-26 52.95760798454285\n",
      "dialy file on date 2020-04-27 generated\n",
      "     25 2020-04-27 47.97984457015991\n",
      "dialy file on date 2020-04-28 generated\n",
      "     26 2020-04-28 55.3607497215271\n",
      "dialy file on date 2020-04-29 generated\n",
      "     27 2020-04-29 50.18465614318848\n",
      "dialy file on date 2020-04-30 generated\n",
      "     28 2020-04-30 48.66619062423706\n",
      "dialy file on date 2020-05-01 generated\n",
      "     29 2020-05-01 52.98935842514038\n",
      "dialy file on date 2020-05-02 generated\n",
      "     30 2020-05-02 55.78065037727356\n",
      "dialy file on date 2020-05-03 generated\n",
      "     31 2020-05-03 55.26949596405029\n",
      "dialy file on date 2020-05-04 generated\n",
      "     32 2020-05-04 52.430288791656494\n",
      "dialy file on date 2020-05-05 generated\n",
      "     33 2020-05-05 51.607019901275635\n",
      "dialy file on date 2020-05-06 generated\n",
      "     34 2020-05-06 54.40348196029663\n",
      "dialy file on date 2020-05-07 generated\n",
      "     35 2020-05-07 57.13452672958374\n",
      "dialy file on date 2020-05-08 generated\n",
      "     36 2020-05-08 51.82374620437622\n",
      "dialy file on date 2020-05-09 generated\n",
      "     37 2020-05-09 54.96239733695984\n",
      "dialy file on date 2020-05-10 generated\n",
      "     38 2020-05-10 51.48520302772522\n",
      "dialy file on date 2020-05-11 generated\n",
      "     39 2020-05-11 49.31848621368408\n",
      "dialy file on date 2020-05-12 generated\n",
      "     40 2020-05-12 48.62085676193237\n",
      "dialy file on date 2020-05-13 generated\n",
      "     41 2020-05-13 53.90578269958496\n",
      "dialy file on date 2020-05-14 generated\n",
      "     42 2020-05-14 47.764745473861694\n",
      "dialy file on date 2020-05-15 generated\n",
      "     43 2020-05-15 52.66644477844238\n",
      "dialy file on date 2020-05-16 generated\n",
      "     44 2020-05-16 49.384225606918335\n",
      "dialy file on date 2020-05-17 generated\n",
      "     45 2020-05-17 51.12879180908203\n",
      "dialy file on date 2020-05-18 generated\n",
      "     46 2020-05-18 50.7822482585907\n",
      "dialy file on date 2020-05-19 generated\n",
      "     47 2020-05-19 56.52998185157776\n",
      "dialy file on date 2020-05-20 generated\n",
      "     48 2020-05-20 47.52985167503357\n",
      "dialy file on date 2020-05-21 generated\n",
      "     49 2020-05-21 52.163527727127075\n",
      "dialy file on date 2020-05-22 generated\n",
      "     50 2020-05-22 53.527257680892944\n",
      "dialy file on date 2020-05-23 generated\n",
      "     51 2020-05-23 47.916810750961304\n",
      "dialy file on date 2020-05-24 generated\n",
      "     52 2020-05-24 45.594624757766724\n",
      "dialy file on date 2020-05-25 generated\n",
      "     53 2020-05-25 44.902865409851074\n",
      "dialy file on date 2020-05-26 generated\n",
      "     54 2020-05-26 57.68884992599487\n",
      "dialy file on date 2020-05-27 generated\n",
      "     55 2020-05-27 56.86735725402832\n",
      "dialy file on date 2020-05-28 generated\n",
      "     56 2020-05-28 54.90101337432861\n",
      "dialy file on date 2020-05-29 generated\n",
      "     57 2020-05-29 55.29934287071228\n",
      "dialy file on date 2020-05-30 generated\n",
      "     58 2020-05-30 56.026286602020264\n",
      "dialy file on date 2020-05-31 generated\n",
      "     59 2020-05-31 45.40549302101135\n",
      "dialy file on date 2020-06-01 generated\n",
      "     60 2020-06-01 52.767340660095215\n",
      "dialy file on date 2020-06-02 generated\n",
      "     61 2020-06-02 50.80993914604187\n",
      "dialy file on date 2020-06-03 generated\n",
      "     62 2020-06-03 53.188435554504395\n",
      "dialy file on date 2020-06-04 generated\n",
      "     63 2020-06-04 61.848034620285034\n",
      "dialy file on date 2020-06-05 generated\n",
      "     64 2020-06-05 56.9259877204895\n",
      "dialy file on date 2020-06-06 generated\n",
      "     65 2020-06-06 54.9660849571228\n",
      "dialy file on date 2020-06-07 generated\n",
      "     66 2020-06-07 55.389777421951294\n",
      "dialy file on date 2020-06-08 generated\n",
      "     67 2020-06-08 50.2886598110199\n"
     ]
    }
   ],
   "source": [
    "table_codes = {117: 'MIRAE'}\n",
    "\n",
    "table_codes = {\"116\": \"AXA\",\"117\": \"MIRAE\",\"107\": \"BOB\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codes = {\"117\": \"MIRAE\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codes = {125: 'IBMF', 152: \"ITI\", 123: \"Quantum\", 166: \"Quant\" }\n",
    "\n",
    "table_codes = {116: \"AXA\"}\n",
    "\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPFT'\n",
    "        init_date = '2020-04-01'\n",
    "        start_date = '2020-04-02'\n",
    "        end_date = '2020-08-02'\n",
    "#         mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "        save_metric(init_date, 'records_processed', records, name, groupby_level, table)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange(start_date, end_date))):\n",
    "            try:\n",
    "                spark.catalog.clearCache()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                sc.stop()\n",
    "            except:\n",
    "\n",
    "                print (\"error no sc\")\n",
    "            # intialize spark again\n",
    "            conf = SparkConf()\n",
    "            conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "            #Create spark context and sparksession\n",
    "            sc = SparkContext.getOrCreate(conf=conf)\n",
    "            SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "            spark = SparkSession(sc)\n",
    "            \n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}_{code}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "            save_metric(ele, 'day_records', day_records, name, groupby_level, table)\n",
    "            save_metric(ele, 'combined_records', combined_records, name, groupby_level, table)\n",
    "            save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level, table)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 AXA\n",
      "initialization time 3.814697265625e-06\n",
      "dialy file on date 2020-05-17 generated\n",
      "     0 2020-05-17 72.02393245697021\n",
      "dialy file on date 2020-05-18 generated\n",
      "     1 2020-05-18 51.818278551101685\n",
      "dialy file on date 2020-05-19 generated\n",
      "     2 2020-05-19 57.03696155548096\n",
      "dialy file on date 2020-05-20 generated\n",
      "     3 2020-05-20 53.4127836227417\n",
      "dialy file on date 2020-05-21 generated\n",
      "     4 2020-05-21 46.8679084777832\n",
      "dialy file on date 2020-05-22 generated\n",
      "     5 2020-05-22 44.79817318916321\n",
      "dialy file on date 2020-05-23 generated\n",
      "     6 2020-05-23 44.747467279434204\n",
      "dialy file on date 2020-05-24 generated\n",
      "     7 2020-05-24 51.076550006866455\n",
      "dialy file on date 2020-05-25 generated\n",
      "     8 2020-05-25 58.75477337837219\n",
      "dialy file on date 2020-05-26 generated\n",
      "     9 2020-05-26 55.878639936447144\n",
      "dialy file on date 2020-05-27 generated\n",
      "     10 2020-05-27 58.95805621147156\n",
      "dialy file on date 2020-05-28 generated\n",
      "     11 2020-05-28 57.95145845413208\n",
      "dialy file on date 2020-05-29 generated\n",
      "     12 2020-05-29 54.4383065700531\n",
      "dialy file on date 2020-05-30 generated\n",
      "     13 2020-05-30 58.054969787597656\n",
      "dialy file on date 2020-05-31 generated\n",
      "     14 2020-05-31 55.256507873535156\n",
      "dialy file on date 2020-06-01 generated\n",
      "     15 2020-06-01 57.69703269004822\n",
      "dialy file on date 2020-06-02 generated\n",
      "     16 2020-06-02 60.94796323776245\n",
      "dialy file on date 2020-06-03 generated\n",
      "     17 2020-06-03 57.2261643409729\n",
      "dialy file on date 2020-06-04 generated\n",
      "     18 2020-06-04 57.5409140586853\n",
      "dialy file on date 2020-06-05 generated\n",
      "     19 2020-06-05 58.29798769950867\n",
      "dialy file on date 2020-06-06 generated\n",
      "     20 2020-06-06 56.80175042152405\n",
      "dialy file on date 2020-06-07 generated\n",
      "     21 2020-06-07 57.333433866500854\n",
      "dialy file on date 2020-06-08 generated\n",
      "     22 2020-06-08 59.5993869304657\n",
      "dialy file on date 2020-06-09 generated\n",
      "     23 2020-06-09 59.02782368659973\n",
      "dialy file on date 2020-06-10 generated\n",
      "     24 2020-06-10 59.677327394485474\n",
      "dialy file on date 2020-06-11 generated\n",
      "     25 2020-06-11 60.881574630737305\n",
      "dialy file on date 2020-06-12 generated\n",
      "     26 2020-06-12 58.383941888809204\n",
      "dialy file on date 2020-06-13 generated\n",
      "     27 2020-06-13 57.02717447280884\n",
      "dialy file on date 2020-06-14 generated\n",
      "     28 2020-06-14 58.19751334190369\n",
      "dialy file on date 2020-06-15 generated\n",
      "     29 2020-06-15 60.4430046081543\n",
      "dialy file on date 2020-06-16 generated\n",
      "     30 2020-06-16 61.09448838233948\n",
      "dialy file on date 2020-06-17 generated\n",
      "     31 2020-06-17 59.885937213897705\n",
      "dialy file on date 2020-06-18 generated\n",
      "     32 2020-06-18 57.154924154281616\n",
      "dialy file on date 2020-06-19 generated\n",
      "     33 2020-06-19 60.70571970939636\n",
      "dialy file on date 2020-06-20 generated\n",
      "     34 2020-06-20 60.34740662574768\n",
      "dialy file on date 2020-06-21 generated\n",
      "     35 2020-06-21 58.37192749977112\n",
      "dialy file on date 2020-06-22 generated\n",
      "     36 2020-06-22 61.11232256889343\n",
      "dialy file on date 2020-06-23 generated\n",
      "     37 2020-06-23 59.23706340789795\n",
      "dialy file on date 2020-06-24 generated\n",
      "     38 2020-06-24 63.21398425102234\n",
      "dialy file on date 2020-06-25 generated\n",
      "     39 2020-06-25 55.7721688747406\n",
      "dialy file on date 2020-06-26 generated\n",
      "     40 2020-06-26 60.27089166641235\n",
      "dialy file on date 2020-06-27 generated\n",
      "     41 2020-06-27 58.77382707595825\n",
      "dialy file on date 2020-06-28 generated\n",
      "     42 2020-06-28 62.15694236755371\n",
      "dialy file on date 2020-06-29 generated\n",
      "     43 2020-06-29 61.75360107421875\n",
      "dialy file on date 2020-06-30 generated\n",
      "     44 2020-06-30 59.869102478027344\n",
      "dialy file on date 2020-07-01 generated\n",
      "     45 2020-07-01 60.925227642059326\n",
      "dialy file on date 2020-07-02 generated\n",
      "     46 2020-07-02 62.52224349975586\n",
      "None\n",
      "An error occurred while calling o17910.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor145.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 85 in stage 1142.0 failed 1 times, most recent failure: Lost task 85.0 in stage 1142.0 (TID 29593, 6d47dcb59506, executor driver): java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.finish(LZ4BlockOutputStream.java:263)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:193)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.close(UnsafeRowSerializer.scala:96)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.commitAndGet(DiskBlockObjectWriter.scala:175)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:163)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 32 more\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.finish(LZ4BlockOutputStream.java:263)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:193)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.close(UnsafeRowSerializer.scala:96)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.commitAndGet(DiskBlockObjectWriter.scala:175)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:163)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-7-d50ad688bb5a>\", line 61, in <module>\n",
      "    day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
      "  File \"<timed exec>\", line 253, in dialy_job\n",
      "  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 1027, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 131, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o17910.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor145.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 85 in stage 1142.0 failed 1 times, most recent failure: Lost task 85.0 in stage 1142.0 (TID 29593, 6d47dcb59506, executor driver): java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.finish(LZ4BlockOutputStream.java:263)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:193)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.close(UnsafeRowSerializer.scala:96)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.commitAndGet(DiskBlockObjectWriter.scala:175)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:163)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 32 more\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.finish(LZ4BlockOutputStream.java:263)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:193)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.close(UnsafeRowSerializer.scala:96)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.commitAndGet(DiskBlockObjectWriter.scala:175)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:163)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_codes = {117: 'MIRAE'}\n",
    "\n",
    "table_codes = {\"116\": \"AXA\",\"117\": \"MIRAE\",\"107\": \"BOB\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codes = {\"117\": \"MIRAE\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codes = {125: 'IBMF', 152: \"ITI\", 123: \"Quantum\", 166: \"Quant\" }\n",
    "\n",
    "table_codes = {116: \"AXA\"}\n",
    "\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPFT'\n",
    "        init_date = '2020-04-01'\n",
    "        start_date = '2020-05-17'\n",
    "        end_date = '2020-08-02'\n",
    "#         mcr_month_date = '2020-05-01'\n",
    "\n",
    "#         records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level, table)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange(start_date, end_date))):\n",
    "            try:\n",
    "                spark.catalog.clearCache()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                sc.stop()\n",
    "            except:\n",
    "\n",
    "                print (\"error no sc\")\n",
    "            # intialize spark again\n",
    "            conf = SparkConf()\n",
    "            conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "            #Create spark context and sparksession\n",
    "            sc = SparkContext.getOrCreate(conf=conf)\n",
    "            SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "            spark = SparkSession(sc)           \n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "            save_metric(ele, 'day_records', day_records, name, groupby_level, table)\n",
    "            save_metric(ele, 'combined_records', combined_records, name, groupby_level, table)\n",
    "            save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level, table)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 MIRAE\n",
      "inital file on date 2020-04-01 written\n",
      "initialization time 1056.371550321579\n",
      "dialy file on date 2020-04-02 generated\n",
      "     0 2020-04-02 396.82241320610046\n",
      "dialy file on date 2020-04-03 generated\n",
      "     1 2020-04-03 404.23174500465393\n",
      "dialy file on date 2020-04-04 generated\n",
      "     2 2020-04-04 395.8851306438446\n",
      "dialy file on date 2020-04-05 generated\n",
      "     3 2020-04-05 386.6624491214752\n",
      "dialy file on date 2020-04-06 generated\n",
      "     4 2020-04-06 370.03308272361755\n",
      "dialy file on date 2020-04-07 generated\n",
      "     5 2020-04-07 367.5583963394165\n",
      "dialy file on date 2020-04-08 generated\n",
      "     6 2020-04-08 392.79617738723755\n",
      "dialy file on date 2020-04-09 generated\n",
      "     7 2020-04-09 364.87630248069763\n",
      "dialy file on date 2020-04-10 generated\n",
      "     8 2020-04-10 374.77853870391846\n",
      "dialy file on date 2020-04-11 generated\n",
      "     9 2020-04-11 375.351530790329\n",
      "dialy file on date 2020-04-12 generated\n",
      "     10 2020-04-12 385.92550826072693\n",
      "dialy file on date 2020-04-13 generated\n",
      "     11 2020-04-13 378.7109155654907\n",
      "dialy file on date 2020-04-14 generated\n",
      "     12 2020-04-14 377.4225001335144\n",
      "dialy file on date 2020-04-15 generated\n",
      "     13 2020-04-15 410.13767170906067\n",
      "dialy file on date 2020-04-16 generated\n",
      "     14 2020-04-16 406.90490984916687\n",
      "dialy file on date 2020-04-17 generated\n",
      "     15 2020-04-17 373.90352988243103\n",
      "dialy file on date 2020-04-18 generated\n",
      "     16 2020-04-18 401.01407122612\n",
      "dialy file on date 2020-04-19 generated\n",
      "     17 2020-04-19 391.6396481990814\n",
      "dialy file on date 2020-04-20 generated\n",
      "     18 2020-04-20 381.42080545425415\n",
      "dialy file on date 2020-04-21 generated\n",
      "     19 2020-04-21 419.3285324573517\n",
      "dialy file on date 2020-04-22 generated\n",
      "     20 2020-04-22 444.09981894493103\n",
      "dialy file on date 2020-04-23 generated\n",
      "     21 2020-04-23 456.4450216293335\n",
      "dialy file on date 2020-04-24 generated\n",
      "     22 2020-04-24 424.77515292167664\n",
      "dialy file on date 2020-04-25 generated\n",
      "     23 2020-04-25 417.4860143661499\n",
      "dialy file on date 2020-04-26 generated\n",
      "     24 2020-04-26 408.2150504589081\n",
      "dialy file on date 2020-04-27 generated\n",
      "     25 2020-04-27 419.63519191741943\n",
      "dialy file on date 2020-04-28 generated\n",
      "     26 2020-04-28 439.14655017852783\n",
      "dialy file on date 2020-04-29 generated\n",
      "     27 2020-04-29 429.96007800102234\n",
      "dialy file on date 2020-04-30 generated\n",
      "     28 2020-04-30 459.82894253730774\n",
      "dialy file on date 2020-05-01 generated\n",
      "     29 2020-05-01 432.2781026363373\n",
      "dialy file on date 2020-05-02 generated\n",
      "     30 2020-05-02 454.0637185573578\n",
      "dialy file on date 2020-05-03 generated\n",
      "     31 2020-05-03 474.77581906318665\n",
      "dialy file on date 2020-05-04 generated\n",
      "     32 2020-05-04 320.7504765987396\n",
      "dialy file on date 2020-05-05 generated\n",
      "     33 2020-05-05 330.18865871429443\n",
      "dialy file on date 2020-05-06 generated\n",
      "     34 2020-05-06 326.8796398639679\n",
      "dialy file on date 2020-05-07 generated\n",
      "     35 2020-05-07 323.63676142692566\n",
      "dialy file on date 2020-05-08 generated\n",
      "     36 2020-05-08 327.97113585472107\n",
      "dialy file on date 2020-05-09 generated\n",
      "     37 2020-05-09 322.37167716026306\n",
      "dialy file on date 2020-05-10 generated\n",
      "     38 2020-05-10 322.92790627479553\n",
      "dialy file on date 2020-05-11 generated\n",
      "     39 2020-05-11 321.31567120552063\n",
      "dialy file on date 2020-05-12 generated\n",
      "     40 2020-05-12 342.8367042541504\n",
      "dialy file on date 2020-05-13 generated\n",
      "     41 2020-05-13 329.17677116394043\n",
      "dialy file on date 2020-05-14 generated\n",
      "     42 2020-05-14 327.7821958065033\n",
      "dialy file on date 2020-05-15 generated\n",
      "     43 2020-05-15 325.2447671890259\n",
      "dialy file on date 2020-05-16 generated\n",
      "     44 2020-05-16 335.1557309627533\n",
      "dialy file on date 2020-05-17 generated\n",
      "     45 2020-05-17 323.78603053092957\n",
      "dialy file on date 2020-05-18 generated\n",
      "     46 2020-05-18 327.36168813705444\n",
      "dialy file on date 2020-05-19 generated\n",
      "     47 2020-05-19 329.8829894065857\n",
      "dialy file on date 2020-05-20 generated\n",
      "     48 2020-05-20 326.2998068332672\n",
      "dialy file on date 2020-05-21 generated\n",
      "     49 2020-05-21 331.56375765800476\n",
      "dialy file on date 2020-05-22 generated\n",
      "     50 2020-05-22 338.40880966186523\n",
      "dialy file on date 2020-05-23 generated\n",
      "     51 2020-05-23 329.13641381263733\n",
      "dialy file on date 2020-05-24 generated\n",
      "     52 2020-05-24 326.91056275367737\n",
      "dialy file on date 2020-05-25 generated\n",
      "     53 2020-05-25 327.69987535476685\n",
      "dialy file on date 2020-05-26 generated\n",
      "     54 2020-05-26 324.9957113265991\n",
      "dialy file on date 2020-05-27 generated\n",
      "     55 2020-05-27 331.6766288280487\n",
      "dialy file on date 2020-05-28 generated\n",
      "     56 2020-05-28 331.1264851093292\n",
      "dialy file on date 2020-05-29 generated\n",
      "     57 2020-05-29 335.5385859012604\n",
      "dialy file on date 2020-05-30 generated\n",
      "     58 2020-05-30 329.1005690097809\n",
      "dialy file on date 2020-05-31 generated\n",
      "     59 2020-05-31 326.316015958786\n",
      "dialy file on date 2020-06-01 generated\n",
      "     60 2020-06-01 327.43671703338623\n",
      "dialy file on date 2020-06-02 generated\n",
      "     61 2020-06-02 337.01596117019653\n",
      "dialy file on date 2020-06-03 generated\n",
      "     62 2020-06-03 332.4409956932068\n",
      "dialy file on date 2020-06-04 generated\n",
      "     63 2020-06-04 332.21843957901\n",
      "dialy file on date 2020-06-05 generated\n",
      "     64 2020-06-05 333.26322078704834\n",
      "dialy file on date 2020-06-06 generated\n",
      "     65 2020-06-06 335.0508494377136\n",
      "dialy file on date 2020-06-07 generated\n",
      "     66 2020-06-07 331.019987821579\n",
      "dialy file on date 2020-06-08 generated\n",
      "     67 2020-06-08 342.94785714149475\n",
      "dialy file on date 2020-06-09 generated\n",
      "     68 2020-06-09 442.1325855255127\n",
      "dialy file on date 2020-06-10 generated\n",
      "     69 2020-06-10 465.66503500938416\n",
      "dialy file on date 2020-06-11 generated\n",
      "     70 2020-06-11 481.45730543136597\n",
      "dialy file on date 2020-06-12 generated\n",
      "     71 2020-06-12 463.29528617858887\n",
      "dialy file on date 2020-06-13 generated\n",
      "     72 2020-06-13 484.5711576938629\n",
      "dialy file on date 2020-06-14 generated\n",
      "     73 2020-06-14 491.1719603538513\n",
      "dialy file on date 2020-06-15 generated\n",
      "     74 2020-06-15 502.911678314209\n",
      "dialy file on date 2020-06-16 generated\n",
      "     75 2020-06-16 536.4547412395477\n",
      "dialy file on date 2020-06-17 generated\n",
      "     76 2020-06-17 509.61751985549927\n",
      "dialy file on date 2020-06-18 generated\n",
      "     77 2020-06-18 524.8398010730743\n",
      "dialy file on date 2020-06-19 generated\n",
      "     78 2020-06-19 522.5297100543976\n",
      "dialy file on date 2020-06-20 generated\n",
      "     79 2020-06-20 515.265768289566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 82, in <module>\n",
      "    day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
      "  File \"<timed exec>\", line 253, in dialy_job\n",
      "  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 1027, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 131, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o45460.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor507.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 151 in stage 15.0 failed 1 times, most recent failure: Lost task 151.0 in stage 15.0 (TID 575, f37d609d14e7, executor driver): java.io.FileNotFoundException: /tmp/blockmgr-a119c073-ccfd-4965-8a6d-a5d305296859/05/temp_shuffle_858f9c77-68ef-4364-8d29-5e005df98686 (No space left on device)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:105)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:118)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:245)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 32 more\n",
      "Caused by: java.io.FileNotFoundException: /tmp/blockmgr-a119c073-ccfd-4965-8a6d-a5d305296859/05/temp_shuffle_858f9c77-68ef-4364-8d29-5e005df98686 (No space left on device)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:105)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:118)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:245)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "An error occurred while calling o45460.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor507.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 151 in stage 15.0 failed 1 times, most recent failure: Lost task 151.0 in stage 15.0 (TID 575, f37d609d14e7, executor driver): java.io.FileNotFoundException: /tmp/blockmgr-a119c073-ccfd-4965-8a6d-a5d305296859/05/temp_shuffle_858f9c77-68ef-4364-8d29-5e005df98686 (No space left on device)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:105)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:118)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:245)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 32 more\n",
      "Caused by: java.io.FileNotFoundException: /tmp/blockmgr-a119c073-ccfd-4965-8a6d-a5d305296859/05/temp_shuffle_858f9c77-68ef-4364-8d29-5e005df98686 (No space left on device)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:105)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:118)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:245)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n",
      "107 BOB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 55, in <module>\n",
      "    records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
      "  File \"<timed exec>\", line 171, in initialize\n",
      "  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 1027, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 131, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o45660.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor507.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, f37d609d14e7, executor driver): java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:249)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 32 more\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:249)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "An error occurred while calling o45660.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor507.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, f37d609d14e7, executor driver): java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:249)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 32 more\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:249)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 INVESCO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 55, in <module>\n",
      "    records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
      "  File \"<timed exec>\", line 171, in initialize\n",
      "  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 1027, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 131, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o45860.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor507.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, f37d609d14e7, executor driver): java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:249)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 32 more\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:249)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "An error occurred while calling o45860.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor507.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, f37d609d14e7, executor driver): java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:249)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 32 more\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:249)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMF Reliance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 55, in <module>\n",
      "    records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
      "  File \"<timed exec>\", line 171, in initialize\n",
      "  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 1027, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 131, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o46060.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor507.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, f37d609d14e7, executor driver): java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:260)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:494)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:221)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.createWithExistingInMemorySorter(UnsafeExternalSorter.java:111)\n",
      "\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:158)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:248)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 32 more\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:260)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:494)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:221)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.createWithExistingInMemorySorter(UnsafeExternalSorter.java:111)\n",
      "\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:158)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:248)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "An error occurred while calling o46060.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor507.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, f37d609d14e7, executor driver): java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:260)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:494)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:221)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.createWithExistingInMemorySorter(UnsafeExternalSorter.java:111)\n",
      "\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:158)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:248)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 32 more\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:260)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:494)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:221)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.createWithExistingInMemorySorter(UnsafeExternalSorter.java:111)\n",
      "\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:158)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:248)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 Edelweiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 55, in <module>\n",
      "    records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
      "  File \"<timed exec>\", line 171, in initialize\n",
      "  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 1027, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 131, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o46260.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor507.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 177 in stage 3.0 failed 1 times, most recent failure: Lost task 177.0 in stage 3.0 (TID 180, f37d609d14e7, executor driver): java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.finish(LZ4BlockOutputStream.java:263)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:193)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.close(UnsafeRowSerializer.scala:96)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.commitAndGet(DiskBlockObjectWriter.scala:175)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:163)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 32 more\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.finish(LZ4BlockOutputStream.java:263)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:193)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.close(UnsafeRowSerializer.scala:96)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.commitAndGet(DiskBlockObjectWriter.scala:175)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:163)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "An error occurred while calling o46260.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor507.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 177 in stage 3.0 failed 1 times, most recent failure: Lost task 177.0 in stage 3.0 (TID 180, f37d609d14e7, executor driver): java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.finish(LZ4BlockOutputStream.java:263)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:193)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.close(UnsafeRowSerializer.scala:96)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.commitAndGet(DiskBlockObjectWriter.scala:175)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:163)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 32 more\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.finish(LZ4BlockOutputStream.java:263)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:193)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.close(UnsafeRowSerializer.scala:96)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.commitAndGet(DiskBlockObjectWriter.scala:175)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:163)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 IDBIMF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 55, in <module>\n",
      "    records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
      "  File \"<timed exec>\", line 171, in initialize\n",
      "  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 1027, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 131, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o46460.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor507.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, f37d609d14e7, executor driver): java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:249)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 32 more\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:249)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "An error occurred while calling o46460.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor507.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, f37d609d14e7, executor driver): java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:249)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 32 more\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:249)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125 IBMF\n",
      "inital file on date 2020-04-01 written\n",
      "initialization time 83.91613006591797\n",
      "dialy file on date 2020-04-02 generated\n",
      "     0 2020-04-02 43.684441804885864\n",
      "dialy file on date 2020-04-03 generated\n",
      "     1 2020-04-03 38.5097279548645\n",
      "dialy file on date 2020-04-04 generated\n",
      "     2 2020-04-04 38.944226026535034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 82, in <module>\n",
      "    day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
      "  File \"<timed exec>\", line 253, in dialy_job\n",
      "  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 1027, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 131, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o48280.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor507.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 14.0 failed 1 times, most recent failure: Lost task 4.0 in stage 14.0 (TID 417, f37d609d14e7, executor driver): java.io.IOException: fail to rename file /tmp/blockmgr-307c66ce-5428-43ad-9f02-0c9f646181bd/22/shuffle_4_417_0.index.6e896e41-5293-4c1c-b959-fb493d65623a to /tmp/blockmgr-307c66ce-5428-43ad-9f02-0c9f646181bd/22/shuffle_4_417_0.index\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.writeIndexFileAndCommit(IndexShuffleBlockResolver.scala:204)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.commitAllPartitions(LocalDiskShuffleMapOutputWriter.java:115)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedData(BypassMergeSortShuffleWriter.java:222)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:167)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 32 more\n",
      "Caused by: java.io.IOException: fail to rename file /tmp/blockmgr-307c66ce-5428-43ad-9f02-0c9f646181bd/22/shuffle_4_417_0.index.6e896e41-5293-4c1c-b959-fb493d65623a to /tmp/blockmgr-307c66ce-5428-43ad-9f02-0c9f646181bd/22/shuffle_4_417_0.index\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.writeIndexFileAndCommit(IndexShuffleBlockResolver.scala:204)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.commitAllPartitions(LocalDiskShuffleMapOutputWriter.java:115)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedData(BypassMergeSortShuffleWriter.java:222)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:167)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "An error occurred while calling o48280.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor507.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 14.0 failed 1 times, most recent failure: Lost task 4.0 in stage 14.0 (TID 417, f37d609d14e7, executor driver): java.io.IOException: fail to rename file /tmp/blockmgr-307c66ce-5428-43ad-9f02-0c9f646181bd/22/shuffle_4_417_0.index.6e896e41-5293-4c1c-b959-fb493d65623a to /tmp/blockmgr-307c66ce-5428-43ad-9f02-0c9f646181bd/22/shuffle_4_417_0.index\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.writeIndexFileAndCommit(IndexShuffleBlockResolver.scala:204)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.commitAllPartitions(LocalDiskShuffleMapOutputWriter.java:115)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedData(BypassMergeSortShuffleWriter.java:222)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:167)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 32 more\n",
      "Caused by: java.io.IOException: fail to rename file /tmp/blockmgr-307c66ce-5428-43ad-9f02-0c9f646181bd/22/shuffle_4_417_0.index.6e896e41-5293-4c1c-b959-fb493d65623a to /tmp/blockmgr-307c66ce-5428-43ad-9f02-0c9f646181bd/22/shuffle_4_417_0.index\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.writeIndexFileAndCommit(IndexShuffleBlockResolver.scala:204)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.commitAllPartitions(LocalDiskShuffleMapOutputWriter.java:115)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedData(BypassMergeSortShuffleWriter.java:222)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:167)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n",
      "128 AXISMF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 55, in <module>\n",
      "    records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
      "  File \"<timed exec>\", line 171, in initialize\n",
      "  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 1027, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 131, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o48480.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor507.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, f37d609d14e7, executor driver): java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:260)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:494)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:221)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.createWithExistingInMemorySorter(UnsafeExternalSorter.java:111)\n",
      "\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:158)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:248)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 32 more\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:260)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:494)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:221)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.createWithExistingInMemorySorter(UnsafeExternalSorter.java:111)\n",
      "\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:158)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:248)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "An error occurred while calling o48480.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:934)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor507.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, f37d609d14e7, executor driver): java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:260)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:494)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:221)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.createWithExistingInMemorySorter(UnsafeExternalSorter.java:111)\n",
      "\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:158)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:248)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 32 more\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:260)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:494)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:221)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.createWithExistingInMemorySorter(UnsafeExternalSorter.java:111)\n",
      "\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:158)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:248)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1207, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1033, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1211, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 39, in <module>\n",
      "    sc = SparkContext.getOrCreate(conf=conf)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 371, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 130, in __init__\n",
      "    self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 193, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 310, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1568, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 131, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1207, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1033, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1211, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 35, in <module>\n",
      "    conf = SparkConf()\n",
      "  File \"/usr/local/spark/python/pyspark/conf.py\", line 116, in __init__\n",
      "    self._jconf = _jvm.SparkConf(loadDefaults)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1692, in __getattr__\n",
      "    answer = self._gateway_client.send_command(\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1031, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 979, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 985, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1127, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 35, in <module>\n",
      "    conf = SparkConf()\n",
      "  File \"/usr/local/spark/python/pyspark/conf.py\", line 116, in __init__\n",
      "    self._jconf = _jvm.SparkConf(loadDefaults)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1692, in __getattr__\n",
      "    answer = self._gateway_client.send_command(\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1031, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 979, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 985, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1127, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 35, in <module>\n",
      "    conf = SparkConf()\n",
      "  File \"/usr/local/spark/python/pyspark/conf.py\", line 116, in __init__\n",
      "    self._jconf = _jvm.SparkConf(loadDefaults)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1692, in __getattr__\n",
      "    answer = self._gateway_client.send_command(\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1031, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 979, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 985, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1127, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 35, in <module>\n",
      "    conf = SparkConf()\n",
      "  File \"/usr/local/spark/python/pyspark/conf.py\", line 116, in __init__\n",
      "    self._jconf = _jvm.SparkConf(loadDefaults)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1692, in __getattr__\n",
      "    answer = self._gateway_client.send_command(\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1031, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 979, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 985, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1127, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 35, in <module>\n",
      "    conf = SparkConf()\n",
      "  File \"/usr/local/spark/python/pyspark/conf.py\", line 116, in __init__\n",
      "    self._jconf = _jvm.SparkConf(loadDefaults)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1692, in __getattr__\n",
      "    answer = self._gateway_client.send_command(\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1031, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 979, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 985, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1127, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 35, in <module>\n",
      "    conf = SparkConf()\n",
      "  File \"/usr/local/spark/python/pyspark/conf.py\", line 116, in __init__\n",
      "    self._jconf = _jvm.SparkConf(loadDefaults)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1692, in __getattr__\n",
      "    answer = self._gateway_client.send_command(\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1031, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 979, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 985, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1127, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 35, in <module>\n",
      "    conf = SparkConf()\n",
      "  File \"/usr/local/spark/python/pyspark/conf.py\", line 116, in __init__\n",
      "    self._jconf = _jvm.SparkConf(loadDefaults)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1692, in __getattr__\n",
      "    answer = self._gateway_client.send_command(\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1031, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 979, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 985, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1127, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 35, in <module>\n",
      "    conf = SparkConf()\n",
      "  File \"/usr/local/spark/python/pyspark/conf.py\", line 116, in __init__\n",
      "    self._jconf = _jvm.SparkConf(loadDefaults)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1692, in __getattr__\n",
      "    answer = self._gateway_client.send_command(\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1031, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 979, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 985, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1127, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 35, in <module>\n",
      "    conf = SparkConf()\n",
      "  File \"/usr/local/spark/python/pyspark/conf.py\", line 116, in __init__\n",
      "    self._jconf = _jvm.SparkConf(loadDefaults)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1692, in __getattr__\n",
      "    answer = self._gateway_client.send_command(\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1031, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 979, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 985, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1127, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 35, in <module>\n",
      "    conf = SparkConf()\n",
      "  File \"/usr/local/spark/python/pyspark/conf.py\", line 116, in __init__\n",
      "    self._jconf = _jvm.SparkConf(loadDefaults)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1692, in __getattr__\n",
      "    answer = self._gateway_client.send_command(\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1031, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 979, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 985, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1127, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 35, in <module>\n",
      "    conf = SparkConf()\n",
      "  File \"/usr/local/spark/python/pyspark/conf.py\", line 116, in __init__\n",
      "    self._jconf = _jvm.SparkConf(loadDefaults)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1692, in __getattr__\n",
      "    answer = self._gateway_client.send_command(\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1031, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 979, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 985, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1127, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 35, in <module>\n",
      "    conf = SparkConf()\n",
      "  File \"/usr/local/spark/python/pyspark/conf.py\", line 116, in __init__\n",
      "    self._jconf = _jvm.SparkConf(loadDefaults)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1692, in __getattr__\n",
      "    answer = self._gateway_client.send_command(\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1031, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 979, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 985, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1127, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "None\n",
      "An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "None\n",
      "An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "None\n",
      "An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "None\n",
      "An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "None\n",
      "An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "None\n",
      "An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "None\n",
      "An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "None\n",
      "An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "None\n",
      "An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "None\n",
      "An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "None\n",
      "An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "None\n",
      "An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n",
      "None\n",
      "An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-4c1886c8b4b9>\", line 35, in <module>\n",
      "    conf = SparkConf()\n",
      "  File \"/usr/local/spark/python/pyspark/conf.py\", line 116, in __init__\n",
      "    self._jconf = _jvm.SparkConf(loadDefaults)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1692, in __getattr__\n",
      "    answer = self._gateway_client.send_command(\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1031, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 979, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 985, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1127, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:33493)\n"
     ]
    }
   ],
   "source": [
    "table_codes = {117: 'MIRAE'}\n",
    "\n",
    "table_codes = {\"116\": \"AXA\",\"117\": \"MIRAE\",\"107\": \"BOB\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codes = {\"117\": \"MIRAE\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codes = {125: 'IBMF', 152: \"ITI\", 123: \"Quantum\", 166: \"Quant\" }\n",
    "\n",
    "table_codes = {\"117\": \"MIRAE\",\"107\": \"BOB\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPFT'\n",
    "        init_date = '2020-04-01'\n",
    "        start_date = '2020-04-02'\n",
    "        end_date = '2020-08-02'\n",
    "#         mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "        save_metric(init_date, 'records_processed', records, name, groupby_level, table)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange(start_date, end_date))):\n",
    "            try:\n",
    "                spark.catalog.clearCache()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                sc.stop()\n",
    "            except:\n",
    "\n",
    "                print (\"error no sc\")\n",
    "            # intialize spark again\n",
    "            conf = SparkConf()\n",
    "            conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "            #Create spark context and sparksession\n",
    "            sc = SparkContext.getOrCreate(conf=conf)\n",
    "            SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "            spark = SparkSession(sc)           \n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "            save_metric(ele, 'day_records', day_records, name, groupby_level, table)\n",
    "            save_metric(ele, 'combined_records', combined_records, name, groupby_level, table)\n",
    "            save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level, table)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 AXA\n",
      "inital file on date 2020-03-31 written\n",
      "initialization time 72.35419249534607\n",
      "dialy file on date 2020-04-01 generated\n",
      "     0 2020-04-01 39.198323249816895\n",
      "job time is 39.19968247413635\n",
      "overall time is 111.5541021823883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "table_codes = {117: 'MIRAE'}\n",
    "\n",
    "table_codes = {\"116\": \"AXA\",\"117\": \"MIRAE\",\"107\": \"BOB\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codes = {\"117\": \"MIRAE\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codes = {125: 'IBMF', 152: \"ITI\", 123: \"Quantum\", 166: \"Quant\" }\n",
    "\n",
    "table_codes = {116: \"AXA\"}\n",
    "\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPFT'\n",
    "        init_date = '2020-03-31'\n",
    "        start_date = '2020-04-01'\n",
    "        end_date = '2020-04-02'\n",
    "#         mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "        #save_metric(init_date, 'records_processed', records, name, groupby_level, table)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange(start_date, end_date))):\n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "            #save_metric(ele, 'day_records', day_records, name, groupby_level, table)\n",
    "            #save_metric(ele, 'combined_records', combined_records, name, groupby_level, table)\n",
    "            #save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level, table)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-25T08:32:00.266Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "init_date = '2020-04-01'\n",
    "groupby_level = 'SPFT'\n",
    "table = 'trans116'\n",
    "direct_db = 'kfintech_funds'\n",
    "nav_table = 'nav_master'\n",
    "scheme_table = 'scheme_master'\n",
    "\n",
    "init_date = '2020-04-01'\n",
    "groupby_level = 'SPFT'\n",
    "table = 'trans116'\n",
    "direct_db = 'kfintech_funds'\n",
    "nav_table = 'nav_master'\n",
    "scheme_table = 'scheme_master'\n",
    "\n",
    "initialize(init_date, table=table,direct_db=direct_db, groupby_level=groupby_level)\n",
    "# init_date = '2020-04-30'\n",
    "# initialize(init_date, table=table,direct_db=direct_db, groupby_level=groupby_level)\n",
    "\n",
    "\n",
    "for ele in daterange('2020-04-02', '2020-04-03'):\n",
    "    dialy_job(ele, groupby_level=groupby_level, table=table, direct_db=direct_db, nav_table=nav_table,\n",
    "                scheme_table=scheme_table, scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "                 category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:51.310674Z",
     "start_time": "2020-08-25T08:31:26.686999Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+---------------+--------------------+-------------------+------------+--------------------+-------------------+\n",
      "|fm_NewMCRId|count(SchemeCode)|count(PlanCode)|            sum(aum)|            avg_aum|count(Folio)|         sum(inflow)|       sum(outflow)|\n",
      "+-----------+-----------------+---------------+--------------------+-------------------+------------+--------------------+-------------------+\n",
      "|        AI3|                1|              7|2.3167262810653696E9|2.650473045073036E9|        3501|2.6207103040275544E8|5.559765378526582E8|\n",
      "|     Others|                1|              5|                 0.0|                0.0|        null|                 0.0|                0.0|\n",
      "+-----------+-----------------+---------------+--------------------+-------------------+------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_data(start_date, end_date, groupby_level='SPT', table='m_Trans_116'):\n",
    "    \n",
    "    final_data = None\n",
    "\n",
    "    dates_list = list(daterange(start_date, end_date))\n",
    "    for date in dates_list:\n",
    "        date_obj = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "        day_num = date_obj.day\n",
    "        latest_data = spark.read.parquet(f\"{table}_dialy/data_{groupby_level}_{date}.parquet\")\n",
    "        if final_data:\n",
    "            final_data = final_data.union(latest_data)\n",
    "        else:\n",
    "            final_data = latest_data\n",
    "    return final_data\n",
    "\n",
    "def generate_mcr_report(table='m_Trans_116', ignored_tr_types = ['CNI', 'CNO', 'TRMI', 'TRMO', \n",
    "                                                                 'TRFI', 'TRFO', 'PLDO', 'UPLO', 'DMT',\n",
    "                                                                 'RMT', 'CNIR', 'CNOR', 'TRMIR', 'TRMOR',\n",
    "                                                                 'TRFIR', 'TRFOR', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR'],\n",
    "        liquid_fund_tr_types = ['NEW', 'ADD', 'IPO', 'SIN', 'NEWR', 'ADDR', 'IPOR', 'SINR'],\n",
    "                       start_date = '2020-05-02', end_date = '2020-06-02', groupby_level='SPT',\n",
    "                        transaction_type='TransactionType',folio='Folio',folio_ignore_types = ['PLDO', 'UPLO', 'DMT', 'RMT', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR'], \n",
    "                      fn_nav = 'fn_nav', newmcrid='fm_NewMCRId', today_pu = 'today_pu', today_ru = 'today_ru', scheme='SchemeCode', aum='aum', plan='PlanCode'):\n",
    "    \n",
    "    till_but_one_day_data = None\n",
    "    inflow = 'inflow'\n",
    "    outflow = 'outflow'\n",
    "    calculated_date = 'calculated_date'\n",
    "    batch_close_date = 'batch_close_date'\n",
    "    balance_pu = 'balance_pu'\n",
    "    balance_ru = 'balance_ru'\n",
    "    balance_units = 'balance_units'\n",
    "    \n",
    "    \n",
    "    dates_list = list(daterange(start_date, end_date))\n",
    "    \n",
    "    for date in dates_list[:-1]:\n",
    "        date_obj = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "        day_num = date_obj.day\n",
    "        latest_data = spark.read.parquet(f\"{table}_dialy/data_{groupby_level}_{date}.parquet\")\n",
    "#         print (ele, latest_data.count())\n",
    "        if till_but_one_day_data:\n",
    "            till_but_one_day_data = till_but_one_day_data.union(latest_data)\n",
    "        else:\n",
    "            till_but_one_day_data = latest_data\n",
    "            \n",
    "    \n",
    "    date_obj = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    final_day = date_obj + datetime.timedelta(1)\n",
    "    final_day_str = final_day.strftime('%Y-%m-%d')\n",
    "    final_day_data = spark.read.parquet(f\"{table}_dialy/data_{groupby_level}_{final_day_str}.parquet\")\n",
    "    last_but_one_day_data = latest_data\n",
    "\n",
    "    till_but_one_day_data = till_but_one_day_data.filter( ~(col(transaction_type).isin(ignored_tr_types)) ).fillna({today_pu: 0, today_ru: 0, aum: 0, newmcrid: 'Others'})\n",
    "    final_day_data = final_day_data.filter( ~(col(transaction_type).isin(ignored_tr_types)) ).fillna({today_pu: 0, today_ru: 0, aum: 0, newmcrid: 'Others'})\n",
    "    \n",
    "    \n",
    "    all_data = till_but_one_day_data.union(final_day_data)\n",
    "    sp_inf_ouf_data = all_data\n",
    "\n",
    "    liquid_condition = ( (col(newmcrid) == 'A1b') & (col(calculated_date) == final_day_str) & (col(batch_close_date) == final_day_str) & (col(transaction_type).isin(liquid_fund_tr_types)) )\n",
    "    \n",
    "    final_day_data = final_day_data.withColumn(balance_pu,    when(liquid_condition, col(balance_pu) - col(today_pu)).otherwise(col(balance_pu)))\n",
    "    final_day_data = final_day_data.withColumn(balance_ru,    when(liquid_condition, col(balance_ru) - col(today_ru)).otherwise(col(balance_ru)))\n",
    "    final_day_data = final_day_data.withColumn(balance_units, when(liquid_condition, col(balance_pu) - col(balance_ru)).otherwise(col(balance_units)) )\n",
    "    final_day_data = final_day_data.withColumn(aum,           when(liquid_condition,  col(balance_units) * col(fn_nav)).otherwise(col(aum))    )\n",
    "\n",
    "    net_aum = final_day_data.groupby([newmcrid]).agg(sum(aum))\n",
    "    avg_data = till_but_one_day_data.union(final_day_data)\n",
    "#     avg_data = all_data\n",
    "    \n",
    "    # inflow, outflow logic change\n",
    "    sp_data = avg_data\n",
    "    # sp_inf_ouf_data = sp_inf_ouf_data.withColumn(inflow, (col(today_pu)*col(fn_nav)))\n",
    "    # sp_inf_ouf_data = sp_inf_ouf_data.withColumn(outflow, (col(today_ru)*col(fn_nav)))\n",
    "    sp = sp_data.groupby([newmcrid]).agg(countDistinct(scheme),countDistinct(plan))\n",
    "    \n",
    "#     inf_ouf_data = get_data(datetime.datetime(date_obj.year, 4, 2).strftime('%Y-%m-%d'), start_date, groupby_level, table).union(sp_data)\n",
    "#     inf_ouf_data = inf_ouf_data.withColumn(inflow, (col(today_pu)*col(fn_nav)))\n",
    "#     inf_ouf_data = inf_ouf_data.withColumn(outflow, (col(today_ru)*col(fn_nav)))\n",
    "#     inf_ouf_data = inf_ouf_data.groupby([newmcrid]).agg(sum(col(inflow)),sum(col(outflow)))\n",
    "    \n",
    "#     spinout = sp.join(inf_ouf_data, on=[newmcrid], how='left')\n",
    "#     spinout.show()\n",
    "    spinout = sp\n",
    "    # sp_inf_ouf_data = sp_inf_ouf_data.withColumn(inflow, (col(today_pu)*col(fn_nav)))\n",
    "    # sp_inf_ouf_data = sp_inf_ouf_data.withColumn(outflow, (col(today_ru)*col(fn_nav)))\n",
    "\n",
    "    folio_count = avg_data.groupby(folio, scheme, plan, newmcrid).agg(sum('aum')).filter(col('sum(aum)') - 0 > 0.1).groupby(newmcrid).agg(countDistinct(folio))\n",
    "\n",
    "    avg_aum = avg_data.groupby([newmcrid]).agg(sum(aum))\n",
    "    avg_aum = avg_aum.withColumn('avg_aum', col(f'sum({aum})')/(len(list(daterange(start_date, end_date))))).drop(f'sum({aum})')\n",
    "    \n",
    "\n",
    "    mcr_net_aum = spinout.join(net_aum, on=[newmcrid], how='left')\n",
    "    mcr = mcr_net_aum.join(avg_aum, on=[newmcrid], how='left')\n",
    "    mcr = mcr.join(folio_count, on=[newmcrid], how='left')\n",
    "    \n",
    "    # inflow outflow\n",
    "    all_data = get_data('2020-04-02', end_date, groupby_level=groupby_level, table=table)\n",
    "    all_data = all_data.filter(~liquid_condition)\n",
    "    all_data = all_data.fillna({today_pu: 0, today_ru: 0, aum: 0, newmcrid: 'Others', inflow:0, outflow:0})\n",
    "#     all_data = all_data.filter( ~(col(transaction_type).isin(ignored_tr_types)) ).fillna({today_pu: 0, today_ru: 0, aum: 0, newmcrid: 'Others', inflow:0, outflow:0})\n",
    "    inout = all_data.groupby([newmcrid]).agg(sum(inflow), sum(outflow))\n",
    "    mcr = mcr.join(inout, on=[newmcrid], how='left')\n",
    "    \n",
    "#     inout.show(1000)\n",
    "    mcr.show(1000)\n",
    "    \n",
    "    mcr.coalesce(1).write.csv(f\"{table}_mcr/mcr_{groupby_level}_{final_day_str}.csv\",header=True, mode='overwrite')\n",
    "#     mcr.coalesce(1).write.parquet(f\"{table}_mcr/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    \n",
    "    return mcr,avg_data, sp_inf_ouf_data, all_data\n",
    " \n",
    "    \n",
    "table = 'trans116'\n",
    "table = 'm_Trans_116'\n",
    "groupby_level = 'SPFT'\n",
    "start_date = '2020-04-02'\n",
    "end_date = '2020-05-02'\n",
    "mcr, _,_,_ = generate_mcr_report(table=table, groupby_level=groupby_level, start_date=start_date, end_date=end_date)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:05:13.238626Z",
     "start_time": "2020-08-19T07:05:11.045764Z"
    },
    "code_folding": [
     9
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 MIRAE\n",
      "None\n",
      "An error occurred while calling o61.load.\n",
      ": com.microsoft.sqlserver.jdbc.SQLServerException: Cannot open database \"BankRecon\" requested by the login. The login failed. ClientConnectionId:093c1d7b-ed3f-47d9-9375-df4d3977ca15\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:262)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:258)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:104)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:5036)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:3668)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.access$000(SQLServerConnection.java:94)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:3627)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7194)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:2935)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2456)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2103)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:1950)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1162)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:735)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:339)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:203)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-7-ae5149c1f2c6>\", line 35, in <module>\n",
      "    records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
      "  File \"<timed exec>\", line 32, in initialize\n",
      "  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 184, in load\n",
      "    return self._df(self._jreader.load())\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 131, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o61.load.\n",
      ": com.microsoft.sqlserver.jdbc.SQLServerException: Cannot open database \"BankRecon\" requested by the login. The login failed. ClientConnectionId:093c1d7b-ed3f-47d9-9375-df4d3977ca15\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:262)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:258)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:104)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:5036)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:3668)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.access$000(SQLServerConnection.java:94)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:3627)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7194)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:2935)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2456)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2103)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:1950)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1162)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:735)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:339)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:203)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_codes = {117: 'MIRAE'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPT'\n",
    "        init_date = '2020-06-30'\n",
    "        mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange('2020-07-01', '2020-08-02'))):\n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "#             save_metric(ele, 'day_records', day_records, name, groupby_level)\n",
    "#             save_metric(ele, 'combined_records', combined_records, name, groupby_level)\n",
    "#             save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:05:13.924841Z",
     "start_time": "2020-08-19T07:05:13.433907Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMF Reliance\n",
      "None\n",
      "An error occurred while calling o101.load.\n",
      ": com.microsoft.sqlserver.jdbc.SQLServerException: Cannot open database \"BankRecon\" requested by the login. The login failed. ClientConnectionId:21ea2dce-924a-45bd-ae0b-951a417920a0\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:262)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:258)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:104)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:5036)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:3668)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.access$000(SQLServerConnection.java:94)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:3627)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7194)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:2935)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2456)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2103)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:1950)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1162)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:735)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:339)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:203)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-8-651d4dd48d65>\", line 35, in <module>\n",
      "    records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
      "  File \"<timed exec>\", line 32, in initialize\n",
      "  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 184, in load\n",
      "    return self._df(self._jreader.load())\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 131, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o101.load.\n",
      ": com.microsoft.sqlserver.jdbc.SQLServerException: Cannot open database \"BankRecon\" requested by the login. The login failed. ClientConnectionId:21ea2dce-924a-45bd-ae0b-951a417920a0\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:262)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:258)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:104)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:5036)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:3668)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.access$000(SQLServerConnection.java:94)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:3627)\n",
      "\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7194)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:2935)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2456)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2103)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:1950)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1162)\n",
      "\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:735)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:339)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:203)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_codes = {'RMF': 'Reliance'}\n",
    "\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPT'\n",
    "        init_date = '2020-06-30'\n",
    "        mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange('2020-07-01', '2020-08-02'))):\n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "#             save_metric(ele, 'day_records', day_records, name, groupby_level)\n",
    "#             save_metric(ele, 'combined_records', combined_records, name, groupby_level)\n",
    "#             save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T09:10:22.530312Z",
     "start_time": "2020-08-19T09:10:22.524239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-04-02'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "# datetime.datetime.year('2020-08-04')\n",
    "\n",
    "date_obj = datetime.datetime.strptime('2020-08-05', '%Y-%m-%d')\n",
    "date_obj.year\n",
    "\n",
    "datetime.datetime(date_obj.year, 4, 2).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T17:20:38.744555Z",
     "start_time": "2020-08-19T17:20:38.732643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'116': 'AXA',\n",
       " '117': 'MIRAE',\n",
       " '107': 'BOB',\n",
       " '120': 'INVESCO',\n",
       " 'RMF': 'Reliance',\n",
       " '118': 'Edelweiss',\n",
       " '135': 'IDBIMF',\n",
       " '125': 'IBMF',\n",
       " '128': 'AXISMF',\n",
       " '178': 'BNPMF',\n",
       " '152': 'ITI',\n",
       " '105': 'JMMF',\n",
       " '103': 'PMF',\n",
       " '166': 'Quant',\n",
       " '130': 'PeerlessMF',\n",
       " '104': 'TAURUS',\n",
       " '108': 'UTI',\n",
       " '123': 'Quantum',\n",
       " '127': 'MOTILAL',\n",
       " '102': 'LIC',\n",
       " '176': 'SundaramMF',\n",
       " '101': 'canrobeco',\n",
       " '129': 'DLFPramerica'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"116\": \"AXA\",\n",
    "\"117\": \"MIRAE\",\n",
    "\"107\": \"BOB\",\n",
    "\"120\": \"INVESCO\",\n",
    "\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\n",
    "\"135\": \"IDBIMF\",\n",
    "\"125\": \"IBMF\",\n",
    "\"128\": \"AXISMF\",\n",
    "\"178\": \"BNPMF\",\n",
    "\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\n",
    "\"103\": \"PMF\",\n",
    "\"166\": \"Quant\",\n",
    "\"130\": \"PeerlessMF\",\n",
    "\"104\": \"TAURUS\",\n",
    "\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\n",
    "\"127\": \"MOTILAL\",\n",
    "\"102\": \"LIC\",\n",
    "\"176\": \"SundaramMF\",\n",
    "\"101\": \"canrobeco\",\n",
    "\"129\": \"DLFPramerica\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#### all the script exaaecution for all the funds\n",
    "table_codes = {\n",
    "               102:'LIC',103:'pmf',104:'tarus',105:'JMMF',107:'BOB',108:'uti',\n",
    "               116:'AXA',117:'mirae',118:'edelwwise',120:'invesco',123:'quantum'\n",
    "               ,125:'IBMF',127:'motilal',128:'axismf',130:'peerless',135:'IDBIMF',152:'ITI',166:'quant',\n",
    "               176:'sundaram',178:'BNPMF', 'RMF':'reliance'}\n",
    "table_codes = {104:'taurus',105:'JMMF',107:'BOB',108:'uti',\n",
    "               116:'AXA',117:'mirae',118:'edelwwise',120:'invesco',123:'quantum'\n",
    "               ,125:'IBMF',127:'motilal',128:'axismf',130:'peerless',135:'IDBIMF',152:'ITI',166:'quant',\n",
    "               176:'sundaram',178:'BNPMF', 'RMF':'reliance'}\n",
    "\n",
    "table_codes = {101:'canrobeco',\n",
    "               102:'LIC',103:'pmf',104:'tarus',105:'JMMF',107:'BOB',108:'uti',\n",
    "               116:'AXA',117:'mirae',118:'edelwwise',120:'invesco',123:'quantum'\n",
    "               ,125:'IBMF',127:'motilal',128:'axismf',129:'pgim',130:'peerless',135:'IDBIMF',152:'ITI',166:'quant'\n",
    "               ,178:'BNPMF', 'RMF':'reliance', '129': 'dlfpramerica', 118:'edelweiss', 130:'peerlessMF', '176':'sundaramMF'}\n",
    "table_codes = {116: 'AXA'}\n",
    "\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPFT'\n",
    "        init_date = '2020-04-30'\n",
    "        mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange('2020-05-01', '2020-06-02'))):\n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "            save_metric(ele, 'day_records', day_records, name, groupby_level)\n",
    "            save_metric(ele, 'combined_records', combined_records, name, groupby_level)\n",
    "            save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#### generate mcr for august for all the funds\n",
    "# table_codes = { '129': 'dlfpramerica', 118:'edelweiss', 130:'peerlessMF', '176':'sundaramMF'}\n",
    "\n",
    "table_codes = {101:'canrobeco',\n",
    "               102:'LIC',103:'pmf',104:'tarus',105:'JMMF',107:'BOB',108:'uti',\n",
    "               116:'AXA',117:'mirae',118:'edelwwise',120:'invesco',123:'quantum'\n",
    "               ,125:'IBMF',127:'motilal',128:'axismf',129:'pgim',130:'peerless',135:'IDBIMF',152:'ITI',166:'quant'\n",
    "               ,178:'BNPMF', 'RMF':'reliance', '129': 'dlfpramerica', 118:'edelweiss', 130:'peerlessMF', '176':'sundaramMF'}\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPT'\n",
    "        init_date = '2020-07-01'\n",
    "        mcr_month_date = '2020-07-01'\n",
    "\n",
    "#         records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level)\n",
    "\n",
    "#         print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "#         job_start = time.time()\n",
    "\n",
    "\n",
    "#         for i,ele in enumerate(list(daterange('2020-07-02', '2020-08-02'))):\n",
    "#             s = time.time()\n",
    "#             day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "#             scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#              category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#             )\n",
    "#             save_metric(ele, 'day_records', day_records, name, groupby_level)\n",
    "#             save_metric(ele, 'combined_records', combined_records, name, groupby_level)\n",
    "#             save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level)\n",
    "#             print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "#         s = time.time() \n",
    "        generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-07-02', end_date = '2020-08-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "#         print (f'job time is {time.time() - job_start}')\n",
    "#         print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
