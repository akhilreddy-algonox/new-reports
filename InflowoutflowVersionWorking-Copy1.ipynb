{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:12.676366Z",
     "start_time": "2020-08-25T08:31:12.527984Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# db_utils\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import sqlalchemy\n",
    "import os\n",
    "import pyodbc\n",
    "import sys\n",
    "# import pymssql\n",
    "import numpy as np\n",
    "\n",
    "from MySQLdb._exceptions import OperationalError\n",
    "from sqlalchemy import create_engine, exc,event\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "from time import time\n",
    "\n",
    "#connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=192.168.15.126;UID=BRS;PWD=Fint$123;Trusted Connection=yes;DATABASE=\"\n",
    "connection_string = None\n",
    "import logging\n",
    "   \n",
    "# logging = Logging()\n",
    "\n",
    "class DB(object):\n",
    "    def __init__(self, database, host='127.0.0.1', user='root', password='', port='3306', tenant_id=None):\n",
    "        \"\"\"\n",
    "        Initialization of databse object.\n",
    "\n",
    "        Args:\n",
    "            databse (str): The database to connect to.\n",
    "            host (str): Host IP address. For dockerized app, it is the name of\n",
    "                the service set in the compose file.\n",
    "            user (str): Username of MySQL server. (default = 'root')\n",
    "            password (str): Password of MySQL server. For dockerized app, the\n",
    "                password is set in the compose file. (default = '')\n",
    "            port (str): Port number for MySQL. For dockerized app, the port that\n",
    "                is mapped in the compose file. (default = '3306')\n",
    "        \"\"\"\n",
    "\n",
    "        if host in [\"common_db\",\"extraction_db\", \"queue_db\", \"template_db\", \"table_db\", \"stats_db\", \"business_rules_db\", \"reports_db\"]:\n",
    "            self.HOST = os.environ['HOST_IP']\n",
    "            self.USER = os.environ['LOCAL_DB_USER']\n",
    "            self.PASSWORD = os.environ['LOCAL_DB_PASSWORD']\n",
    "            self.PORT = os.environ['LOCAL_DB_PORT']\n",
    "            self.DATABASE = f'{tenant_id}_{database}' if tenant_id is not None and tenant_id else database\n",
    "        else:\n",
    "            self.HOST = os.environ['HOST_IP']\n",
    "            self.USER = os.environ['LOCAL_DB_USER']\n",
    "            self.PASSWORD = os.environ['LOCAL_DB_PASSWORD']\n",
    "            self.PORT = os.environ['LOCAL_DB_PORT']\n",
    "            self.DATABASE = f'{tenant_id}_{database}' if tenant_id is not None and tenant_id else database\n",
    "       \n",
    "        logging.info(f'Host: {self.HOST}')\n",
    "        logging.info(f'User: {self.USER}')\n",
    "        logging.info(f'Password: {self.PASSWORD}')\n",
    "        logging.info(f'Port: {self.PORT}')\n",
    "        logging.info(f'Database: {self.DATABASE}')\n",
    "        # self.connect()\n",
    "    def connect(self, max_retry=5):\n",
    "#         retry = 1\n",
    "\n",
    "#         try:\n",
    "#             start = time()\n",
    "#             logging.debug(f'Making connection to {self.DATABASE}...')\n",
    "#             config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{self.DATABASE}?charset=utf8'\n",
    "#             self.db_ = create_engine(config, connect_args={'connect_timeout': 2}, pool_recycle=300)\n",
    "#             logging.info(f'Engine created for {self.DATABASE}')\n",
    "#             while retry <= max_retry:\n",
    "#                 try:\n",
    "#                     self.engine = self.db_.connect()\n",
    "#                     logging.info(f'Connection established succesfully to {self.DATABASE}! ({round(time() - start, 2)} secs to connect)')\n",
    "#                     break\n",
    "#                 except Exception as e:\n",
    "#                     logging.warning(f'Connection failed. Retrying... ({retry}) [{e}]')\n",
    "#                     retry += 1\n",
    "#                     self.db_.dispose()\n",
    "#         except:\n",
    "#             logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "#             return\n",
    "        data = []\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        print(inds)\n",
    "        for pos, ind in enumerate(inds):\n",
    "            if pos % 2 == 0:\n",
    "                sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "            else:\n",
    "                sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "        if connection_string:\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + self.DATABASE)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pyodbc.connect('DRIVER={' + driver + '};SERVER=' + host_ + ';DATABASE=' + database+ ';UID=' + user_ + ';PWD=' + password_ + ';Trusted Connection=yes;')\n",
    "                else:\n",
    "                    conn = pyodbc.connect('DRIVER={' + driver + '};SERVER=' + host_ + ';DATABASE=' + database + ';Trusted Connection=yes;')\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pyodbc.connect('DRIVER={' + driver + '};SERVER=' + host_ + ';DATABASE=' + database + ';Trusted Connection=yes;')\n",
    "\n",
    "    def convert_to_mssql(self, query):\n",
    "        inds = [i for i in range(len(query)) if query[i] == '`']\n",
    "        for pos, ind in enumerate(inds):\n",
    "            if pos % 2 == 0:\n",
    "                query = query[:ind] + '[' + query[ind+1:]\n",
    "            else:\n",
    "                query = query[:ind] + ']' + query[ind + 1:]\n",
    "       \n",
    "        query = query.replace('%s', '?')\n",
    "\n",
    "        return query\n",
    "\n",
    "    def execute(self, query, database=None, index_col='id', **kwargs):\n",
    "        logging.debug(f'Before converting: {query}')\n",
    "        query = self.convert_to_mssql(query)\n",
    "        logging.debug(f'After converting: {query}')\n",
    "\n",
    "        logging.debug('Connecting to DB')\n",
    "        conn = pyodbc.connect(f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={self.HOST};UID={self.USER};PWD={self.PASSWORD};Trusted Connection=yes;DATABASE={self.DATABASE}', as_dict=True)\n",
    "        logging.debug(f'Connection established with {self.DATABASE}. [{conn}]')\n",
    "        curs = conn.cursor()\n",
    "        logging.debug(f'Cursor object created. [{curs}]')\n",
    "        params = tuple(kwargs.get('params', []))\n",
    "       \n",
    "        logging.debug(f'Params: {params}')\n",
    "        logging.debug(f'Params Type: {type(params)}')\n",
    "        params = [int(i) if isinstance(i, np.int64) else i for i in params]\n",
    "        curs.execute(query, params)\n",
    "        logging.debug(f'Query executed.')\n",
    "       \n",
    "        data = None\n",
    "\n",
    "        try:\n",
    "            logging.debug(f'Fetching all data.')\n",
    "            data = curs.fetchall()\n",
    "            # logging.debug(f'Data fetched: {data}')\n",
    "            columns = [column[0] for column in curs.description]\n",
    "            logging.debug(f'Columns: {columns}')\n",
    "            result = []\n",
    "            for row in data:\n",
    "                result.append(dict(zip(columns, row)))\n",
    "            # logging.debug(f'Zipped result: {result}')\n",
    "            if result:\n",
    "                data = pd.DataFrame(result)\n",
    "            else:\n",
    "                data = pd.DataFrame(columns=columns)\n",
    "            # logging.debug(f'Data to DF: {data}')\n",
    "        except:\n",
    "            logging.debug('Update Query')\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            logging.debug(f'Data is not a DataFrame. Returning True. [{type(data)}]')\n",
    "            return True\n",
    "       \n",
    "        try:\n",
    "            if index_col is not None:\n",
    "                logging.debug(f'Setting ID as index')\n",
    "                return data.where((pd.notnull(data)), None).set_index('id')\n",
    "            else:\n",
    "                return data.where((pd.notnull(data)), None)\n",
    "        except:\n",
    "            logging.exception(f'Failed to set ID as index')\n",
    "            return data.where((pd.notnull(data)), None)\n",
    "\n",
    "    def execute__(self, query, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Executes an SQL query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that needs to be executed.\n",
    "            database (str): Name of the database to execute the query in. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            params (list/tuple/dict): List of parameters to pass to in the query.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data from the executed\n",
    "            query. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        data = None\n",
    "\n",
    "#         # Use new database if a new databse is given\n",
    "#         if database is not None:\n",
    "#             try:\n",
    "#                 config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "#                 engine = create_engine(config, pool_recycle=300)\n",
    "#             except:\n",
    "#                 logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "#                 return False\n",
    "#         else:\n",
    "#             engine = self.engine\n",
    "       \n",
    "        print('query', query)\n",
    "        if database is None:\n",
    "            database = 'karvy'\n",
    "        data = None\n",
    "        sql = query\n",
    "        user_ = self.USER\n",
    "        host_ = self.HOST\n",
    "        database = self.DATABASE\n",
    "        password_ = self.PASSWORD\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        for pos, ind in enumerate(inds):\n",
    "            if pos % 2 == 0:\n",
    "                sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "            else:\n",
    "                sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "               \n",
    "        if connection_string:\n",
    "            print('connection string', connection_string)\n",
    "            print('database', database)\n",
    "            print(type(connection_string + database))\n",
    "            print(type(connection_string + database))\n",
    "\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + database)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pymssql.connect(host=host_,database=database,user=user_,password=password_)\n",
    "                else:\n",
    "                    conn = pymssql.connect(host=host_,database=database)\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pymssql.connect(host=host_,database=database)\n",
    "        try:\n",
    "            logging.debug(f'Query: {query}')\n",
    "            logging.debug(f'Query: {query}')\n",
    "            curs = conn.cursor(as_dict = True)\n",
    "            params = kwargs.get('params', [])\n",
    "            logging.debug(f'Params: {params}')\n",
    "            curs.execute(sql, tuple(params))\n",
    "            print('query executed')\n",
    "            try:\n",
    "                data = curs.fetchall()\n",
    "                data = pd.DataFrame(data)\n",
    "                # print(data)\n",
    "            except Exception as e:\n",
    "                logging.debug('Update Query')\n",
    "            # data = pd.read_sql(sql, conn, index_col='id', **kwargs)\n",
    "        except exc.ResourceClosedError:\n",
    "            logging.warning('Query does not have any value to return.')\n",
    "            return True\n",
    "        except (exc.StatementError, OperationalError) as e:\n",
    "            logging.warning(f'Creating new connection. Engine/Connection is probably None. [{e}]')\n",
    "            self.connect()\n",
    "            data = pd.read_sql(query, self.engine, index_col='id', **kwargs)\n",
    "        except:\n",
    "            logging.exception('Something went wrong executing query. Check trace.')\n",
    "            params = kwargs['params'] if 'params' in kwargs else None\n",
    "            conn.rollback()\n",
    "            return False\n",
    "        conn.close()\n",
    "        return data.where((pd.notnull(data)), None).set_index('id')\n",
    "\n",
    "    def execute_(self, query, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Executes an SQL query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that needs to be executed.\n",
    "            database (str): Name of the database to execute the query in. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            params (list/tuple/dict): List of parameters to pass to in the query.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data from the executed\n",
    "            query. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        logging.debug(f'Executing `execute` instead of `execute_`')\n",
    "        return self.execute(query, index_col=None, **kwargs)\n",
    "       \n",
    "        data = None\n",
    "\n",
    "#         # Use new database if a new database is given\n",
    "#         if database is not None:\n",
    "#             try:\n",
    "#                 config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "#                 engine = create_engine(config, pool_recycle=300)\n",
    "#             except:\n",
    "#                 logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "#                 return False\n",
    "#         else:\n",
    "#             engine = self.engine\n",
    "\n",
    "#         try:\n",
    "#             data = pd.read_sql(query, engine, **kwargs)\n",
    "#         except exc.ResourceClosedError:\n",
    "#             return True\n",
    "#         except:\n",
    "#             logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "#             params = kwargs['params'] if 'params' in kwargs else None\n",
    "#             return False\n",
    "        print('query', query)\n",
    "        if database is None:\n",
    "            database = 'karvy'\n",
    "        data = None\n",
    "        sql = query\n",
    "        user_ = self.USER\n",
    "        database = self.DATABASE\n",
    "        host_ = self.HOST\n",
    "        password_ = self.PASSWORD\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        # for pos, ind in enumerate(inds):\n",
    "        #     if pos % 2 == 0:\n",
    "        #         sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "        #     else:\n",
    "        #         sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "               \n",
    "        if connection_string:\n",
    "            print('connection string', connection_string)\n",
    "            print('database', database)\n",
    "            print(type(connection_string + database))\n",
    "            print(type(connection_string + database))\n",
    "\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + database)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pymssql.connect(host=host_,database=database,user=user_,password=password_)\n",
    "                else:\n",
    "                    conn = pymssql.connect(host=host_,database=database)\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pymssql.connect(host=host_,database=database)\n",
    "        try:\n",
    "            logging.debug(f'Query: {query}')\n",
    "            curs = conn.cursor(as_dict = True)\n",
    "            params = kwargs.get('params', [])\n",
    "            logging.debug(f'Params: {params}')\n",
    "            curs.execute(sql, params)\n",
    "            print('query executed')\n",
    "            try:\n",
    "                data = curs.fetchall()\n",
    "                data = pd.DataFrame(data)\n",
    "                print(data)\n",
    "            except Exception as e:\n",
    "                logging.debug('Update Query')\n",
    "\n",
    "            #data = pd.read_sql(sql, conn,**kwargs)\n",
    "        except exc.ResourceClosedError:\n",
    "            logging.warning('Query does not have any value to return.')\n",
    "            return True\n",
    "        except (exc.StatementError, OperationalError) as e:\n",
    "            logging.warning(f'Creating new connection. Engine/Connection is probably None. [{e}]')\n",
    "            self.connect()\n",
    "            data = pd.read_sql(query, conn,**kwargs)\n",
    "        except:\n",
    "            logging.exception('Something went wrong executing query. Check trace.')\n",
    "            params = kwargs['params'] if 'params' in kwargs else None\n",
    "            conn.rollback()\n",
    "            return False\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "#         return data.where((pd.notnull(data)), None)\n",
    "        try:\n",
    "            return data.replace({pd.np.nan: None}).set_index('id')\n",
    "        except AttributeError as e:\n",
    "            return True\n",
    "\n",
    "    def insert(self, data, table, **kwargs):\n",
    "        \"\"\"\n",
    "        Write records stored in a DataFrame to a SQL database.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The DataFrame that needs to be write to SQL database.\n",
    "            table (str): The table in which the rcords should be written to.\n",
    "            kwargs: Keyword arguments for pandas to_sql function.\n",
    "                See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html\n",
    "                to know the arguments that can be passed.\n",
    "\n",
    "        Returns:\n",
    "            (bool) True is succesfully inserted, else false.\n",
    "        \"\"\"\n",
    "        logging.info(f'Inserting into `{table}`')\n",
    "       \n",
    "        conn = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={self.HOST};UID={self.USER};PWD={self.PASSWORD};Trusted Connection=yes;DATABASE={self.DATABASE}'\n",
    "\n",
    "       \n",
    "#         conn =  \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=IP_ADDRESS;DATABASE=DataLake;UID=USER;PWD=PASS\"\n",
    "        quoted = quote_plus(conn)\n",
    "        new_con = 'mssql+pyodbc:///?odbc_connect={}'.format(quoted)\n",
    "        self.engine = create_engine(new_con)\n",
    "#         print(self.engine)\n",
    "        try:\n",
    "            data.to_sql(table, self.engine,chunksize = None, **kwargs)\n",
    "            try:\n",
    "                self.execute(f'ALTER TABLE `{table}` ADD PRIMARY KEY (`id`);')\n",
    "            except:\n",
    "                pass\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception('Something went wrong inserting. Check trace.')\n",
    "            return False\n",
    "\n",
    "   \n",
    "   \n",
    "    def insert_(self, data, table, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Write records stored in a DataFrame to a SQL database.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The DataFrame that needs to be write to SQL database.\n",
    "            table (str): The table in which the rcords should be written to.\n",
    "            database (str): The database the table lies in. Leave it none if you\n",
    "                want use database during object creation.\n",
    "            kwargs: Keyword arguments for pandas to_sql function.\n",
    "                See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html\n",
    "                to know the arguments that can be passed.\n",
    "\n",
    "        Returns:\n",
    "            (bool) True is succesfully inserted, else false.\n",
    "        \"\"\"\n",
    "        logging.info(f'Inserting into {table}')\n",
    "\n",
    "        # # Use new database if a new databse is given\n",
    "        # if database is not None:\n",
    "        #     try:\n",
    "        #         config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "        #         engine = create_engine(config, pool_recycle=300)\n",
    "        #     except:\n",
    "        #         logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "        #         return False\n",
    "        # else:\n",
    "        #     engine = self.engine\n",
    "\n",
    "        if database is None:\n",
    "            database = 'karvy'\n",
    "        data = None\n",
    "        sql = query\n",
    "        user_ = self.USER\n",
    "        database = self.DATABASE\n",
    "        host_ = self.HOST\n",
    "        password_ = self.PASSWORD\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        # for pos, ind in enumerate(inds):\n",
    "        #     if pos % 2 == 0:\n",
    "        #         sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "        #     else:\n",
    "        #         sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "               \n",
    "        if connection_string:\n",
    "            print('connection string', connection_string)\n",
    "            print('database', database)\n",
    "            print(type(connection_string + database))\n",
    "            print(type(connection_string + database))\n",
    "\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + database)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pymssql.connect(host=host_,database=database,user=user_,password=password_)\n",
    "                else:\n",
    "                    conn = pymssql.connect(host=host_,database=database)\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pymssql.connect(host=host_,database=database)\n",
    "        try:\n",
    "            logging.debug(f'Query: {query}')\n",
    "            # data.to_sql(table, conn, **kwargs)\n",
    "            curs = conn.cursor(as_dict = True)\n",
    "            curs.execute(sql, tuple(kwargs.get('params', [])))\n",
    "            print('query executed')\n",
    "            try:\n",
    "                data = curs.fetchall()\n",
    "                data = pd.DataFrame(data)\n",
    "                print(data)\n",
    "            except Exception as e:\n",
    "                logging.debug('Update Query')\n",
    "\n",
    "            try:\n",
    "                self.execute(f'ALTER TABLE {table} ADD PRIMARY KEY (id);')\n",
    "            except:\n",
    "                pass\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception('Something went wrong inserting. Check trace.')\n",
    "            return False\n",
    "\n",
    "    def insert_dict(self, data, table):\n",
    "        \"\"\"\n",
    "        Insert dictionary into a SQL database table.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The DataFrame that needs to be write to SQL database.\n",
    "            table (str): The table in which the rcords should be written to.\n",
    "\n",
    "        Returns:\n",
    "            (bool) True is succesfully inserted, else false.\n",
    "        \"\"\"\n",
    "        logging.info(f'Inserting dictionary data into {table}...')\n",
    "        logging.debug(f'Data:\\n{data}')\n",
    "\n",
    "        try:\n",
    "            column_names = []\n",
    "            params = []\n",
    "\n",
    "            for column_name, value in data.items():\n",
    "                column_names.append(f'{column_name}')\n",
    "                params.append(value)\n",
    "\n",
    "            logging.debug(f'Column names: {column_names}')\n",
    "            logging.debug(f'Params: {params}')\n",
    "\n",
    "            columns_string = ', '.join(column_names)\n",
    "            param_placeholders = ', '.join(['%s'] * len(column_names))\n",
    "\n",
    "            query = f'INSERT INTO {table} ({columns_string}) VALUES ({param_placeholders})'\n",
    "\n",
    "            return self.execute(query, params=params)\n",
    "        except:\n",
    "            logging.exception('Error inserting data.')\n",
    "            return False\n",
    "\n",
    "    def update(self, table, update=None, where=None, database=None, force_update=False):\n",
    "        # Use new database if a new databse is given\n",
    "        # if database is not None:\n",
    "        #     try:\n",
    "        #         config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "        #         self.engine = create_engine(config, pool_recycle=300)\n",
    "        #     except:\n",
    "        #         logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "        #         return False\n",
    "\n",
    "        logging.info(f'Updating table: {table}')\n",
    "        logging.info(f'Update data: {update}')\n",
    "        logging.info(f'Where clause data: {where}')\n",
    "        logging.info(f'Force update flag: {force_update}')\n",
    "\n",
    "        try:\n",
    "            set_clause = []\n",
    "            set_value_list = []\n",
    "            where_clause = []\n",
    "            where_value_list = []\n",
    "\n",
    "            if where is not None and where:\n",
    "                for set_column, set_value in update.items():\n",
    "                    set_clause.append(f'{set_column}=%s')\n",
    "                    set_value_list.append(set_value)\n",
    "                set_clause_string = ', '.join(set_clause)\n",
    "            else:\n",
    "                logging.error(f'Update dictionary is None/empty. Must have some update clause.')\n",
    "                return False\n",
    "\n",
    "            if where is not None and where:\n",
    "                for where_column, where_value in where.items():\n",
    "                    where_clause.append(f'{where_column}=%s')\n",
    "                    where_value_list.append(where_value)\n",
    "                where_clause_string = ' AND '.join(where_clause)\n",
    "                query = f'UPDATE {table} SET {set_clause_string} WHERE {where_clause_string}'\n",
    "            else:\n",
    "                if force_update:\n",
    "                    query = f'UPDATE {table} SET {set_clause_string}'\n",
    "                else:\n",
    "                    message = 'Where dictionary is None/empty. If you want to force update every row, pass force_update as True.'\n",
    "                    logging.error(message)\n",
    "                    return False\n",
    "\n",
    "            params = set_value_list + where_value_list\n",
    "            self.execute(query, params=params)\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception('Something went wrong updating. Check trace.')\n",
    "            return False\n",
    "\n",
    "    def get_column_names(self, table, database=None):\n",
    "        \"\"\"\n",
    "        Get all column names from an SQL table.\n",
    "\n",
    "        Args:\n",
    "            table (str): Name of the table from which column names should be extracted.\n",
    "            database (str): Name of the database in which the table lies. Leave\n",
    "                it none if you want use database during object creation.\n",
    "\n",
    "        Returns:\n",
    "            (list) List of headers. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(f'Getting column names of table {table}')\n",
    "            return list(self.execute(f'SELECT * FROM {table}', database))\n",
    "        except:\n",
    "            logging.exception('Something went wrong getting column names. Check trace.')\n",
    "            return\n",
    "\n",
    "    def execute_default_index(self, query, database=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Executes an SQL query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that needs to be executed.\n",
    "            database (str): Name of the database to execute the query in. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            params (list/tuple/dict): List of parameters to pass to in the query.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data from the executed\n",
    "            query. (None if an error occurs)\n",
    "        \"\"\"\n",
    "\n",
    "        logging.debug(f'Executing `execute` instead of `execute_default_index`')\n",
    "        return self.execute(query, index_col=None, **kwargs)\n",
    "        data = None\n",
    "\n",
    "        # # Use new database if a new databse is given\n",
    "        # if database is not None:\n",
    "        #     try:\n",
    "        #         config = f'mysql://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{database}?charset=utf8'\n",
    "        #         engine = create_engine(config, pool_recycle=300)\n",
    "        #     except:\n",
    "        #         logging.exception(f'Something went wrong while connecting. Check trace.')\n",
    "        #         return False\n",
    "        # else:\n",
    "        #     engine = self.engine\n",
    "\n",
    "        print('query', query)\n",
    "        if database is None:\n",
    "            database = 'karvy'\n",
    "        data = None\n",
    "        sql = query\n",
    "        user_ = self.USER\n",
    "        host_ = self.HOST\n",
    "        database = self.DATABASE\n",
    "        password_ = self.PASSWORD\n",
    "        inds = [i for i in range(len(sql)) if sql[i] == '']\n",
    "        for pos, ind in enumerate(inds):\n",
    "            if pos % 2 == 0:\n",
    "                sql = sql[:ind] + '[' + sql[ind+1:]\n",
    "            else:\n",
    "                sql = sql[:ind] + ']' + sql[ind + 1:]\n",
    "               \n",
    "        if connection_string:\n",
    "            print('connection string', connection_string)\n",
    "            print('database', database)\n",
    "            print(type(connection_string + database))\n",
    "            print(type(connection_string + database))\n",
    "\n",
    "            try:\n",
    "                conn = pyodbc.connect(connection_string + database)\n",
    "            except Exception as e:\n",
    "                print('Connection string invalid. ', e)\n",
    "        else:\n",
    "            try:\n",
    "                if user_ or password_:\n",
    "                    conn = pymssql.connect(host=host_,database=database,user=user_,password=password_)\n",
    "                else:\n",
    "                    conn = pymssql.connect(host=host_,database=database)\n",
    "            except Exception as e:\n",
    "                print(\"Error establishing connection to DB. \", e)\n",
    "                conn = pymssql.connect(host=host_,database=database)\n",
    "\n",
    "        try:\n",
    "            logging.debug(f'Query: {query}')\n",
    "            # data.to_sql(table, conn, **kwargs)\n",
    "            curs = conn.cursor(as_dict = True)\n",
    "           \n",
    "            curs.execute(sql, tuple(kwargs.get('params', [])))\n",
    "            print('query executed')\n",
    "            try:\n",
    "                data = curs.fetchall()\n",
    "                data = pd.DataFrame(data)\n",
    "                print(data)\n",
    "            except Exception as e:\n",
    "                logging.debug('Update Query')\n",
    "            # data = pd.read_sql(query, conn, **kwargs)\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "        except exc.ResourceClosedError:\n",
    "            return True\n",
    "        except:\n",
    "            logging.exception(f'Something went wrong while executing query. Check trace.')\n",
    "            params = kwargs['params'] if 'params' in kwargs else None\n",
    "            return False\n",
    "\n",
    "        return data.where((pd.notnull(data)), None).set_index('id')\n",
    "\n",
    "\n",
    "    def get_all(self, table, database=None, discard=None):\n",
    "        \"\"\"\n",
    "        Get all data from an SQL table.\n",
    "\n",
    "        Args:\n",
    "            table (str): Name of the table from which data should be extracted.\n",
    "            database (str): Name of the database in which the table lies. Leave\n",
    "                it none if you want use database during object creation.\n",
    "            discard (list): columns to be excluded while selecting all\n",
    "        Returns:\n",
    "            (DataFrame) A pandas dataframe containing the data. (None if an error\n",
    "            occurs)\n",
    "        \"\"\"\n",
    "        logging.info(f'Getting all data from {table}')\n",
    "        if discard:\n",
    "            logging.info(f'Discarding columns {discard}')\n",
    "            columns = list(self.execute_default_index(f'SHOW COLUMNS FROM {table}',database).Field)\n",
    "            columns = [col for col in columns if col not in discard]\n",
    "            columns_str = json.dumps(columns).replace(\"'\",'').replace('\"','')[1:-1]\n",
    "            return self.execute(f'SELECT {columns_str} FROM {table}', database)\n",
    "\n",
    "        return self.execute(f'SELECT * FROM {table}', database)\n",
    "\n",
    "    def get_latest(self, data, group_by_col, sort_col):\n",
    "        \"\"\"\n",
    "        Group data by a column containing repeated values and get latest from it by\n",
    "        taking the latest value based on another column.\n",
    "\n",
    "        Example:\n",
    "        Get the latest products\n",
    "            id     product   date\n",
    "            220    6647     2014-09-01\n",
    "            220    6647     2014-10-16\n",
    "            826    3380     2014-11-11\n",
    "            826    3380     2015-05-19\n",
    "            901    4555     2014-09-01\n",
    "            901    4555     2014-11-01\n",
    "\n",
    "        The function will return\n",
    "            id     product   date\n",
    "            220    6647     2014-10-16\n",
    "            826    3380     2015-05-19\n",
    "            901    4555     2014-11-01\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): Pandas DataFrame to query on.\n",
    "            group_by_col (str): Column containing repeated values.\n",
    "            sort_col (str): Column to identify the latest record.\n",
    "\n",
    "        Returns:\n",
    "            (DataFrame) Contains the latest records. (None if an error occurs)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info('Grouping data...')\n",
    "            logging.info(f'Data: {data}')\n",
    "            logging.info(f'Group by column: {group_by_col}')\n",
    "            logging.info(f'Sort column: {sort_col}')\n",
    "            return data.sort_values(sort_col).set_index('id').groupby(group_by_col).tail(1)\n",
    "        except KeyError as e:\n",
    "            logging.error(f'Column {e.args[0]} does not exist.')\n",
    "            return None\n",
    "        except:\n",
    "            logging.exception('Something went wrong while grouping data.')\n",
    "            return None\n",
    "\n",
    "db_config = {\n",
    "   'host': '13.233.100.20',\n",
    "   'port': '1433',\n",
    "   'user': 'SA',\n",
    "   'password':'Akhil@Akhil1'\n",
    "}\n",
    "import os\n",
    "os.environ['HOST_IP'] = '13.233.100.20'\n",
    "os.environ['LOCAL_DB_USER']='SA'\n",
    "os.environ['LOCAL_DB_PASSWORD'] = 'Akhil@Akhil1'\n",
    "os.environ['LOCAL_DB_PORT'] = '1433'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:12.910490Z",
     "start_time": "2020-08-25T08:31:12.898964Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def debug_df(df, num=20):\n",
    "    df.printSchema()\n",
    "    df.show(num)\n",
    "    \n",
    "\n",
    "def decrease_date(s, days):\n",
    "    date = datetime.datetime.strptime(s, \"%Y-%m-%d\")\n",
    "    modified_date = date - datetime.timedelta(days=days)\n",
    "    return datetime.datetime.strftime(modified_date, \"%Y-%m-%d\")\n",
    "\n",
    "def read_df(table, columns_to_retrieve, database):\n",
    "    \n",
    "    query = f\"SELECT {','.join(columns_to_retrieve)} from {table}\"\n",
    "    # logging.info(f\"query to be executed is {query}\")\n",
    "    \n",
    "    data = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "            .option(\"query\", query) \\\n",
    "            .option(\"user\", user) \\\n",
    "            .option(\"password\", password) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .load()\n",
    "    return data\n",
    "\n",
    "from datetime import timedelta, date\n",
    "#\n",
    "def daterange(start_date, end_date):\n",
    "    start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield datetime.datetime.strftime(start_date + timedelta(n), \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:13.385413Z",
     "start_time": "2020-08-25T08:31:13.347638Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "# required libraries\n",
    "from pyspark import SparkContext, SparkConf #\n",
    "from pyspark.sql import SparkSession # for dataframe conversions\n",
    "# for type conversions\n",
    "from datetime import datetime\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, udf, sum # col, udf (user defined functions)\n",
    "from pyspark.sql.types import DateType, IntegerType # type\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import trim # for trimming\n",
    "from pyspark.sql.functions import collect_list, sort_array, row_number # for grouping and taking the last/first element\n",
    "from pyspark.sql.functions import *\n",
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import datetime \n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "# intialize spark\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 2*multiprocessing.cpu_count())\n",
    " \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "def populate_db_func(purred: pd.Series, units: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(pd.Series(np.where(purred=='P', units, 0)))\n",
    "\n",
    "def populate_cr_func(purred: pd.Series, units: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(pd.Series(np.where(purred=='R', units, 0)))\n",
    "\n",
    "populate_db = pandas_udf(populate_db_func, returnType=FloatType())\n",
    "populate_cr = pandas_udf(populate_cr_func, returnType=FloatType())    \n",
    "    \n",
    "#Create spark context and sparksession\n",
    "\n",
    "SparkContext.setSystemProperty(\"spark.driver.memory\", \"60g\")\n",
    "SparkContext.setSystemProperty(\"spark.executor.memory\", \"60g\")\n",
    "SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"200g\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "(2*multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:13.732129Z",
     "start_time": "2020-08-25T08:31:13.722136Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# configs\n",
    "import os\n",
    "os.environ['HOST_IP'] = '192.168.15.126'\n",
    "os.environ['LOCAL_DB_USER'] = 'BRS'\n",
    "os.environ['LOCAL_DB_PASSWORD'] = 'Kfintech123$'\n",
    "os.environ['LOCAL_DB_PORT'] = '1433'\n",
    "\n",
    "import os\n",
    "\n",
    "# comment when using the configs from env file\n",
    "default_ip = '13.233.100.20'\n",
    "default_user  = 'SA'\n",
    "default_password = 'Akhil@Akhil1'\n",
    "default_port = '1433'\n",
    "default_tenant_id = 'karvy'\n",
    "\n",
    "# # initializations \n",
    "server = os.environ.get('HOST_IP', default_ip)\n",
    "port = os.environ.get('LOCAL_DB_PORT', default_port)\n",
    "user = os.environ.get('LOCAL_DB_USER', default_user)\n",
    "password = os.environ.get('LOCAL_DB_PASSWORD', default_password)\n",
    "\n",
    "db_config = {\n",
    "   'host': server,\n",
    "   'port': port,\n",
    "   'user': user,\n",
    "   'password':password\n",
    "}\n",
    "\n",
    "\n",
    "def save_metric(date, metric_name, metric_value, fund_name, group_level, table_name, database='IB_Comp_funds'):\n",
    "    db = DB(database, tenant_id='',**db_config)\n",
    "    try:\n",
    "        query = f\"INSERT INTO `karvy_metrics` VALUES ( '{date}','{metric_name}','{metric_value}', '{table_name}', '{group_level}', '{fund_name}')\"\n",
    "        db.execute_(query)\n",
    "    except Exception as e:\n",
    "        logging.error(\"Unable to insert metrics data\")\n",
    "        logging.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:14.750085Z",
     "start_time": "2020-08-25T08:31:14.723074Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 µs, sys: 0 ns, total: 12 µs\n",
      "Wall time: 19.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def initialize(date_str, table, database='funds', date_column='BatchCloseDate', tenant_id='karvy',\n",
    "              transaction_status='Active', purred = 'Purred', transaction_type = 'TransactionType',\n",
    "              folio = 'Folio', purchase_units = 'DB_Units', redemption_units = 'Cr_Units',scheme = 'SchemeCode',\n",
    "               plan = 'PlanCode',groupby_level='SP',ter_flag='TerFlag', direct_db=None):\n",
    "    \"\"\"Initialization script which takes in batch_close_date and populates opening balance\n",
    "    Args:\n",
    "        date(str) The date upto which we need to initialize\n",
    "        table(str) \n",
    "        date_column(str)\n",
    "        tenant_id(str)\n",
    "    \n",
    "    Note: batch_close_date is (yyyy-MM-dd) (for ex. 2020-04-30), inclusive \n",
    "    \"\"\"\n",
    "    # fundtable column names\n",
    "    purchase_units = 'DB_Units'\n",
    "    redemption_units = 'Cr_Units'\n",
    "    scheme = 'SchemeCode'\n",
    "    plan = 'PlanCode'\n",
    "    \n",
    "    # configurations we use\n",
    "    batch_close_date = 'batch_close_date'\n",
    "    db_units = 'purchase_units'\n",
    "    cr_units = 'redemption_units'\n",
    "    balance_units = 'balance_units'\n",
    "    day_purchase_units = 'day_pu'\n",
    "    day_redemption_units = 'day_ru'\n",
    "    balance_purchase_units = 'balance_pu'\n",
    "    balance_redemption_units = 'balance_ru'\n",
    "    calculated_date = 'calculated_date'\n",
    "    \n",
    "#     inflow_db_units = 'inflow_purchase_units'\n",
    "#     inflow_cr_units = 'inflow_redemption_units'\n",
    "#     inflow_balance_units = 'inflow_balance_units'\n",
    "#     inflow_day_purchase_units = 'inflow_day_pu'\n",
    "#     inflow_day_redemption_units = 'inflow_day_ru'\n",
    "#     inflow_balance_purchase_units = 'inflow_balance_pu'\n",
    "#     inflow_balance_redemption_units = 'inflow_balance_ru'\n",
    "\n",
    "    \n",
    "#     outflow_db_units = 'outflow_purchase_units'\n",
    "#     outflow_cr_units = 'outflow_redemption_units'\n",
    "#     outlfow_balance_units = 'outlfow_balance_units'\n",
    "#     outlfow_day_purchase_units = 'outlfow_day_pu'\n",
    "#     outlfow_day_redemption_units = 'outlfow_day_ru'\n",
    "#     outlfow_balance_purchase_units = 'outlfow_balance_pu'\n",
    "#     outlfow_balance_redemption_units = 'outlfow_balance_ru'\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    database = direct_db or (f'{tenant_id or default_tenant_id}_{database}')\n",
    "    \n",
    "\n",
    "    # read data\n",
    "    data = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .load()\n",
    "#     data = data.filter(col(scheme) == 'TF')\n",
    "    \n",
    "#     data = data.cache()\n",
    "    total_count = data.count()\n",
    "    # debug_df(data, 10)\n",
    "    \n",
    "    # some preprocessings in the data, additional trimmings etc\n",
    "    data = data.withColumn(transaction_status, upper(trim(col(transaction_status))))\n",
    "    data = data.withColumn(purred, upper(trim(col(purred))))\n",
    "    data = data.withColumn(folio, upper(trim(col(folio))))\n",
    "    data = data.withColumn(scheme, upper(trim(col(scheme))))\n",
    "    data = data.withColumn(plan, upper(trim(col(plan))))\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "#     data = data.filter((col(scheme).isin('IC','HC')))\n",
    "    \n",
    "#     print ('here')\n",
    "    \n",
    "    # cast the date column into dates, as we are concerned only with dates now\n",
    "    data = data.withColumn(date_column, col(date_column).cast('date'))\n",
    "    \n",
    "    # filter the date till the batch_close_date (inclusive)\n",
    "    data = data.filter(col(date_column) <= date_str)\n",
    "        \n",
    "    # filter the data according to rules\n",
    "    data = data.filter((col(date_column).isNotNull()) )\n",
    "    # data = data.filter((col(date_column) != '') ) # this will not work for few types\n",
    "    data = data.filter( ~trim(col(date_column)).cast(\"string\").eqNullSafe(''))\n",
    "    \n",
    "    # do must be rules\n",
    "    data = data.filter( (trim(upper((col(transaction_status))))) == \"Y\")\n",
    "    data = data.filter( (trim(upper(col(purred))) == \"P\") | (trim(upper(col(purred))) == \"R\") )\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "    # our configurations\n",
    "    data = data.withColumn(batch_close_date, data[date_column])\n",
    "    \n",
    "#     print ('pur, redem')\n",
    "    # bring in purchase and redemption units\n",
    "    data = data.withColumn(db_units, when((col(purred) == \"P\"), col(purchase_units)).otherwise(0))\n",
    "    data = data.withColumn(cr_units, when((col(purred) == \"R\"), col(redemption_units)).otherwise(0))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#     print ('1')\n",
    "    # get the group by and window partitions based on partitions\n",
    "    group_by_cols = []\n",
    "    window_partition = []\n",
    "    if groupby_level == 'SP':\n",
    "        window_partition = [scheme, plan]\n",
    "        group_by_cols = [scheme, plan, batch_close_date]\n",
    "        # scheme_plan wise we might need to filter out some transaction types\n",
    "        ignored_tr_types = ['CNI', 'CNO', 'TRMI', \n",
    "                            'TRMO', 'TRFI', 'TRFO', 'PLDO',\n",
    "                            'UPLO', 'DMT', 'RMT', 'CNIR', 'CNOR', 'TRMIR', 'TRMOR',\n",
    "                            'TRFIR', 'TRFOR', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR']\n",
    "        data = data.filter( ~(col(transaction_type).isin(ignored_tr_types)) )\n",
    "    elif groupby_level == 'SPT':\n",
    "        window_partition = [scheme, plan, transaction_type]\n",
    "        group_by_cols  = [scheme, plan, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPF':\n",
    "        window_partition = [scheme, plan, folio]\n",
    "        group_by_cols  = [scheme, plan, folio, batch_close_date]\n",
    "    elif groupby_level == 'SPFB':\n",
    "            window_partition = [scheme, plan, folio, broker]\n",
    "            group_by_cols  = [scheme, plan, folio, broker, batch_close_date]\n",
    "    elif groupby_level == 'SPFT':\n",
    "        window_partition = [scheme, plan, folio, transaction_type]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPFTTer':\n",
    "        window_partition = [scheme, plan, folio, transaction_type, ter_flag]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, ter_flag, batch_close_date]\n",
    "    elif groupby_level == 'SPFTB':\n",
    "        window_partition = [scheme, plan, folio, transaction_type, broker]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, broker, batch_close_date]\n",
    "        \n",
    "        # populate broker code\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # roll up the data\n",
    "    rolledup_data = data.groupBy(group_by_cols)\n",
    "    rolledup_data = rolledup_data.agg({db_units:'sum', cr_units:'sum'})\n",
    "        \n",
    "    rolledup_data = rolledup_data.withColumnRenamed(f\"sum({db_units})\", day_purchase_units).withColumnRenamed(f\"sum({cr_units})\", day_redemption_units)\n",
    "    rolledup_data = rolledup_data.withColumn(balance_purchase_units, sum(col(day_purchase_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    rolledup_data = rolledup_data.withColumn(balance_redemption_units, sum(col(day_redemption_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    rolledup_data = rolledup_data.withColumn(balance_units, (col(balance_purchase_units) - col(balance_redemption_units)))\n",
    "    \n",
    "#     print ('rolling up done')\n",
    "    # get the latest data\n",
    "    latest_data = rolledup_data.filter(col(batch_close_date) <= date_str)\n",
    "    w = Window.partitionBy(window_partition).orderBy(col(batch_close_date).desc())\n",
    "    latest_data = latest_data.withColumn(\"rrn\", row_number().over(w)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    \n",
    "    # maintained the calculated date (the latest data upto the calculated date)\n",
    "    latest_data = latest_data.withColumn(calculated_date, lit(date_str).cast('date'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # store in parquet file for optimization of space and only one file and multi partitions\n",
    "    # but write now store in csv and maintain date wise and colesce one\n",
    "    #latest_data = latest_data.cache()\n",
    "    \n",
    "    \n",
    "#     latest_data.show()\n",
    "    # latest_data.coalesce(1).write.csv(f\"{table}_latest/data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "    latest_data.write.parquet(f\"{table}_latest/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    print (f'inital file on date {date_str} written')\n",
    "    return total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "code_folding": [
     168,
     177,
     180,
     183,
     186,
     197
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 AXA\n",
      "43784\n",
      "total trans 170998\n",
      "inital file on date 2020-08-31 written\n",
      "CPU times: user 78.2 ms, sys: 27.3 ms, total: 106 ms\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def initialize(date_str, table, database='funds', date_column='BatchCloseDate', tenant_id='karvy',\n",
    "              transaction_status='Active', purred = 'Purred', transaction_type = 'TransactionType',\n",
    "              folio = 'Folio', purchase_units = 'DB_Units', redemption_units = 'Cr_Units',scheme = 'SchemeCode',\n",
    "               plan = 'PlanCode',groupby_level='SP',ter_flag='TerFlag', direct_db=None,\n",
    "              broker_column = 'BrokerARN',transaction_no = 'TransactionNo',purchase_transaction_no = 'PurchaseTransactionNo',\n",
    "              trans_table='Trans_116'):\n",
    "    \"\"\"Initialization script which takes in batch_close_date and populates opening balance\n",
    "    Args:\n",
    "        date(str) The date upto which we need to initialize\n",
    "        table(str) \n",
    "        date_column(str)\n",
    "        tenant_id(str)\n",
    "    \n",
    "    Note: batch_close_date is (yyyy-MM-dd) (for ex. 2020-04-30), inclusive \n",
    "    \"\"\"\n",
    "    # fundtable column names\n",
    "    purchase_units = 'DB_Units'\n",
    "    redemption_units = 'Cr_Units'\n",
    "    scheme = 'SchemeCode'\n",
    "    plan = 'PlanCode'\n",
    "    \n",
    "    # configurations we use\n",
    "    batch_close_date = 'batch_close_date'\n",
    "    db_units = 'purchase_units'\n",
    "    cr_units = 'redemption_units'\n",
    "    balance_units = 'balance_units'\n",
    "    day_purchase_units = 'day_pu'\n",
    "    day_redemption_units = 'day_ru'\n",
    "    balance_purchase_units = 'balance_pu'\n",
    "    balance_redemption_units = 'balance_ru'\n",
    "    calculated_date = 'calculated_date'\n",
    "    \n",
    "#     inflow_db_units = 'inflow_purchase_units'\n",
    "#     inflow_cr_units = 'inflow_redemption_units'\n",
    "#     inflow_balance_units = 'inflow_balance_units'\n",
    "#     inflow_day_purchase_units = 'inflow_day_pu'\n",
    "#     inflow_day_redemption_units = 'inflow_day_ru'\n",
    "#     inflow_balance_purchase_units = 'inflow_balance_pu'\n",
    "#     inflow_balance_redemption_units = 'inflow_balance_ru'\n",
    "\n",
    "    \n",
    "#     outflow_db_units = 'outflow_purchase_units'\n",
    "#     outflow_cr_units = 'outflow_redemption_units'\n",
    "#     outlfow_balance_units = 'outlfow_balance_units'\n",
    "#     outlfow_day_purchase_units = 'outlfow_day_pu'\n",
    "#     outlfow_day_redemption_units = 'outlfow_day_ru'\n",
    "#     outlfow_balance_purchase_units = 'outlfow_balance_pu'\n",
    "#     outlfow_balance_redemption_units = 'outlfow_balance_ru'\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    database = direct_db or (f'{tenant_id or default_tenant_id}_{database}')\n",
    "    \n",
    "\n",
    "    # read data\n",
    "    data = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .load()\n",
    "#     data = data.filter(col(scheme) == 'TF')\n",
    "    \n",
    "#     data = data.cache()\n",
    "    total_count = data.count()\n",
    "    # debug_df(data, 10)\n",
    "    \n",
    "    # some preprocessings in the data, additional trimmings etc\n",
    "    data = data.withColumn(transaction_status, upper(trim(col(transaction_status))))\n",
    "    data = data.withColumn(purred, upper(trim(col(purred))))\n",
    "    data = data.withColumn(folio, upper(trim(col(folio))))\n",
    "    data = data.withColumn(scheme, upper(trim(col(scheme))))\n",
    "    data = data.withColumn(plan, upper(trim(col(plan))))\n",
    "    data = data.withColumn(broker_column, upper(trim(col(broker_column))))\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "#     data = data.filter((col(scheme).isin('IC','HC')))\n",
    "    \n",
    "#     print ('here')\n",
    "    \n",
    "    # cast the date column into dates, as we are concerned only with dates now\n",
    "    data = data.withColumn(date_column, col(date_column).cast('date'))\n",
    "    \n",
    "    # filter the date till the batch_close_date (inclusive)\n",
    "    data = data.filter(col(date_column) <= date_str)\n",
    "        \n",
    "    # filter the data according to rules\n",
    "    data = data.filter((col(date_column).isNotNull()) )\n",
    "    # data = data.filter((col(date_column) != '') ) # this will not work for few types\n",
    "    data = data.filter( ~trim(col(date_column)).cast(\"string\").eqNullSafe(''))\n",
    "    \n",
    "    # do must be rules\n",
    "    data = data.filter( (trim(upper((col(transaction_status))))) == \"Y\")\n",
    "    data = data.filter( (trim(upper(col(purred))) == \"P\") | (trim(upper(col(purred))) == \"R\") )\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "    # our configurations\n",
    "    data = data.withColumn(batch_close_date, data[date_column])\n",
    "\n",
    "    if 'B' in groupby_level:\n",
    "    #     print (data.count())\n",
    "        broker = 'broker_code'\n",
    "    #     data.groupby(folio, scheme, plan).agg(count(broker_column), countDistinct(broker_column)).show()\n",
    "        data = data.fillna({broker_column:'EMPTY_BROKER'})\n",
    "        data =data.cache()\n",
    "        # folios having single broker code will remain same\n",
    "        multi_broker_folio = data.groupby(folio, scheme, plan).agg(countDistinct(broker_column)).filter(col(f'count({broker_column})') > 1)\n",
    "#         print (multi_broker_folio.count())\n",
    "# #         print (data.count())\n",
    "        single_broker_folio_data = data.join(multi_broker_folio.select(folio, scheme, plan), on=[folio, scheme, plan], how='left_anti')\n",
    "        single_broker_data = single_broker_folio_data.withColumn(broker, col(broker_column))\n",
    "#         print (single_broker_folio_data.count())\n",
    "        multi_broker_folio_data = data.join(multi_broker_folio.select(folio, scheme, plan), on=[folio, scheme, plan], how='left_semi')\n",
    "#         print (multi_broker_folio_data.count())\n",
    "        # bring in correct broker codes\n",
    "        # for purchase p they will be same\n",
    "        data_p = multi_broker_folio_data.filter(col(purred) == 'P')\n",
    "        data_p = data_p.withColumn(broker, col(broker_column))\n",
    "#         print (data_p.count())\n",
    "        # for redemptions \n",
    "        data_r = multi_broker_folio_data.filter(col(purred) == 'R')\n",
    "        print (data_r.count())\n",
    "        # bring in the redemptions \n",
    "        query = f\"SELECT * from {trans_table}\"\n",
    "        trans_data  = spark.read.format(\"jdbc\") \\\n",
    "                .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "                .option(\"query\", query) \\\n",
    "                .option(\"user\", user) \\\n",
    "                .option(\"password\", password) \\\n",
    "                .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "                .load()\n",
    "\n",
    "        # get the redemptions\n",
    "        trans_data = trans_data.join(data_r, on=[scheme, plan, folio, transaction_no], how='left_semi')\n",
    "        print ('total trans', trans_data.count())\n",
    "\n",
    "        grouped = data_p.groupby([transaction_no, broker_column]).count()\n",
    "        trans_data = trans_data.drop(date_column)\n",
    "\n",
    "        data_r = data_r.drop(purchase_units)\n",
    "        trans_data = trans_data.select(scheme, plan, folio, transaction_no, purchase_transaction_no, purchase_units).join(data_r,\n",
    "                                     on=[scheme, plan, folio, transaction_no]).drop(transaction_no)\n",
    "\n",
    "        trans_data = trans_data.join(data_p.select(transaction_no, broker), trans_data[purchase_transaction_no] == data_p[transaction_no], how='left').drop(purchase_transaction_no)\n",
    "\n",
    "        data = single_broker_data.union(data_p.select(single_broker_data.columns)).union(trans_data.select(single_broker_data.columns))\n",
    "#         print ('final data', data.count())\n",
    "    \n",
    "    \n",
    "#     print ('pur, redem')\n",
    "    # bring in purchase and redemption units\n",
    "    data = data.withColumn(db_units, when((col(purred) == \"P\"), col(purchase_units)).otherwise(0))\n",
    "    data = data.withColumn(cr_units, when((col(purred) == \"R\"), col(redemption_units)).otherwise(0))\n",
    "#     data = data.withColumn(db_units, populate_db(col(purred), col(purchase_units)))\n",
    "#     data = data.withColumn(cr_units, populate_cr(col(purred), col(redemption_units)))\n",
    "\n",
    "    \n",
    "    \n",
    "#     print ('1')\n",
    "    # get the group by and window partitions based on partitions\n",
    "    group_by_cols = []\n",
    "    window_partition = []\n",
    "    if groupby_level == 'SP':\n",
    "        window_partition = [scheme, plan]\n",
    "        group_by_cols = [scheme, plan, batch_close_date]\n",
    "        # scheme_plan wise we might need to filter out some transaction types\n",
    "        ignored_tr_types = ['CNI', 'CNO', 'TRMI', \n",
    "                            'TRMO', 'TRFI', 'TRFO', 'PLDO',\n",
    "                            'UPLO', 'DMT', 'RMT', 'CNIR', 'CNOR', 'TRMIR', 'TRMOR',\n",
    "                            'TRFIR', 'TRFOR', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR']\n",
    "        data = data.filter( ~(col(transaction_type).isin(ignored_tr_types)) )\n",
    "    elif groupby_level == 'SPT':\n",
    "        window_partition = [scheme, plan, transaction_type]\n",
    "        group_by_cols  = [scheme, plan, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPF':\n",
    "        window_partition = [scheme, plan, folio]\n",
    "        group_by_cols  = [scheme, plan, folio, batch_close_date]\n",
    "    elif groupby_level == 'SPFB':\n",
    "            window_partition = [scheme, plan, folio, broker]\n",
    "            group_by_cols  = [scheme, plan, folio, broker, batch_close_date]\n",
    "    elif groupby_level == 'SPFT':\n",
    "        window_partition = [scheme, plan, folio, transaction_type]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPFTTer':\n",
    "        data = data.fillna({ter_flag:'EMPTY_TER'})\n",
    "        window_partition = [scheme, plan, folio, transaction_type, ter_flag]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, ter_flag, batch_close_date]\n",
    "    elif groupby_level == 'SPFTBTer':\n",
    "        data = data.fillna({ter_flag:'EMPTY_TER'})\n",
    "        window_partition = [scheme, plan, folio, transaction_type, ter_flag, broker]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, ter_flag, broker, batch_close_date]\n",
    "    elif groupby_level == 'SPFTB':\n",
    "        window_partition = [scheme, plan, folio, transaction_type, broker]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, broker, batch_close_date]\n",
    "        \n",
    "        # populate broker code\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # roll up the data\n",
    "    rolledup_data = data.groupBy(group_by_cols)\n",
    "    rolledup_data = rolledup_data.agg({db_units:'sum', cr_units:'sum'})\n",
    "        \n",
    "    rolledup_data = rolledup_data.withColumnRenamed(f\"sum({db_units})\", day_purchase_units).withColumnRenamed(f\"sum({cr_units})\", day_redemption_units)\n",
    "    rolledup_data = rolledup_data.withColumn(balance_purchase_units, sum(col(day_purchase_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    rolledup_data = rolledup_data.withColumn(balance_redemption_units, sum(col(day_redemption_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    rolledup_data = rolledup_data.withColumn(balance_units, (col(balance_purchase_units) - col(balance_redemption_units)))\n",
    "    \n",
    "#     print ('rolling up done')\n",
    "    # get the latest data\n",
    "    latest_data = rolledup_data.filter(col(batch_close_date) <= date_str)\n",
    "    w = Window.partitionBy(window_partition).orderBy(col(batch_close_date).desc())\n",
    "    latest_data = latest_data.withColumn(\"rrn\", row_number().over(w)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    \n",
    "    # maintained the calculated date (the latest data upto the calculated date)\n",
    "    latest_data = latest_data.withColumn(calculated_date, lit(date_str).cast('date'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # store in parquet file for optimization of space and only one file and multi partitions\n",
    "    # but write now store in csv and maintain date wise and colesce one\n",
    "    # latest_data = latest_data.cache()\n",
    "    \n",
    "    \n",
    "#     latest_data.show()\n",
    "    # latest_data.coalesce(1).write.csv(f\"{table}_latest/data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "    latest_data.write.parquet(f\"{table}_latest/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    print (f'inital file on date {date_str} written')\n",
    "    return total_count\n",
    "\n",
    "code='116'\n",
    "name='AXA'\n",
    "print (code, name)\n",
    "start = time.time()\n",
    "table = f'm_Trans_{code}'\n",
    "trans_table = f'Trans_{code}'\n",
    "groupby_level='SPFTBTer'\n",
    "init_date = '2020-08-31'\n",
    "start_date = '2020-04-01'\n",
    "end_date = '2020-09-02'\n",
    "#         mcr_month_date = '2020-05-01'\n",
    "\n",
    "records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level, trans_table=trans_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dialyjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:16.388873Z",
     "start_time": "2020-08-25T08:31:16.352684Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22 µs, sys: 0 ns, total: 22 µs\n",
      "Wall time: 29.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "def dialy_job(date_str, table='trans116', database='funds', date_column='BatchCloseDate', tenant_id='karvy',\n",
    "              transaction_status='Active', purred = 'Purred', transaction_type = 'TransactionType',\n",
    "              folio = 'Folio', purchase_units = 'DB_Units', redemption_units = 'Cr_Units',scheme = 'SchemeCode',\n",
    "               plan = 'PlanCode',groupby_level='SP', direct_db=None, fn_fromdt = 'fn_fromdt',fn_fromdt_format = 'dd/MM/yyyy',\n",
    "              fn_scheme = 'fn_scheme',fn_plan = 'fn_plan', fn_nav = 'fn_nav', nav_table='nav_master', \n",
    "              scheme_table='scheme_master',scheme_code = 'scheme_code', \n",
    "              plan_code = 'plan_code', category = 'SebiSchemeCategory',\n",
    "              subcategory = 'SebiSchemeSubCategory',nature = 'nature', newmcrid='NewMCRId', ter_flag='TerFlag',\n",
    "              broker_column = 'BrokerARN',transaction_no = 'TransactionNo',purchase_transaction_no = 'PurchaseTransactionNo',\n",
    "              trans_table='Trans_116'\n",
    "             ):\n",
    "    \"\"\"Dialy run this and store the latest data and aum data too\"\"\"\n",
    "    \n",
    "    # inflow outflow\n",
    "    inflow_db_trtypes = ['NEW', 'ADD', 'IPO', 'SIN', 'LTIN', 'LTIA', 'STPN', 'STPA', 'STPI','DIR', 'DSPI', 'SWIN','SWIA']\n",
    "    inflow_cr_trtypes = ['NEWR', 'ADDR', 'IPOR', 'SINR', 'LTINR', 'LTIAR', 'STPNR', \n",
    "                         'STPAR', 'STPIR','DIRR', 'DSPIR', 'SWINR','SWIAR']\n",
    "    \n",
    "    outflow_db_trtypes = ['FULR', 'REDR', 'LTOFR', 'LTOPR', 'STPOR', 'SWDR', 'TRGR', 'SWOPR', 'SWOFR']\n",
    "    outflow_cr_trtypes = ['FUL', 'RED', 'SWD','TRG','LTOF', 'LTOP','STPO', 'SWOP', 'SWOF']\n",
    "    \n",
    "    \n",
    "    \n",
    "    # configurations we use\n",
    "    batch_close_date = 'batch_close_date'\n",
    "    db_units = 'purchase_units'\n",
    "    cr_units = 'redemption_units'\n",
    "    balance_units = 'balance_units'\n",
    "    day_purchase_units = 'day_pu'\n",
    "    day_redemption_units = 'day_ru'\n",
    "    balance_purchase_units = 'balance_pu'\n",
    "    balance_redemption_units = 'balance_ru'\n",
    "    calculated_date = 'calculated_date'\n",
    "    today_pu = 'today_pu'\n",
    "    today_ru = 'today_ru'\n",
    "    effective_nav = 'effective_nav'\n",
    "    aum = 'aum'\n",
    "    aaum = 'aaum'\n",
    "    inflow = 'inflow'\n",
    "    outflow = 'outflow'\n",
    "    inflow_db_units = 'inflow_purchase_units'\n",
    "    inflow_cr_units = 'inflow_redemption_units'\n",
    "    outflow_db_units = 'outflow_purchase_units'\n",
    "    outflow_cr_units = 'outflow_redemption_units'\n",
    "    inflow_units = 'inflow_units'\n",
    "    outflow_units = 'outflow_units'\n",
    "    \n",
    "    # get the latest data from the previously stored file\n",
    "    date_obj = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    day_num = date_obj.day\n",
    "    previous_day = date_obj - datetime.timedelta(1)\n",
    "    previous_day_str = previous_day.strftime('%Y-%m-%d')\n",
    "    latest_data = spark.read.parquet(f\"{table}_latest/data_{groupby_level}_{previous_day_str}.parquet\")\n",
    "\n",
    "    # debug_df(latest_data)\n",
    "    \n",
    "    # get  the todays data\n",
    "    database = direct_db or (f'{tenant_id or default_tenant_id}_{database}')\n",
    "    query = f\"SELECT * from {table} where CAST({date_column} AS DATE)='{date_str}'\"\n",
    "    data  = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "            .option(\"query\", query) \\\n",
    "            .option(\"user\", user) \\\n",
    "            .option(\"password\", password) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .load()\n",
    "#     data = data.filter(col(scheme) == 'TF')\n",
    "    \n",
    "    data = data.cache()\n",
    "    \n",
    "#     data = data.filter((col(scheme)=='IC'))\n",
    "    # latest_data = latest_data.filter((col(scheme) == 'OV') & (col(plan) == 'RG'))\n",
    "    # data = data.filter((col(scheme) == 'OV') & (col(plan) == 'RG'))\n",
    "    # print (data.count())\n",
    "    # debug_df(data)\n",
    "    \n",
    "    # calculate all the steps as in initialization\n",
    "    # some preprocessings in the data, additional trimmings etc\n",
    "    data = data.withColumn(transaction_status, upper(trim(col(transaction_status))))\n",
    "    data = data.withColumn(purred, upper(trim(col(purred))))\n",
    "    data = data.withColumn(folio, upper(trim(col(folio))))\n",
    "    data = data.withColumn(scheme, upper(trim(col(scheme))))\n",
    "    data = data.withColumn(plan, upper(trim(col(plan))))\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "    \n",
    "    day_count = data.count()\n",
    "#     data = data.filter((col(scheme).isin('IC','HC')))\n",
    "    \n",
    "\n",
    "    # cast the date column into dates, as we are concerned only with dates now\n",
    "    data = data.withColumn(date_column, col(date_column).cast('date'))\n",
    "    \n",
    "   \n",
    "    # filter the date till the batch_close_date (inclusive)\n",
    "    # data = data.filter(col(date_column) <= date_str)\n",
    "    \n",
    "        \n",
    "    # filter the data according to rules\n",
    "    data = data.filter((col(date_column).isNotNull()) )\n",
    "    # data = data.filter((col(date_column) != '') ) # this will not work for few types\n",
    "    data = data.filter( ~trim(col(date_column)).cast(\"string\").eqNullSafe(''))\n",
    "    \n",
    "    # do must be rules\n",
    "    data = data.filter( (trim(upper((col(transaction_status))))) == \"Y\")\n",
    "    data = data.filter( (trim(upper(col(purred))) == \"P\") | (trim(upper(col(purred))) == \"R\") )\n",
    "    data = data.withColumn(transaction_type, upper(trim(col(transaction_type))))\n",
    "    \n",
    "    # our configurations\n",
    "    data = data.withColumn(batch_close_date, data[date_column])\n",
    "    \n",
    "\n",
    "    if 'B' in groupby_level:\n",
    "    #     print (data.count())\n",
    "        broker = 'broker_code'\n",
    "    #     data.groupby(folio, scheme, plan).agg(count(broker_column), countDistinct(broker_column)).show()\n",
    "        data = data.fillna({broker_column:'EMPTY_BROKER'})\n",
    "\n",
    "        # folios having single broker code will remain same\n",
    "        multi_broker_folio = data.groupby(folio, scheme, plan).agg(countDistinct(broker_column)).filter(col(f'count({broker_column})') > 1)\n",
    "\n",
    "        single_broker_folio_data = data.join(multi_broker_folio.select(folio, scheme, plan), on=[folio, scheme, plan], how='left_anti')\n",
    "        single_broker_data = single_broker_folio_data.withColumn(broker, col(broker_column))\n",
    "\n",
    "        multi_broker_folio_data = data.join(multi_broker_folio.select(folio, scheme, plan), on=[folio, scheme, plan], how='left_semi')\n",
    "        # bring in correct broker codes\n",
    "        # for purchase p they will be same\n",
    "        data_p = multi_broker_folio_data.filter(col(purred) == 'P')\n",
    "        data_p = data_p.withColumn(broker, col(broker_column))\n",
    "\n",
    "        # for redemptions \n",
    "        data_r = multi_broker_folio_data.filter(col(purred) == 'R')\n",
    "        # bring in the redemptions \n",
    "        query = f\"SELECT * from {trans_table}\"\n",
    "        trans_data  = spark.read.format(\"jdbc\") \\\n",
    "                .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n",
    "                .option(\"query\", query) \\\n",
    "                .option(\"user\", user) \\\n",
    "                .option(\"password\", password) \\\n",
    "                .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "                .load()\n",
    "\n",
    "        # get the redemptions\n",
    "        trans_data = trans_data.join(data_r, on=[scheme, plan, folio, transaction_no], how='left_semi')\n",
    "\n",
    "        grouped = data_p.groupby([transaction_no, broker_column]).count()\n",
    "        trans_data = trans_data.drop(date_column)\n",
    "\n",
    "        data_r = data_r.drop(redemption_units)\n",
    "        trans_data = trans_data.select(scheme, plan, folio, transaction_no, purchase_transaction_no, redemption_units).join(data_r,\n",
    "                                     on=[scheme, plan, folio, transaction_no]).drop(transaction_no)\n",
    "\n",
    "        trans_data = trans_data.join(data_p.select(transaction_no, broker), trans_data[purchase_transaction_no] == data_p[transaction_no], how='left').drop(purchase_transaction_no)\n",
    "\n",
    "        data = single_broker_data.union(data_p.select(single_broker_data.columns)).union(trans_data.select(single_broker_data.columns))\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print ('pur, redem')\n",
    "    # bring in purchase and redemption units\n",
    "    data = data.withColumn(db_units, when((col(purred) == \"P\"), col(purchase_units)).otherwise(0))\n",
    "    data = data.withColumn(cr_units, when((col(purred) == \"R\"), col(redemption_units)).otherwise(0))\n",
    "#     data = data.withColumn(db_units, populate_db(col(purred), col(purchase_units)))\n",
    "#     data = data.withColumn(cr_units, populate_cr(col(purred), col(redemption_units)))\n",
    "\n",
    "    \n",
    "    \n",
    "#     print ('1')\n",
    "    # get the group by and window partitions based on partitions\n",
    "    group_by_cols = []\n",
    "    window_partition = []\n",
    "    if groupby_level == 'SP':\n",
    "        window_partition = [scheme, plan]\n",
    "        group_by_cols = [scheme, plan, batch_close_date]\n",
    "        # scheme_plan wise we might need to filter out some transaction types\n",
    "        ignored_tr_types = ['CNI', 'CNO', 'TRMI', \n",
    "                            'TRMO', 'TRFI', 'TRFO', 'PLDO',\n",
    "                            'UPLO', 'DMT', 'RMT', 'CNIR', 'CNOR', 'TRMIR', 'TRMOR',\n",
    "                            'TRFIR', 'TRFOR', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR']\n",
    "        data = data.filter( ~(col(transaction_type).isin(ignored_tr_types)) )\n",
    "    elif groupby_level == 'SPT':\n",
    "        window_partition = [scheme, plan, transaction_type]\n",
    "        group_by_cols  = [scheme, plan, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPF':\n",
    "        window_partition = [scheme, plan, folio]\n",
    "        group_by_cols  = [scheme, plan, folio, batch_close_date]\n",
    "    elif groupby_level == 'SPFB':\n",
    "            window_partition = [scheme, plan, folio, broker]\n",
    "            group_by_cols  = [scheme, plan, folio, broker, batch_close_date]\n",
    "    elif groupby_level == 'SPFT':\n",
    "        window_partition = [scheme, plan, folio, transaction_type]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, batch_close_date]\n",
    "    elif groupby_level == 'SPFTTer':\n",
    "        data = data.fillna({ter_flag:'EMPTY_TER'})\n",
    "        window_partition = [scheme, plan, folio, transaction_type, ter_flag]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, ter_flag, batch_close_date]\n",
    "    elif groupby_level == 'SPFTBTer':\n",
    "        window_partition = [scheme, plan, folio, transaction_type, ter_flag, broker]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, ter_flag, broker, batch_close_date]\n",
    "    elif groupby_level == 'SPFTB':\n",
    "        window_partition = [scheme, plan, folio, transaction_type, broker]\n",
    "        group_by_cols = [scheme, plan, folio, transaction_type, broker, batch_close_date]\n",
    "   \n",
    "    \n",
    "    \n",
    "    # roll up the data\n",
    "    rolledup_data = data.groupBy(group_by_cols)\n",
    "    rolledup_data = rolledup_data.agg({db_units:'sum', cr_units:'sum'})\n",
    "        \n",
    "    rolledup_data = rolledup_data.withColumnRenamed(f\"sum({db_units})\", day_purchase_units).withColumnRenamed(f\"sum({cr_units})\", day_redemption_units)\n",
    "\n",
    "    # inflow outflow units\n",
    "    \n",
    "    \n",
    "    latest_data = latest_data.drop(day_purchase_units, day_redemption_units, batch_close_date, balance_units)\n",
    "    latest_data = latest_data.withColumnRenamed(balance_purchase_units, day_purchase_units)\n",
    "    latest_data = latest_data.withColumnRenamed(balance_redemption_units, day_redemption_units)\n",
    "    latest_data = latest_data.withColumnRenamed(calculated_date, batch_close_date)\n",
    "    \n",
    "    # debug_df(rolledup_data)\n",
    "    # debug_df(latest_data)\n",
    "    combined_data = latest_data.union(rolledup_data.select(latest_data.columns))\n",
    "    combined_data = combined_data.cache()\n",
    "    combined_count = combined_data.count()\n",
    "    # debug_df(rolledup_data)\n",
    "    # debug_df(latest_data)\n",
    "    # debug_df(combined_data)\n",
    "    combined_data = combined_data.withColumn(balance_purchase_units, sum(col(day_purchase_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    combined_data = combined_data.withColumn(balance_redemption_units, sum(col(day_redemption_units)).over(Window.partitionBy(window_partition).orderBy(batch_close_date).rowsBetween(-sys.maxsize, 0)))\n",
    "    combined_data = combined_data.withColumn(balance_units, (col(balance_purchase_units) - col(balance_redemption_units)))\n",
    "    # debug_df(combined_data)\n",
    "    \n",
    "    \n",
    "    # store the latest day data again\n",
    "    # get the latest data\n",
    "    combined_data = combined_data.filter(col(batch_close_date) <= date_str)\n",
    "    w = Window.partitionBy(window_partition).orderBy(col(batch_close_date).desc())\n",
    "    combined_data = combined_data.withColumn(\"rrn\", row_number().over(w)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    \n",
    "    \n",
    "    # maintained the calculated date (the latest data upto the calculated date)\n",
    "    combined_data = combined_data.withColumn(calculated_date, lit(date_str).cast('date'))\n",
    "    \n",
    "    # debug_df(combined_data)\n",
    "    # store in parquet file for optimization of space and only one file and multi partitions\n",
    "    # but write now store in csv and maintain date wise and colesce one\n",
    "    combined_data = combined_data.cache()\n",
    "    # combined_data.coalesce(1).write.csv(f\"{table}_latest/data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "    combined_data.write.parquet(f\"{table}_latest/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    \n",
    "    # join the nav, scheme_master data\n",
    "    nav_data = read_df(nav_table, '*', database)\n",
    "    nav_data = nav_data.withColumnRenamed(fn_scheme, scheme)\n",
    "    nav_data = nav_data.withColumnRenamed(fn_plan, plan)\n",
    "    nav_data = nav_data.withColumn(fn_fromdt, col(fn_fromdt).cast('date'))\n",
    "\n",
    "    scheme_master = read_df(scheme_table, [scheme_code, plan_code, nature, category, subcategory, newmcrid], database)\n",
    "    scheme_master = scheme_master.withColumnRenamed(scheme_code, scheme)\n",
    "    scheme_master = scheme_master.withColumnRenamed(plan_code, plan)\n",
    "\n",
    "    scheme_master_ = scheme_master.dropDuplicates([scheme, plan])\n",
    "    nav_scheme = nav_data.join(scheme_master_, on=[scheme, plan], how='left')\n",
    "    nav_data = nav_scheme\n",
    "    \n",
    "    # debug_df(nav_data)\n",
    "    \n",
    "    # calculate the aum \n",
    "    combined_data = combined_data.withColumn(effective_nav, date_sub(col(calculated_date), 1))\n",
    "    combined_data = combined_data.withColumn(today_pu, when((col(calculated_date) == col(batch_close_date)), col(day_purchase_units)).otherwise(0))\n",
    "    combined_data = combined_data.withColumn(today_ru, when((col(calculated_date) == col(batch_close_date)), col(day_redemption_units)).otherwise(0))\n",
    "    \n",
    "    # inflow outflow addition\n",
    "    inflow_db_condition = col(transaction_type).isin(inflow_db_trtypes)\n",
    "    inflow_cr_condition = col(transaction_type).isin(inflow_cr_trtypes)\n",
    "    combined_data = combined_data.withColumn(inflow_db_units, when(inflow_db_condition, col(today_pu)).otherwise(0))\n",
    "    combined_data = combined_data.withColumn(inflow_cr_units, when(inflow_cr_condition, col(today_ru)).otherwise(0))\n",
    "    \n",
    "    outflow_db_condition = col(transaction_type).isin(outflow_db_trtypes)\n",
    "    outflow_cr_condition = col(transaction_type).isin(outflow_cr_trtypes)\n",
    "    combined_data = combined_data.withColumn(outflow_db_units, when(outflow_db_condition, col(today_pu)).otherwise(0))\n",
    "    combined_data = combined_data.withColumn(outflow_cr_units, when(outflow_cr_condition, col(today_ru)).otherwise(0))\n",
    "    \n",
    "    combined_data = combined_data.withColumn(inflow_units, col(inflow_db_units) - col(inflow_cr_units))\n",
    "    combined_data = combined_data.withColumn(outflow_units, col(outflow_cr_units) - col(outflow_db_units))\n",
    "    \n",
    "    # debug_df(combined_data)\n",
    "    \n",
    "    nav_filteredFT = nav_data.filter(col(fn_fromdt) < date_str)\n",
    "    navw = Window.partitionBy([scheme, plan]).orderBy(col(fn_fromdt).desc())\n",
    "    nav_populate = nav_filteredFT.withColumn(\"rrn\", row_number().over(navw)).where(col(\"rrn\") == 1).drop(\"rrn\")\n",
    "    nav_populate = nav_populate.withColumn(calculated_date, lit(date_str))\n",
    "    nav_populate = nav_populate.select([scheme, plan, fn_fromdt, fn_nav, calculated_date, category, subcategory, nature, newmcrid])\n",
    "    \n",
    "#     debug_df(nav_populate)\n",
    "    \n",
    "    joined = combined_data.join(nav_populate, on=[scheme, plan, calculated_date], how='left')\n",
    "    joined = joined.withColumn(aum, col(fn_nav) * col(balance_units))\n",
    "    joined = joined.withColumn(inflow, col(fn_nav) * col(inflow_units))\n",
    "    joined = joined.withColumn(outflow, col(fn_nav) * col(outflow_units))\n",
    "    \n",
    "#     aum_dummy = f'{aum}_d'\n",
    "#     final_joined = joined.withColumn(aum_dummy, col(aum))\n",
    "#     final_joined = final_joined.fillna({aum_dummy: 0})\n",
    "    \n",
    "#     # moving average logic\n",
    "#     print (day_num)\n",
    "#     if day_num == 1:\n",
    "#         final_joined = final_joined.withColumn(f'pre_{aum_dummy}', lit(0))\n",
    "#         final_joined = final_joined.withColumn(aaum, col(aum_dummy))\n",
    "#     elif day_num == 2:\n",
    "#         final_joined = final_joined.withColumn(f'pre_{aum_dummy}', lit(0))\n",
    "#         final_joined = final_joined.withColumn(aaum, col(aum_dummy))\n",
    "#     else:\n",
    "#         previous_day_aum = spark.read.parquet(f\"{table}_dialy/aaum_data_{groupby_level}_{previous_day_str}.parquet\")\n",
    "#         previous_day_aum.withColumnRenamed(aum_dummy, f'pre_{aum_dummy}')\n",
    "#         final_joined = final_joined.join(previous_day_aum.select(window_partition + [f'pre_{aum_dummy}']), on=window_partition, how='left')\n",
    "#         final_joined = final_joined.fillna({f'pre_{aum_dummy}': 0})\n",
    "#         final_joined = final_joined.withColumn(aaum, (col(aum_dummy) + col(f'pre_{aum_dummy}') / day_num))\n",
    "        \n",
    "    \n",
    "#     debug_df(joined)\n",
    "    #joined = joined.cache()\n",
    "    # store the data in the files\n",
    "#     joined.coalesce(1).write.csv(f\"{table}_dialy/data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "    joined.write.parquet(f\"{table}_dialy/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "#     final_joined = final_joined.cache()\n",
    "#     final_joined.coalesce(1).write.csv(f\"{table}_dialy/aaum_data_{groupby_level}_{date_str}.csv\",header=True, mode='overwrite')\n",
    "#     final_joined.coalesce(1).write.parquet(f\"{table}_dialy/aaum_data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    \n",
    "    # upload the data if needed\n",
    "    # Done\n",
    "    print (f'dialy file on date {date_str} generated')\n",
    "    return day_count, combined_count\n",
    "\n",
    "# dialy_job('2020-05-01', groupby_level='SPFT', table='Trans_116', direct_db='BankRecon', nav_table='fund_navreg_AXA_29072020',\n",
    "#         scheme_table='Fund_Master_AXA_29072020', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory'\n",
    "#         )\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='Trans_128', direct_db='BankRecon', nav_table='fund_navreg_axismf',\n",
    "#         scheme_table='fund_master_axismf', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory'\n",
    "#         )\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='Trans_120', direct_db='BankRecon', nav_table='fund_navreg_invesco',\n",
    "#         scheme_table='fund_master_INVESCO', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory'\n",
    "#         )\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='m_Trans_116', direct_db='BankRecon', nav_table='fund_navreg_AXA_29072020',\n",
    "#         scheme_table='Fund_Master_AXA_29072020', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#         )\n",
    "\n",
    "# dialy_job('2020-06-01', groupby_level='SP', table='m_Trans_117', direct_db='BankRecon', nav_table='fund_navreg_MIRAE',\n",
    "#         scheme_table='fund_master_MIRAE', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#         )\n",
    "# print ('done')\n",
    "# import time\n",
    "# for i,ele in enumerate(list(daterange('2020-05-01', '2020-06-02'))):\n",
    "#     s = time.time()\n",
    "#     dialy_job(ele, groupby_level='SPFT', table='m_Trans_117', direct_db='BankRecon', nav_table='fund_navreg_MIRAE',\n",
    "#         scheme_table='fund_master_MIRAE', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#          category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#         )\n",
    "#     generate_mcr_report(table=f'm_Trans_{code}', groupby_level='SP', start_date = '2020-05-02', end_date = '2020-06-02')\n",
    "\n",
    "#     print (i, ele, time.time() - s)\n",
    "\n",
    "# day_records, combined_records = dialy_job('2020-04-02', groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "#             scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#              category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 AXA\n",
      "inital file on date 2020-08-31 written\n",
      "initialization time 111.50154161453247\n",
      "dialy file on date 2020-09-01 generated\n",
      "     0 2020-09-01 21.588369369506836\n",
      "job time is 21.589236736297607\n",
      "overall time is 133.0912094116211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "table_codes = {117: 'MIRAE'}\n",
    "\n",
    "table_codes = {\"116\": \"AXA\",\"117\": \"MIRAE\",\"107\": \"BOB\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codes = {\"117\": \"MIRAE\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codess ={130:\"peerless\", 120:\"INVESCO\", }\n",
    "\n",
    "table_codes = {125: 'IBMF', 152: \"ITI\", 123: \"Quantum\",}\n",
    "\n",
    "table_codes = {  107: \"BOB\",135: \"IDBIMF\", 178: \"BNPMF\", 103: \"PMF\"}\n",
    "\n",
    "table_codes = { 103:\"peerless\", 118:\"edelwwise\"}\n",
    "table_codes = {\"129\": \"DLFPramerica\",\"120\": \"INVESCO\"}\n",
    "table_codes = {\"RMF\": \"Reliance\" }\n",
    "table_codes = {\"116\": \"AXA\"}\n",
    "# table_codes = {\"135\": 'IDBIMF'}\n",
    "# table_codes = {118:\"edelwwise\"}\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "#         try:\n",
    "#             sc.stop()\n",
    "#         except:\n",
    "\n",
    "#             print (\"error no sc\")\n",
    "#         # intialize spark again\n",
    "#         conf = SparkConf()\n",
    "#         conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 16)\n",
    "\n",
    "#         #Create spark context and sparksession\n",
    "#         sc = SparkContext.getOrCreate(conf=conf)\n",
    "#         SparkContext.setSystemProperty(\"spark.driver.memory\", \"60g\")\n",
    "#         SparkContext.setSystemProperty(\"spark.executor.memory\", \"60g\")\n",
    "#         SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "#         SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"100g\")\n",
    "#         spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        trans_table = f'Trans_{code}'\n",
    "        groupby_level='SPFTBTer'\n",
    "        init_date = '2020-08-31'\n",
    "        start_date = '2020-09-01'\n",
    "        end_date = '2020-09-02'\n",
    "#         mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level, trans_table=trans_table)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level, table)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level, table)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange(start_date, end_date))):\n",
    "#             try:\n",
    "#                 spark.catalog.clearCache()\n",
    "#             except:\n",
    "#                 pass\n",
    "#             try:\n",
    "#                 sc.stop()\n",
    "#             except:\n",
    "\n",
    "#                 print (\"error no sc\")\n",
    "#             # intialize spark again\n",
    "#             conf = SparkConf()\n",
    "#             conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 16)\n",
    "\n",
    "#             #Create spark context and sparksession\n",
    "#             sc = SparkContext.getOrCreate(conf=conf)\n",
    "#             SparkContext.setSystemProperty(\"spark.driver.memory\", \"120g\")\n",
    "#             SparkContext.setSystemProperty(\"spark.executor.memory\", \"120g\")\n",
    "#             SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "#             SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"100g\")\n",
    "#             spark = SparkSession(sc)\n",
    "            \n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}_{code}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId',\n",
    "                                                      trans_table=trans_table\n",
    "            )\n",
    "#             save_metric(ele, 'day_records', day_records, name, groupby_level, table)\n",
    "#             save_metric(ele, 'combined_records', combined_records, name, groupby_level, table)\n",
    "#             save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level, table)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMF Reliance\n",
      "initialization time 4.76837158203125e-06\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "table_codes = {117: 'MIRAE'}\n",
    "\n",
    "table_codes = {\"116\": \"AXA\",\"117\": \"MIRAE\",\"107\": \"BOB\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codes = {\"117\": \"MIRAE\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codess ={130:\"peerless\", 120:\"INVESCO\", }\n",
    "\n",
    "table_codes = {125: 'IBMF', 152: \"ITI\", 123: \"Quantum\",}\n",
    "\n",
    "table_codes = {  107: \"BOB\",135: \"IDBIMF\", 178: \"BNPMF\", 103: \"PMF\"}\n",
    "\n",
    "table_codes = { 103:\"peerless\", 118:\"edelwwise\"}\n",
    "table_codes = {\"129\": \"DLFPramerica\",\"120\": \"INVESCO\"}\n",
    "table_codes = {\"RMF\": \"Reliance\" }\n",
    "# table_codes = {\"116\": \"AXA\"}\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 16)\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"60g\")\n",
    "        SparkContext.setSystemProperty(\"spark.executor.memory\", \"60g\")\n",
    "        SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "        SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"100g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPFTTer'\n",
    "        init_date = '2020-03-31'\n",
    "        start_date = '2020-04-07'\n",
    "        end_date = '2020-09-02'\n",
    "#         mcr_month_date = '2020-05-01'\n",
    "\n",
    "#         records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level, table)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level, table)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange(start_date, end_date))):\n",
    "            try:\n",
    "                spark.catalog.clearCache()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                sc.stop()\n",
    "            except:\n",
    "\n",
    "                print (\"error no sc\")\n",
    "            # intialize spark again\n",
    "            conf = SparkConf()\n",
    "            conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 16)\n",
    "\n",
    "            #Create spark context and sparksession\n",
    "            sc = SparkContext.getOrCreate(conf=conf)\n",
    "            SparkContext.setSystemProperty(\"spark.driver.memory\", \"120g\")\n",
    "            SparkContext.setSystemProperty(\"spark.executor.memory\", \"120g\")\n",
    "            SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "            SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"100g\")\n",
    "            spark = SparkSession(sc)\n",
    "            \n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}_{code}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "            save_metric(ele, 'day_records', day_records, name, groupby_level, table)\n",
    "            save_metric(ele, 'combined_records', combined_records, name, groupby_level, table)\n",
    "            save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level, table)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "table_codes = {117: 'MIRAE'}\n",
    "\n",
    "table_codes = {\"116\": \"AXA\",\"117\": \"MIRAE\",\"107\": \"BOB\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codes = {\"117\": \"MIRAE\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codess ={130:\"peerless\", 120:\"INVESCO\", }\n",
    "\n",
    "table_codes = {125: 'IBMF', 152: \"ITI\", 123: \"Quantum\",}\n",
    "\n",
    "table_codes = {  107: \"BOB\",135: \"IDBIMF\", 178: \"BNPMF\", 103: \"PMF\"}\n",
    "\n",
    "table_codes = { 103:\"peerless\", 118:\"edelwwise\"}\n",
    "table_codes = {\"129\": \"DLFPramerica\",\"120\": \"INVESCO\"}\n",
    "table_codes = {\"RMF\": \"Reliance\" }\n",
    "table_codes = {\"116\": \"AXA\"}\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 16)\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"60g\")\n",
    "        SparkContext.setSystemProperty(\"spark.executor.memory\", \"60g\")\n",
    "        SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "        SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"100g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPFTTer'\n",
    "        init_date = '2020-03-31'\n",
    "        start_date = '2020-04-01'\n",
    "        end_date = '2020-09-02'\n",
    "#         mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "        #save_metric(init_date, 'records_processed', records, name, groupby_level, table)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange(start_date, end_date))):\n",
    "            try:\n",
    "                spark.catalog.clearCache()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                sc.stop()\n",
    "            except:\n",
    "\n",
    "                print (\"error no sc\")\n",
    "            # intialize spark again\n",
    "            conf = SparkConf()\n",
    "            conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 16)\n",
    "\n",
    "            #Create spark context and sparksession\n",
    "            sc = SparkContext.getOrCreate(conf=conf)\n",
    "            SparkContext.setSystemProperty(\"spark.driver.memory\", \"120g\")\n",
    "            SparkContext.setSystemProperty(\"spark.executor.memory\", \"120g\")\n",
    "            SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "            SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"100g\")\n",
    "            spark = SparkSession(sc)\n",
    "            \n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}_{code}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "            #save_metric(ele, 'day_records', day_records, name, groupby_level, table)\n",
    "            #save_metric(ele, 'combined_records', combined_records, name, groupby_level, table)\n",
    "            #save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level, table)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "table_codes = {117: 'MIRAE'}\n",
    "\n",
    "table_codes = {\"116\": \"AXA\",\"117\": \"MIRAE\",\"107\": \"BOB\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codes = {\"117\": \"MIRAE\",\"120\": \"INVESCO\",\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\"135\": \"IDBIMF\",\"125\": \"IBMF\",\"128\": \"AXISMF\",\"178\": \"BNPMF\",\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\"103\": \"PMF\",\"166\": \"Quant\",\"130\": \"PeerlessMF\",\"104\": \"TAURUS\",\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\"127\": \"MOTILAL\",\"102\": \"LIC\",\"176\": \"SundaramMF\",\"101\": \"canrobeco\",\"129\": \"DLFPramerica\"}\n",
    "\n",
    "table_codes = {125: 'IBMF', 152: \"ITI\", 123: \"Quantum\",}\n",
    "table_codes = {125:'IBMF', 104:'TARUS', 103:\"peerless\", 123:\"Quantum\", 118:\"edelwwise\"}\n",
    "\n",
    "table_codes = {\"RMF\": \"Reliance\" }\n",
    "\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 16)\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        \n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"60g\")\n",
    "        SparkContext.setSystemProperty(\"spark.executor.memory\", \"60g\")\n",
    "        SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "        SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"200g\")\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPFT'\n",
    "        init_date = '2020-03-31'\n",
    "        start_date = '2020-06-05'\n",
    "        end_date = '2020-08-02'\n",
    "#         mcr_month_date = '2020-05-01'\n",
    "\n",
    "        #records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "        #save_metric(init_date, 'records_processed', records, name, groupby_level, table)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange(start_date, end_date))):\n",
    "            try:\n",
    "                spark.catalog.clearCache()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                sc.stop()\n",
    "            except:\n",
    "\n",
    "                print (\"error no sc\")\n",
    "            # intialize spark again\n",
    "            conf = SparkConf()\n",
    "            conf.setMaster(\"local[*]\").setAppName(\"My app\").set(\"spark.sql.shuffle.partitions\", 16)\n",
    "\n",
    "            #Create spark context and sparksession\n",
    "            \n",
    "            SparkContext.setSystemProperty(\"spark.driver.memory\", \"60g\")\n",
    "            SparkContext.setSystemProperty(\"spark.executor.memory\", \"60g\")\n",
    "            SparkContext.setSystemProperty(\"spark.executor.offHeap.enabled\", \"true\")\n",
    "            SparkContext.setSystemProperty(\"spark.executor.offHeap.size\", \"200g\")\n",
    "            sc = SparkContext.getOrCreate(conf=conf)\n",
    "            spark = SparkSession(sc)\n",
    "            \n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}_{code}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "            save_metric(ele, 'day_records', day_records, name, groupby_level, table)\n",
    "            save_metric(ele, 'combined_records', combined_records, name, groupby_level, table)\n",
    "            save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level, table)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-25T08:32:00.266Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "init_date = '2020-04-01'\n",
    "groupby_level = 'SPFT'\n",
    "table = 'trans116'\n",
    "direct_db = 'kfintech_funds'\n",
    "nav_table = 'nav_master'\n",
    "scheme_table = 'scheme_master'\n",
    "\n",
    "init_date = '2020-04-01'\n",
    "groupby_level = 'SPFT'\n",
    "table = 'trans116'\n",
    "direct_db = 'kfintech_funds'\n",
    "nav_table = 'nav_master'\n",
    "scheme_table = 'scheme_master'\n",
    "\n",
    "initialize(init_date, table=table,direct_db=direct_db, groupby_level=groupby_level)\n",
    "# init_date = '2020-04-30'\n",
    "# initialize(init_date, table=table,direct_db=direct_db, groupby_level=groupby_level)\n",
    "\n",
    "\n",
    "for ele in daterange('2020-04-02', '2020-04-03'):\n",
    "    dialy_job(ele, groupby_level=groupby_level, table=table, direct_db=direct_db, nav_table=nav_table,\n",
    "                scheme_table=scheme_table, scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "                 category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|fm_NewMCRId|            sum(aum)|\n",
      "+-----------+--------------------+\n",
      "|    AIII_29| 9.175777190211585E8|\n",
      "|     AII_25|3.7345910768308026E8|\n",
      "|     AII_26| 4.441361186929758E9|\n",
      "|      AI_12| 3.734928158315652E8|\n",
      "|     AII_21| 7.246340465773715E8|\n",
      "|       AI_1| 6.771047297224059E8|\n",
      "|    AIII_31| 3.005910526386833E8|\n",
      "|    AIII_28| 3.176874591605646E9|\n",
      "|       AI_3| 2.529782207578232E9|\n",
      "|       null|                null|\n",
      "|     AII_19| 1.711511444015332E9|\n",
      "|     AII_17|   4.0515296491536E8|\n",
      "|       AI_2| 2.331079486276221E9|\n",
      "|       AI_6|3.2928791216110605E8|\n",
      "|    AIII_27| 8.683341086366962E8|\n",
      "+-----------+--------------------+\n",
      "\n",
      "+-----------+--------------------+\n",
      "|fm_NewMCRId|            sum(aum)|\n",
      "+-----------+--------------------+\n",
      "|    AIII_29| 9.175777190211582E8|\n",
      "|     AII_25|3.7345910768307984E8|\n",
      "|     AII_26| 4.441361186929739E9|\n",
      "|      AI_12|3.7349281583156604E8|\n",
      "|     AII_21|  7.24634046577371E8|\n",
      "|       AI_1| 6.771047297224042E8|\n",
      "|    AIII_31| 3.005910526386834E8|\n",
      "|    AIII_28|  3.17687459160563E9|\n",
      "|       AI_3|2.5297822075783257E9|\n",
      "|       null|                null|\n",
      "|     AII_19|1.7115114440152931E9|\n",
      "|     AII_17| 4.051529649153596E8|\n",
      "|       AI_2|2.3310794862781386E9|\n",
      "|       AI_6| 3.292879121611063E8|\n",
      "|    AIII_27| 8.683341086366105E8|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latest_data = spark.read.parquet(f\"{'m_Trans_116'}_dialy/data_{'SPFTTer'}_{'2020-09-01'}.parquet\")\n",
    "latest_data_ = spark.read.parquet(f\"{'m_Trans_116'}_dialy/data_{'SPFT'}_{'2020-09-01'}.parquet\")\n",
    "# latest_data.coalesce(1).write.csv('axa_calculations_ter.csv', header=True,mode='overwrite')\n",
    "newmcrid='fm_NewMCRId'\n",
    "latest_data.groupby(newmcrid).agg(sum('aum')).show()\n",
    "latest_data_.groupby(newmcrid).agg(sum('aum')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|fm_NewMCRId|            sum(aum)|\n",
      "+-----------+--------------------+\n",
      "|    AIII_29| 9.175777190211585E8|\n",
      "|     AII_25|3.7345910768308026E8|\n",
      "|     AII_26| 4.441361186929758E9|\n",
      "|      AI_12| 3.734928158315652E8|\n",
      "|     AII_21| 7.246340465773715E8|\n",
      "|       AI_1| 6.771047297224059E8|\n",
      "|    AIII_31| 3.005910526386833E8|\n",
      "|    AIII_28| 3.176874591605646E9|\n",
      "|       AI_3| 2.529782207578232E9|\n",
      "|       null|                null|\n",
      "|     AII_19| 1.711511444015332E9|\n",
      "|     AII_17|   4.0515296491536E8|\n",
      "|       AI_2| 2.331079486276221E9|\n",
      "|       AI_6|3.2928791216110605E8|\n",
      "|    AIII_27| 8.683341086366962E8|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llatest_data = spark.read.parquet(f\"{'m_Trans_116'}_dialy/data_{'SPFTTer'}_{'2020-09-01'}.parquet\")\n",
    "llatest_data.groupby(newmcrid).agg(sum('aum')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|fm_NewMCRId|            sum(aum)|\n",
      "+-----------+--------------------+\n",
      "|    AIII_29| 7.691624059313667E8|\n",
      "|     AII_25|-1.23559474364505...|\n",
      "|     AII_26| 4.130736985815619E9|\n",
      "|      AI_12|-1.98018786825583...|\n",
      "|     AII_21| 7.586508808656341E8|\n",
      "|       AI_1| 6.771047297223973E8|\n",
      "|    AIII_31| 2.874654285550581E7|\n",
      "|    AIII_28| 3.038452141457969E9|\n",
      "|       AI_3|-1.17023161735102...|\n",
      "|       null|                null|\n",
      "|     AII_19|-3.06165551048728...|\n",
      "|     AII_17|4.0515296491536057E8|\n",
      "|       AI_2|-7.02685621425754...|\n",
      "|       AI_6|-1.24080189235766...|\n",
      "|    AIII_27| 6.237194375079607E8|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blatest_data = spark.read.parquet(f\"{'m_Trans_116'}_dialy/data_{'SPFTBTer'}_{'2020-09-01'}.parquet\")\n",
    "blatest_data.groupby(newmcrid).agg(sum('aum')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T08:31:51.310674Z",
     "start_time": "2020-08-25T08:31:26.686999Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_data(start_date, end_date, groupby_level='SPT', table='m_Trans_116'):\n",
    "    \n",
    "    final_data = None\n",
    "\n",
    "    dates_list = list(daterange(start_date, end_date))\n",
    "    for date in dates_list:\n",
    "        date_obj = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "        day_num = date_obj.day\n",
    "        latest_data = spark.read.parquet(f\"{table}_dialy/data_{groupby_level}_{date}.parquet\")\n",
    "        if final_data:\n",
    "            final_data = final_data.union(latest_data)\n",
    "        else:\n",
    "            final_data = latest_data\n",
    "    return final_data\n",
    "\n",
    "def generate_mcr_report(table='m_Trans_116', ignored_tr_types = ['CNI', 'CNO', 'TRMI', 'TRMO', \n",
    "                                                                 'TRFI', 'TRFO', 'PLDO', 'UPLO', 'DMT',\n",
    "                                                                 'RMT', 'CNIR', 'CNOR', 'TRMIR', 'TRMOR',\n",
    "                                                                 'TRFIR', 'TRFOR', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR'],\n",
    "        liquid_fund_tr_types = ['NEW', 'ADD', 'IPO', 'SIN', 'NEWR', 'ADDR', 'IPOR', 'SINR'],\n",
    "                       start_date = '2020-05-02', end_date = '2020-06-02', groupby_level='SPT',\n",
    "                        transaction_type='TransactionType',folio='Folio',folio_ignore_types = ['PLDO', 'UPLO', 'DMT', 'RMT', 'PLDOR', 'UPLOR', 'DMTR', 'RMTR'], \n",
    "                      fn_nav = 'fn_nav', newmcrid='fm_NewMCRId', today_pu = 'today_pu', today_ru = 'today_ru', scheme='SchemeCode', aum='aum', plan='PlanCode'):\n",
    "    \n",
    "    till_but_one_day_data = None\n",
    "    inflow = 'inflow'\n",
    "    outflow = 'outflow'\n",
    "    calculated_date = 'calculated_date'\n",
    "    batch_close_date = 'batch_close_date'\n",
    "    balance_pu = 'balance_pu'\n",
    "    balance_ru = 'balance_ru'\n",
    "    balance_units = 'balance_units'\n",
    "    \n",
    "    \n",
    "    dates_list = list(daterange(start_date, end_date))\n",
    "    \n",
    "    for date in dates_list[:-1]:\n",
    "        date_obj = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "        day_num = date_obj.day\n",
    "        latest_data = spark.read.parquet(f\"{table}_dialy/data_{groupby_level}_{date}.parquet\")\n",
    "#         print (ele, latest_data.count())\n",
    "        if till_but_one_day_data:\n",
    "            till_but_one_day_data = till_but_one_day_data.union(latest_data)\n",
    "        else:\n",
    "            till_but_one_day_data = latest_data\n",
    "            \n",
    "    \n",
    "    date_obj = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    final_day = date_obj + datetime.timedelta(1)\n",
    "    final_day_str = final_day.strftime('%Y-%m-%d')\n",
    "    final_day_data = spark.read.parquet(f\"{table}_dialy/data_{groupby_level}_{final_day_str}.parquet\")\n",
    "    last_but_one_day_data = latest_data\n",
    "\n",
    "    till_but_one_day_data = till_but_one_day_data.filter( ~(col(transaction_type).isin(ignored_tr_types)) ).fillna({today_pu: 0, today_ru: 0, aum: 0, newmcrid: 'Others'})\n",
    "    final_day_data = final_day_data.filter( ~(col(transaction_type).isin(ignored_tr_types)) ).fillna({today_pu: 0, today_ru: 0, aum: 0, newmcrid: 'Others'})\n",
    "    \n",
    "    \n",
    "    all_data = till_but_one_day_data.union(final_day_data)\n",
    "    sp_inf_ouf_data = all_data\n",
    "\n",
    "    liquid_condition = ( (col(newmcrid) == 'A1b') & (col(calculated_date) == final_day_str) & (col(batch_close_date) == final_day_str) & (col(transaction_type).isin(liquid_fund_tr_types)) )\n",
    "    \n",
    "    final_day_data = final_day_data.withColumn(balance_pu,    when(liquid_condition, col(balance_pu) - col(today_pu)).otherwise(col(balance_pu)))\n",
    "    final_day_data = final_day_data.withColumn(balance_ru,    when(liquid_condition, col(balance_ru) - col(today_ru)).otherwise(col(balance_ru)))\n",
    "    final_day_data = final_day_data.withColumn(balance_units, when(liquid_condition, col(balance_pu) - col(balance_ru)).otherwise(col(balance_units)) )\n",
    "    final_day_data = final_day_data.withColumn(aum,           when(liquid_condition,  col(balance_units) * col(fn_nav)).otherwise(col(aum))    )\n",
    "\n",
    "    net_aum = final_day_data.groupby([newmcrid]).agg(sum(aum))\n",
    "    avg_data = till_but_one_day_data.union(final_day_data)\n",
    "#     avg_data = all_data\n",
    "    \n",
    "    # inflow, outflow logic change\n",
    "    sp_data = avg_data\n",
    "    # sp_inf_ouf_data = sp_inf_ouf_data.withColumn(inflow, (col(today_pu)*col(fn_nav)))\n",
    "    # sp_inf_ouf_data = sp_inf_ouf_data.withColumn(outflow, (col(today_ru)*col(fn_nav)))\n",
    "    sp = sp_data.groupby([newmcrid]).agg(countDistinct(scheme),countDistinct(plan))\n",
    "    \n",
    "#     inf_ouf_data = get_data(datetime.datetime(date_obj.year, 4, 2).strftime('%Y-%m-%d'), start_date, groupby_level, table).union(sp_data)\n",
    "#     inf_ouf_data = inf_ouf_data.withColumn(inflow, (col(today_pu)*col(fn_nav)))\n",
    "#     inf_ouf_data = inf_ouf_data.withColumn(outflow, (col(today_ru)*col(fn_nav)))\n",
    "#     inf_ouf_data = inf_ouf_data.groupby([newmcrid]).agg(sum(col(inflow)),sum(col(outflow)))\n",
    "    \n",
    "#     spinout = sp.join(inf_ouf_data, on=[newmcrid], how='left')\n",
    "#     spinout.show()\n",
    "    spinout = sp\n",
    "    # sp_inf_ouf_data = sp_inf_ouf_data.withColumn(inflow, (col(today_pu)*col(fn_nav)))\n",
    "    # sp_inf_ouf_data = sp_inf_ouf_data.withColumn(outflow, (col(today_ru)*col(fn_nav)))\n",
    "\n",
    "    folio_count = avg_data.groupby(folio, scheme, plan, newmcrid).agg(sum('aum')).filter(col('sum(aum)') - 0 > 0.1).groupby(newmcrid).agg(countDistinct(folio))\n",
    "\n",
    "    avg_aum = avg_data.groupby([newmcrid]).agg(sum(aum))\n",
    "    avg_aum = avg_aum.withColumn('avg_aum', col(f'sum({aum})')/(len(list(daterange(start_date, end_date))))).drop(f'sum({aum})')\n",
    "    \n",
    "\n",
    "    mcr_net_aum = spinout.join(net_aum, on=[newmcrid], how='left')\n",
    "    mcr = mcr_net_aum.join(avg_aum, on=[newmcrid], how='left')\n",
    "    mcr = mcr.join(folio_count, on=[newmcrid], how='left')\n",
    "    \n",
    "    # inflow outflow\n",
    "    all_data = get_data('2020-04-02', end_date, groupby_level=groupby_level, table=table)\n",
    "    all_data = all_data.filter(~liquid_condition)\n",
    "    all_data = all_data.fillna({today_pu: 0, today_ru: 0, aum: 0, newmcrid: 'Others', inflow:0, outflow:0})\n",
    "#     all_data = all_data.filter( ~(col(transaction_type).isin(ignored_tr_types)) ).fillna({today_pu: 0, today_ru: 0, aum: 0, newmcrid: 'Others', inflow:0, outflow:0})\n",
    "    inout = all_data.groupby([newmcrid]).agg(sum(inflow), sum(outflow))\n",
    "    mcr = mcr.join(inout, on=[newmcrid], how='left')\n",
    "    \n",
    "#     inout.show(1000)\n",
    "    mcr.show(1000)\n",
    "    \n",
    "    mcr.coalesce(1).write.csv(f\"{table}_mcr/mcr_{groupby_level}_{final_day_str}.csv\",header=True, mode='overwrite')\n",
    "#     mcr.coalesce(1).write.parquet(f\"{table}_mcr/data_{groupby_level}_{date_str}.parquet\", mode='overwrite')\n",
    "    \n",
    "    \n",
    "    return mcr,avg_data, sp_inf_ouf_data, all_data\n",
    " \n",
    "    \n",
    "table = 'trans116'\n",
    "table = 'm_Trans_116'\n",
    "groupby_level = 'SPFT'\n",
    "start_date = '2020-04-02'\n",
    "end_date = '2020-05-02'\n",
    "mcr, _,_,_ = generate_mcr_report(table=table, groupby_level=groupby_level, start_date=start_date, end_date=end_date)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:05:13.238626Z",
     "start_time": "2020-08-19T07:05:11.045764Z"
    },
    "code_folding": [
     9
    ]
   },
   "outputs": [],
   "source": [
    "table_codes = {117: 'MIRAE'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPT'\n",
    "        init_date = '2020-06-30'\n",
    "        mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange('2020-07-01', '2020-08-02'))):\n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "#             save_metric(ele, 'day_records', day_records, name, groupby_level)\n",
    "#             save_metric(ele, 'combined_records', combined_records, name, groupby_level)\n",
    "#             save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:05:13.924841Z",
     "start_time": "2020-08-19T07:05:13.433907Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "table_codes = {'RMF': 'Reliance'}\n",
    "\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPT'\n",
    "        init_date = '2020-06-30'\n",
    "        mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange('2020-07-01', '2020-08-02'))):\n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "#             save_metric(ele, 'day_records', day_records, name, groupby_level)\n",
    "#             save_metric(ele, 'combined_records', combined_records, name, groupby_level)\n",
    "#             save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T09:10:22.530312Z",
     "start_time": "2020-08-19T09:10:22.524239Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "# datetime.datetime.year('2020-08-04')\n",
    "\n",
    "date_obj = datetime.datetime.strptime('2020-08-05', '%Y-%m-%d')\n",
    "date_obj.year\n",
    "\n",
    "datetime.datetime(date_obj.year, 4, 2).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T17:20:38.744555Z",
     "start_time": "2020-08-19T17:20:38.732643Z"
    }
   },
   "outputs": [],
   "source": [
    "{\"116\": \"AXA\",\n",
    "\"117\": \"MIRAE\",\n",
    "\"107\": \"BOB\",\n",
    "\"120\": \"INVESCO\",\n",
    "\"RMF\": \"Reliance\",\n",
    "\"118\": \"Edelweiss\",\n",
    "\"135\": \"IDBIMF\",\n",
    "\"125\": \"IBMF\",\n",
    "\"128\": \"AXISMF\",\n",
    "\"178\": \"BNPMF\",\n",
    "\"152\": \"ITI\",\n",
    "\"105\": \"JMMF\",\n",
    "\"103\": \"PMF\",\n",
    "\"166\": \"Quant\",\n",
    "\"130\": \"PeerlessMF\",\n",
    "\"104\": \"TAURUS\",\n",
    "\"108\": \"UTI\",\n",
    "\"123\": \"Quantum\",\n",
    "\"127\": \"MOTILAL\",\n",
    "\"102\": \"LIC\",\n",
    "\"176\": \"SundaramMF\",\n",
    "\"101\": \"canrobeco\",\n",
    "\"129\": \"DLFPramerica\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#### all the script exaaecution for all the funds\n",
    "table_codes = {\n",
    "               102:'LIC',103:'pmf',104:'tarus',105:'JMMF',107:'BOB',108:'uti',\n",
    "               116:'AXA',117:'mirae',118:'edelwwise',120:'invesco',123:'quantum'\n",
    "               ,125:'IBMF',127:'motilal',128:'axismf',130:'peerless',135:'IDBIMF',152:'ITI',166:'quant',\n",
    "               176:'sundaram',178:'BNPMF', 'RMF':'reliance'}\n",
    "table_codes = {104:'taurus',105:'JMMF',107:'BOB',108:'uti',\n",
    "               116:'AXA',117:'mirae',118:'edelwwise',120:'invesco',123:'quantum'\n",
    "               ,125:'IBMF',127:'motilal',128:'axismf',130:'peerless',135:'IDBIMF',152:'ITI',166:'quant',\n",
    "               176:'sundaram',178:'BNPMF', 'RMF':'reliance'}\n",
    "\n",
    "table_codes = {101:'canrobeco',\n",
    "               102:'LIC',103:'pmf',104:'tarus',105:'JMMF',107:'BOB',108:'uti',\n",
    "               116:'AXA',117:'mirae',118:'edelwwise',120:'invesco',123:'quantum'\n",
    "               ,125:'IBMF',127:'motilal',128:'axismf',129:'pgim',130:'peerless',135:'IDBIMF',152:'ITI',166:'quant'\n",
    "               ,178:'BNPMF', 'RMF':'reliance', '129': 'dlfpramerica', 118:'edelweiss', 130:'peerlessMF', '176':'sundaramMF'}\n",
    "table_codes = {116: 'AXA'}\n",
    "\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPFT'\n",
    "        init_date = '2020-04-30'\n",
    "        mcr_month_date = '2020-05-01'\n",
    "\n",
    "        records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level)\n",
    "\n",
    "        print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "        job_start = time.time()\n",
    "\n",
    "\n",
    "        for i,ele in enumerate(list(daterange('2020-05-01', '2020-06-02'))):\n",
    "            s = time.time()\n",
    "            day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "            scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "             category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "            )\n",
    "            save_metric(ele, 'day_records', day_records, name, groupby_level)\n",
    "            save_metric(ele, 'combined_records', combined_records, name, groupby_level)\n",
    "            save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level)\n",
    "            print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "        s = time.time() \n",
    "#         generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-06-02', end_date = '2020-07-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "        print (f'job time is {time.time() - job_start}')\n",
    "        print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#### generate mcr for august for all the funds\n",
    "# table_codes = { '129': 'dlfpramerica', 118:'edelweiss', 130:'peerlessMF', '176':'sundaramMF'}\n",
    "\n",
    "table_codes = {101:'canrobeco',\n",
    "               102:'LIC',103:'pmf',104:'tarus',105:'JMMF',107:'BOB',108:'uti',\n",
    "               116:'AXA',117:'mirae',118:'edelwwise',120:'invesco',123:'quantum'\n",
    "               ,125:'IBMF',127:'motilal',128:'axismf',129:'pgim',130:'peerless',135:'IDBIMF',152:'ITI',166:'quant'\n",
    "               ,178:'BNPMF', 'RMF':'reliance', '129': 'dlfpramerica', 118:'edelweiss', 130:'peerlessMF', '176':'sundaramMF'}\n",
    "# exception taurus\n",
    "for code,name in (table_codes.items()):\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            spark.catalog.clearCache()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sc.stop()\n",
    "        except:\n",
    "\n",
    "            print (\"error no sc\")\n",
    "        # intialize spark again\n",
    "        conf = SparkConf()\n",
    "        conf.setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "\n",
    "        #Create spark context and sparksession\n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        SparkContext.setSystemProperty(\"spark.driver.memory\", \"40g\")\n",
    "        spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (code, name)\n",
    "        start = time.time()\n",
    "        table = f'm_Trans_{code}'\n",
    "        groupby_level='SPT'\n",
    "        init_date = '2020-07-01'\n",
    "        mcr_month_date = '2020-07-01'\n",
    "\n",
    "#         records = initialize(init_date, table=table,direct_db='BankRecon', groupby_level=groupby_level)\n",
    "#         save_metric(init_date, 'records_processed', records, name, groupby_level)\n",
    "\n",
    "#         print (f'initialization time {time.time() - start}')\n",
    "#         save_metric(init_date, 'intialization_time', time.time() - start, name, groupby_level)\n",
    "#         job_start = time.time()\n",
    "\n",
    "\n",
    "#         for i,ele in enumerate(list(daterange('2020-07-02', '2020-08-02'))):\n",
    "#             s = time.time()\n",
    "#             day_records, combined_records = dialy_job(ele, groupby_level=groupby_level, table=table, direct_db='BankRecon', nav_table=f'fund_navreg_{name}',\n",
    "#             scheme_table=f'fund_master_{name}', scheme_code='fm_scheme', plan_code='fm_plan', nature='fm_nature',\n",
    "#              category = 'fm_SebiSchemeCategory',subcategory = 'fm_SebiSchemeSubCategory', newmcrid='fm_NewMCRId'\n",
    "#             )\n",
    "#             save_metric(ele, 'day_records', day_records, name, groupby_level)\n",
    "#             save_metric(ele, 'combined_records', combined_records, name, groupby_level)\n",
    "#             save_metric(ele, 'dialy_job_time', time.time() - s, name, groupby_level)\n",
    "#             print (\"    \",i, ele, time.time() - s)\n",
    "\n",
    "#         s = time.time() \n",
    "        generate_mcr_report(table=table, groupby_level=groupby_level, start_date = '2020-07-02', end_date = '2020-08-02')\n",
    "#         save_metric(mcr_month_date, 'mcr_generate_time', time.time() - s, name, groupby_level)\n",
    "\n",
    "#         print (f'job time is {time.time() - job_start}')\n",
    "#         print (f'overall time is {time.time() - start}')\n",
    "#         save_metric(mcr_month_date, 'overall_time', time.time() - start, name, groupby_level)\n",
    "\n",
    "        print ()\n",
    "    except Exception as e:\n",
    "        print (traceback.print_exc())\n",
    "        print (str(e))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
